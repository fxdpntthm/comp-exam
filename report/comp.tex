\newif\ifcomments\commentstrue

\RequirePackage[svgnames,dvipsnames,prologue,x11names]{xcolor}

\documentclass[manuscript,screen,nonacm]{acmart}

\usepackage{comp}

\title{Practical Functional Programming in \SFC and its extensions}
% \subtitle{Extensions to System F}

\author{Apoorv Ingle}
%\orcid{0000-0002-7399-9762}
\affiliation{%
  \institution{University of Iowa}
  \department{Department of Computer Science}
  \streetaddress{McLean Hall}
  \city{Iowa City}
  \state{Iowa}
  \country{USA}}
% \keywords{typeclass, type family}

\begin{document}
\begin{abstract}
The following report is a juxtaposition of the series of extensions to \SFC and its theoretical developments. We begin with a brief description of \SF in a type intrinsic setting and give some context about its importance and its meta-theoretic properties. We then move on to describe the first extension to \SF, \SFC, that enriches the calculus with explicit type equalities. The repercussions of this extension is extensive as it enables arbitrary type level computation. \SFC is then extended to \SFP with an expressive kind---the type of types---system that enables an even more expressive type level computation. Finally, we discuss \SFK which enables expressing kind level equalities along with existing type level equalities in the system by effectively squashing the distinction between types and kinds.

While the theoretical aspects of each of these systems are of academic importance, for these systems to be useful as a practical programming language, we want to expose this power to the programmers---the end users of these systems---, without compromising on saftey and a usability. We describe a framework \HMX that parameterizes over the feature domain X such that for each of the systems described above we get a type inference algorithm for free, meaning, we do not have to redesign and reimplement them.

We conclude the report with some open problems in the area.
\end{abstract}
\maketitle

% \pagestyle{plain}

\newcommand\STLC{\textbf{STLC}\xspace}
\newcommand\cy[1]{[\citeyear{#1}]}
\section{\SF}
\subsection{History and Inception}
Simply typed lambda calculus (\STLC) was devised by \citet{church_formulation_1940} to avoid the paradoxes that are possible in untyped lambda calculus. \STLC, in its bare form has three constructs: Variables, lambda abstractions and applications. Lambda abstractions and applications are devices to model functions and function calls respectively. Simple types, however, makes the system too restrictive. Consider an identity function that takes an argument of some base type, say $i$, and returns it unchanged.
$$
id_\alpha = \Lam {x\co \alpha} x
$$
In \STLC, although the function behavior is the same at each base type, there would be a different identity function for each such base type. Further, if a new base type is introduced in the system, we would have to define a new identity function for it. Functions, such as identity, are said to be parametrically polymorphic\cite{strachey_fundamental_2000} when their behavior does not change depending on the type of the arguments.

To give a better account for such parametric functions, \citet{reynolds_towards_1974} proposed an extension of \STLC which later famously became \SF. Reynolds extension to \STLC is straightforward. It contains two extra syntactic constructs: type abstractions and type applications. This extension is simple enough to express parametric functions. For example, the identity function will be expressed as follows:
$$
id = \TLam \alpha \Lam {x\co\alpha}. x
$$
\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Type Vars} &&\alpha,\beta,\gamma \\
    \text{Term Vars} &&x,y          \\
  \end{syntax}
  \begin{syntax}
    \text{Kinds}     &&\kappa       &::= \star \bnfor \kappa \to \kappa\\
    \text{Types}     &&\tau,\sigma  &::= \alpha \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \syntaxhl{\Forall {\alpha\co\kappa} \tau}\\
    \text{Terms}     &&\Tm,N          &::= x \bnfor  H \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \syntaxhl{\TLam{\tau\co\kappa} M \bnfor M\App \tau}\\
  \end{syntax}
  \begin{syntax}
    \text{Typing Context} &&\TEnv &::= \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\alpha\co\kappa \bnfor \TEnv,H\co T\\
    \text{Substitutions}  &&\Subst
  \end{syntax}
  \caption{\SF as an extension of \STLC with kinds}
  \label{fig:system-f-syntax}
\end{figure}

\newcommand\KVar{
  \ib{\irule[\trule{k-var}]
    {\alpha\co\kappa \in \TEnv};
    {\Kinding \TEnv \alpha \kappa}}
}

\newcommand\KVarCo{
  \ib{\irule[\trule{co-var}]
    {c \co \sigma\sim\tau \in \TEnv};
    {\Kinding \TEnv c \sigma\sim\tau}}
}


\newcommand\KConst{
  \ib{\irule[\trule{k-const}]
    {T\co\kappa \in \TEnv};
    {\Kinding \TEnv T \kappa}}
}

\newcommand\KTApp{
  \ib{\irule[\trule{k-tapp}]
    {\Kinding \TEnv {\tau_1} {\kappa_1 \to \kappa}}
    {\Kinding \TEnv {\tau_2} {\kappa_1}};
    {\Kinding \TEnv {\tau_1\App \tau_2} \kappa}}
}


\newcommand\TVar{
  \ib{\irule[\trule{var}]
    {x\co\tau \in \TEnv}
    {\Kinding \TEnv \tau \kappa};
    {\Typing \TEnv x \tau}}
}

\newcommand\TConst{
  \ib{\irule[\trule{const}]
    {H\co T \in \TEnv};
    {\Typing \TEnv H T}}
}

\newcommand\TAbs{
  \ib{\irule[\trule{$\I\to$}]
    {\Typing {\TEnv, x\co\tau_1} {\Tm} {\tau_2}};
    {\Typing \TEnv {\Lam x \Tm} {\tau_1 \to \tau_2}}}
}

\newcommand\TApp{
  \ib{\irule[\trule{$\E\to$}]
    {\Typing \TEnv {\Tm_1} {\tau_2 \to \tau}}
    {\Typing \TEnv {\Tm_2} {\tau_2}};
    {\Typing \TEnv {\Tm_1\App\Tm_2} {\tau}}}
}

\newcommand\TForallI{
  \ib{\irule[\trule{$\I\forall$}]
    {\Typing {\TEnv,\alpha\co\kappa} \Tm \tau}
    {\alpha \not\in \dom\TEnv};
    {\Typing \TEnv {\TLam {\alpha\co\kappa} \Tm} {\Forall{\alpha\co\kappa}\tau}}
  }
}

\newcommand\TForallE{
  \ib{\irule[\trule{$\E\forall$}]
    {\Typing \TEnv \Tm {\Forall{\alpha\co\kappa}\sigma}}
    {\Kinding \TEnv \tau \kappa}
    {\Subst = \Sub \alpha \tau};
    {\Typing \TEnv {\Tm\App\tau} {\Subst\sigma}}
  }
}

\begin{figure}[ht]
  \begin{gather*}
    \fbox{$\Kinding \KEnv \tau \kappa$}\\
    \KVar \rsp \KConst \rsp \KTApp
  \end{gather*}

  \centering
  \begin{gather*}
    \fbox{$\Typing \TEnv \Tm \tau$}\\
    \TVar \rsp \TConst\\
    \TAbs \rsp \TApp\\
    \TForallI \rsp \TForallE
  \end{gather*}

  \caption{Static Semantics of \SF}
  \label{fig:sf-typing}
\end{figure}

\newcommand{\SOPL}{P$_2$\xspace}
This extension from \STLC makes the semantics extremely powerful. \citet{girard_interpretation_1972}, about two years before Reynolds, proved the Representation theorm ($\mathcal{G}$), that says: all total functions on natural numbers in the second order propositional logic (\SOPL) can be expressed in \SF. \citet{reynolds_types_1983} formulated the Abstraction theorem ($\mathcal{R}$) that proves \SF is an embedding of \SOPL and \citet{wadler_girard-reynolds_2001} later showed that the abstraction followed by embedding is an identity transformation.

\begin{figure}[ht]
  \centering
    \begin{tikzpicture}
    \draw (0, 0) circle [x radius=0.5, y radius=1];
    \node [label=above:\SF] (f2) at (0, 2) {};

    \begin{scope}[xshift=3cm]
      \draw (0, 0) circle [x radius=0.7, y radius=1.5];
      \draw[dotted] (0, 0) circle [x radius=0.5, y radius=1];
      \node [label=above:\SOPL] (p2) at (0, 2) {};
    \end{scope}
  \end{tikzpicture}
  \caption{Relationship between \SF and \SOPL}
  \label{fig:sf-p2-relation}
\end{figure}


\section{Features of Typed Functional Programming Languages}\label{sec:language-features}
\subsection{User Defined Datatypes}
Organizing data into a logical fashion is important aspect of any programming language. The domain elements are mapped on to structures that store information. The business logic of the domain is then just transformations of the structures from one form to the other. In functional programming languages, algebraic datatypes are the essential way to define new structures.

\subsubsection{Algebraic Datatypes}
Algebraic Datatypes (ADT) can be viewed as a composite datatype. They are composed of named sums of products. They enhance the expressivity of the language by allowing users to declare their own structures and aid in organizing data. For example, a balanced binary tree in Haskell can be declared as follows:

\begin{code}
  data Tree c = Leaf c | Branch (Tree c, Tree c)
\end{code}

The keyword \lstinline{data} introduces a user defined type with name \lstinline{Tree} parameterized over any type !c!. A value of type !Tree c! can be defined using either a !Leaf! or a !Branch!, and in the case of !Branch!, it consists of a pair of !Tree!s. !Leaf! and !Branch! are called as data constructors, while !Tree! is called as a type constructor. It is important to observe that the structure the datatype does not dependent on the type parameter !c!, i.e. it is used parametrically. More specifically, the structure of !Tree Int! and !Tree Float! will be the same.

\subsubsection{Generalized Algebraic Datatypes}
As the name suggests, they are generalizations of algebraic datatypes where the structure of the type can depend on the type arguments. We use the term type index for GADTs rather than type parameter in algebraic datatype. They were first introduced by \citep{cheney_first-class_2003} as first class phantom types and later studied under different name such as guarded recursive datatypes\cite{xi_guarded_2003}. Such Their utility is in modelling domain specific languages where certain constraints can be enforced by the type checker. Consider an example to model a generic !Trie k v! datatype that represents a finite mapping from keys of type !k! to values of type !v!.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      data Trie k v where
          TSingle :: Maybe v -> Trie () v
          TSum    :: Trie k1 v -> Trie k2 v
                  -> Trie (Either k1 k2) v
          TProd   :: Trie k1 (Trie k2 v)



    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      lookup :: Trie k v -> k -> Maybe v
      lookup (TSingle m) ()         = m
      lookup (TSum ta tb) (Left a)  = lookup ta a
      lookup (TSum ta tb) (Right b) = lookup tb b
      lookup (TProd ta) (a, b)      = case (lookup ta a) of
                                         Nothing -> Nothing
                                         Just b -> lookup tb b
    \end{code}
  \end{minipage}
  \caption{Trie datatype as GADT}
  \label{fig:gadt-example}
\end{figure}


\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
data LamTm a where
  Var :: String -> LamTm a
  Lam :: (LamTm a -> LamTm b) -> LamTm (a -> b)
  App :: LamTm (a -> b) -> LamTm a -> LamTm b
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      eval :: LamTm a -> LamTm a
      eval (App (Lam f) a) = f a

      
    \end{code}
  \end{minipage}
  \caption{Expressing Lambda Terms as GADT}
  \label{fig:gadt-example}
\end{figure}
Writing an !eval! function for a !LamTm a! would be impossible if it were not for the constrainted type arguments of data constructors !Lam! and !App!. The only way of 

\subsubsection{Generative Types}
\subsection{From Parametric Polymorphism to Ad-hoc Polymorphism}
In contrast to the term \lstinline{id}, consider the following two terms:
\begin{code}
    t1 :: Int = 1 + 2
    t2 :: Float = 1.1 + 2.3
\end{code}

In the first case we see that the operator \lstinline{+} is applied to two integers, but in the second case it is applied to two floating point numbers. Although the programmer uses the same symbol, the meanings of each of the programs is completely different. One adds two \lstinline{Int} and returns an \lstinline{Int} while the other adds two \lstinline{Float} and returns a \lstinline{Float}. Further, the compiled code for each would also be different. The runtime code would make a call to two different built in constructs. Such kind of function reuse, which is dependent on the context or the type of arguments, is called as ad-hoc polymorphism\cite{strachey_fundamental_2000}. Implicit operator overloading is a general mechanism to impliment ad-hoc polymorphism where the compiler resolves the overloaded operator to the actual operator.

To make implicit overloading practical \citet{wadler_polymorphism_1989} proposed a dictonary passing style by using a mechanism. This technique became the foundation for Haskell\cite{haskell_2010} typeclasses. Kaes\cy{kaes_parametric_1988} had similar ideas related to implimentation aspects of ad-hoc polymorphism before Wadler et al. ML and its varients\cite{milner_definition_1997,leroy_ocaml_2023} takes a more restricted approach and refuses to allow operator overloading by design.

Implicit objects\cite{oliveira_typeclasses_2010} provide a mechanism for users to choose the intended behavior instance if there are multiple available options. This pushes the burden of choosing the right instance on programmer whenever the typechecker cannot figure out the right option or when the user wants to force the typechecker to resolve it to a specific instance.

Various attempts\cite{dreyer_modular_2007, wehr_ml_2008, white_modular_2014} have been made to enable a happy co-existence of parametric and implicit ad-hoc polymorphism together in a language by encoding typeclasses as modules and vice versa. However, in presense of modularity to support separate code compilation, it becomes impossible to avoid incoherence without cripling the language expressibility.

\subsection{Typeclasses}
\subsubsection{Operator overoading}
Typeclasses can be viewed as relations over types and every instance declaration extends that relation. Consider a typeclass \lstinline{Ord a} shown in \pref{fig:tc-ord}. It represents a unary relation for the types whose values can be compared. Instances of the typeclass \lstinline{Ord} are written \lstinline{Ord Int} and \lstinline{Ord Float} say that \lstinline{Float} and \lstinline{Int} belong to the unary relation \lstinline{Ord}.

%% Something about methods

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Ord a where
         (<)  :: a -> a -> Bool
         (<=) :: a -> a -> Bool
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Ord Int where
         (<)  = int_le
         (<=) = int_leq
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Ord Float where
         (<)  = le_float
         (<=) = leq_float
    \end{code}
  \end{minipage}
  \caption{\lstinline{Ord} typeclass and instances}
  \label{fig:tc-ord}
\end{figure}
\subsubsection{Multi-Parameter Typeclasses and Functional Dependencies}
A straightforward generalization of single parameter typeclases is multiparameter typeclasses. It naturally extends the idea of having n-ary propositions or relations over types. Such relations can be useful to express properties such as containment relations. For example, following\cite{jones_tcfd_2000}, a typeclass to unify different containers under a single interface can be defined using a !Coll e c! typeclass, as shown in \pref{fig:tc-collection}. One can imagine having instances for such a typeclass as \lstinline{Coll Int [Int]}, \lstinline{Coll Float (BST Float)} etc. Speaking in terms of relations, we say that \lstinline{(Int, [Int]) $\in$ Coll} and similarly \lstinline{(Float, BST Float) $\in$ Coll}.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Coll e c
      where
         empty :: c
         insert :: e -> c -> c
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Coll Int [Int]
      where
         empty = ...
         insert = ...
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Coll Float (BST Float)
      where
         empty = ...
         insert = ...
    \end{code}
  \end{minipage}
  \caption[Collection typeclass]{\lstinline{Collection} typeclass and its instances}
  \label{fig:tc-collection}
\end{figure}

The use of \lstinline{empty} in a polymorphic setting however interferes with the compilation decision. It is not enough to determine what the type of \lstinline{e} should be just by knowing the type \lstinline{c}. Such types are called ambiguous types. Formally speaking, if a type variable only appears in the constraints of the type it is called an ambiguous type. The type of !empty!, !FORALL e c. Coll e c => c!, is ambiguous due to the occurance of !e! only the constraint. Such types do not have a well defined semantics as the compiler cannot make a unique choice of what the compiled code should be. The problem is here is that the typeclasses are relations and hence are too general for this use case. To make typeclasses behave in a more constrained way, Jones introduced functional dependencies\cite{jones_tcfd_2000}. The insight is to be able to specify the functional nature of relations, i.e. to be able to say if we \emph{know} certain type parameters of the typeclass, we can \emph{determine} the other type parameters as well.

\begin{figure}[ht]
  \begin{center}
    \begin{code}
      class Coll e c | c ~> e where
          empty :: c
          extend :: e -> c -> c
    \end{code}
  \end{center}
  \caption[Collection typeclass]{\lstinline{Collection} Typeclass with Functional Dependency}
  \label{fig:tc-collection-fd}
\end{figure}

The extra annotation !c ~> e! on the typeclass !Coll e c! as shown in \pref{fig:tc-collection-fd} says that !e! can be uniquely determined for given a !c!.
We call !c! to be the determiner and !e! to be the determinant of the functional dependency.

% improvement, simplification
%

\subsection{Type Functions}
\subsubsection{Associated Types}
The main reason to have multiparameter type classes with functional dependencies was to enable the type system to express type functions. In other words, if the determiner type variables of the functional dependency determine the determinant type variables, then it would be syntactically pleasing to write them in a function form.
Using our previous !Coll c e! example, if !c ~> e! then, it would be more obvious to the programmers to instead write !Elem c! instead of !e!, where !Elem c! is a special type function that takes in a type and returns a type. This effectively gives an alternative route to express functional dependencies.
\begin{figure}[ht]
  \begin{center}
    \begin{minipage}[ht]{0.4\linewidth}
      \begin{code}
        class Coll c where
           type Elem c
           empty :: c
           extend :: Elem c -> c -> c
      \end{code}
    \end{minipage}%
    \begin{minipage}[ht]{0.4\linewidth}
      \begin{code}
        instance Coll [Int] where
           type Elem [Int] = Int
           empty = ...
           extend = ...
      \end{code}
    \end{minipage}
  \end{center}
  \caption[Collection typeclass]{\lstinline{Coll} Typeclass with Type Function}
  \label{fig:type-fam}
\end{figure}

In \pref{fig:type-fam}, the typeclass !Coll! has an associated type function !Elem! that depends on the type parameter !c! of the typeclass. 

\subsubsection{Open Type Functions}
There is nothing special about type functions to only be associated with classes. Type functions are able to express complex type level computations. Further, they are also open, meaning, just like class instances, the open type functions can be extended. An example of how type computations is expressed by type functions that define addition of naturals at type level, shown in \pref{fig:open-type-fun-add}
\begin{figure}[ht]
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      data Z
      data S n
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      type family Add m n
      type instance Add Z n = n
      type instance Add (S m) n = S (Add m n)
    \end{code}
  \end{minipage}
  \caption{Addition with type functions}
  \label{fig:open-type-fun-add}
\end{figure}
Open type functions thus give type computation a flavor of rewrite rules.
\subsubsection{Closed Type Functions}
There maybe cases where the type functions have to be restricted extensions. For example The previously defined
!Add! type function is supposed to be defined only on !Z! and !S n! types. Closed type families help in this situtation. 
\begin{figure}[ht]
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      type family Add m n where
          Add Z n = n
          Add (S m) n = S (Add m n)
    \end{code}
  \end{minipage}
  \caption{Addition with type functions}
  \label{fig:closed-type-fun-add}
\end{figure}

\subsection{Modules and Packages}
Modularity in code is essential to maintain separation of concerns. It is a good sofware practice organize related things together, while unrelated things separate. Modules are a way to do that. 

\subsection{Intrinsic Typing vs Extrinsic Typing} % Principal typing
Intrinsic typing: When the type is tied together with its term. This makes typechecking easy as there is not really any inference bit to peform but makes terms ``rigid''. This rigidity has philosophical implications.
Extrinsic: The type is loosly tied with the term.

Principal typing: Given a term one can infer the most general type for that term.
as a decision problem, infering a principal type for a term in general is undecidable.
System F has undecidable type inference\cite{wells_typability_1999}.

But not all hope is lost. Hindley-Milner (HM) type system, which is a restriction of System F, has a decidable type inference algorithm. HM is expressive enough for most practical programming purposes; ML and Haskell are all based on HM type systems.
Various works have tried to find the sweat spot of what is the minimal type annotation needed for the type inference.
The idea is that we need to reduce the programmer burden of writing down the types where it is ``obvious''.
Philosophically, if the system has a principal typing property then the distinction between intrinsic types and extrinsic types disappers as we can use an algorithm to infer the type of the term and decorate the original term with the infered types to recover the intrinsic term. In a sense, there is a galois connection between the intrinsic term and its extrinsic counterpart. The type inference mapping takes an term from extrinsic to an intrinsic while type erasure mapping takes an extrinsicly typed term to intrinsicly typed term

% \subsection{Treatment of Equalities}
% There are two ways of remembering type equalities that the typechecker would have proved or the ones provided by the user: Store them as terms or store them as types. Both of them seem to be equally attractive however they have significant computational differences

% Storing them as term makes the type equality coercions explicit in the code, helps type checking. For a proof assistant it would be an attractive feature, but for a programming language it would mean that we carry such
% type equality proofs into the compiled language. Dynamic type dispatch is a an important application for being able to carry proofs about type equalities.

% Storing them as types has an advantage that we can erase the equality proofs before compiling it to a more efficient runtime representation. After all, type equalities are only really needed by the type checker to prove that nothing will go wrong at runtime. At runtime we don't need it.
%  Intensional vs extensional % Intensional vs Extensional typing


\section{\SFC}\label{sec:sfc}
The Glasgow Haskell Compiler (GHC)\cite{ghc_2020}, is a widely used Haskell\cite{haskell_2010} compiler. Its inception was a result of a lack of a test bed for lazy strongly typed functional programming languages. It is an industry grade compiler, meaning the compiler generated code can perform with efficiency comparable to other language compilers like Java or C++. The original core language of GHC was based on \SF. However, it soon became clear that with the addition of GADTs and generative types, using pure \SF would be too cumbersome, if not impossible to express in some constructs. For example consider a generative type declaration
\begin{code}
  newtype Fun = MkFun (Fun -> Fun)
\end{code}
The intension of this declaration is to make !Fun! and !Fun -> Fun! be isomorphic, however, it there was no good way to encode this in pure \SF. GHC relied on a hack to express this in the core language.
\SFC\cite{sulzmann_system_2007} was desiged and implimented to solve this problem. In the following sections we take a look of the core language, followed by how it can be used to encode all the features discussed in \pref{sec:language-features}. 
% In one statment: \SFC is an intensional intrinsically typed programming language.
% It is intensional, meaning each term encodes its typing derivation, and it is intrinsically typed, meaning all the terms are index by types.

% Some key points to cover:

% Store the equality between types explicitly in the AST during type checking.

% New feature: coercion is a type and its kind tells us what types does the coercion equate.

% Features that can be directly expressed in \SFC: New types or generative types, associated types, functional dependencies, generalized algebraic datatypes (GADTs).

% Brand new feature user defined open type functions.

% Makes type rewriting complicated. But makes the type system more expressive by making it extensible.
% Proving type soundness is a bit more involved now.

% why don't we have a rule that says: if $\sigma_1 \sim \sigma_2$ then

\subsubsection{Syntax}
\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Type Vars} &\alpha,\beta,\gamma  &\qquad\text{Type constants} &T \\
    \text{Term Vars} &x,y                  &\qquad\text{Indices}        &i,n \in \mathbb{N} \\
    \text{Coercion Vars} &c & &
  \end{syntax}
  \begin{syntax}
    \text{Kinds}     &&\kappa       &\bnfeq \star \bnfor \kappa \to \kappa \bnfor \sigma \sim \tau\\
    \text{Types}     &&\tau,\sigma  &\bnfeq \alpha \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \Forall {\alpha\co\kappa} \tau \bnfor F_n\\
    \text{Coercions} &&\nu,\Co      &\bnfeq c \bnfor \refl\tau \bnfor \sym\Co \bnfor \trans\nu\Co % equiv relation
                                        \bnfor \Forall {\alpha\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
                                        \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co\\  % compose/decompose
    \text{Types/Coercions} && \phi &\bnfeq \tau \bnfor \Co\\
    \text{Patterns}  &&P    &\bnfeq H\App \many{\alpha\co\kappa}\App{\many{x\co\tau}} \\
    \text{Terms}     &&M,N  &\bnfeq x \bnfor  \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co\\

    \end{syntax}
    \begin{syntax}
    \text{Typing Context} &&\TEnv,\Delta &\bnfeq \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\alpha\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \gamma \co \tau\sim\sigma\\
    \text{Substitutions}  &&\Subst       &\bnfeq \empt \bnfor \Set{\many{\alpha \mapsto \tau}}
  \end{syntax}

  \begin{syntax}
    \text{Program} &&P_{gm} &\bnfeq \many{D_{cl}} \mathrel{;} \many{\Tm}\\
    \text{Data Declarations} &&D_{cl} &\bnfeq \textbf{\texttt{data }}\App T\co\many{\kappa} \to \star\App \textbf{\texttt{ where }}\App \many{C_{trs}(T)} \bnfor \textbf{\texttt{type }}\App F_n : \many\kappa^n \to \kappa \bnfor  \textbf{\texttt{axiom }}\App C\App \many{\alpha\co\kappa} : \sigma_1 \sim \sigma_2 \\
    \text{Data Constructors} &&C_{trs}(T) &\bnfeq H : \Forall {\many{\alpha\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \many{(\tau \sim \tau)} \then \many\sigma \to T\many\alpha}\\
  \end{syntax}
  
  \caption{The Syntax of \SFC}
  \label{fig:system-fc-syntax}
\end{figure}

\SFC is impredicative; there is no stratification between polytypes and monotypes. Unlike \SFw, there are no lambdas at type level this makes the type level calculus less expressive by disallowing certain types such as $\Lam a (a, a)$. The system does allow higher kinded types---either built or user defined---that can be used by the type application form $\tau\App\tau$. \AI{what is the difference between $\Forall a (a, a)$ and $\Lam a. (a, a)$?} The value type constructors $T$ ranges over built-in types such as !Int! and user defined types such as algebraic datatypes. Declaration of algebraic datatypes introduces data constructors $H$, the types of which are of the form:
$$
H \co \Forall {\many{\alpha\co\kappa}} {\Forall {\many{\beta\co\kappa}} \many{\sigma} \to T \many\alpha}
$$
Here the type variables $\many\alpha$ appear in the same order as in the algebraic datatype delcarations,  and type variables $\many\beta$ are the existential; they do not appear in the return type. This enforcement is necessary to avoid any type variable to escape its scope. The existential type variables play a crucial role in encoding GADTs. 

The type equality coercion, $\gamma$, is the essential additional construct. Coercions are types and have a ``cannonical'' form of $\tau\sim\sigma$. \AI{TODO: Are coercions types or are types coercions?}. They can be constructed, applied, passed around as arguments using the special coercion infrastructure at the level of types. They appear at term level in the form of casts $\Cast M \gamma$. For type theory enthusiasts, type coercions should be thought of as extensional type equality, meaning, if a term $M$ has a type $\tau$ and we have a type coercion $\tau\sim\sigma$, then the type cast $\Cast M  {\tau\sim\sigma}$ says that $M$ can be treated as if it has type $\sigma$. The soundness property of the type system guarantees that after the types (including casts) have been erased, the program will not crash or get stuck. Unlike other regular types, however, coercions do not classify values, i.e. there are no term level constructs that have a type $\tau\sim\sigma$. This is similar to how in \SFw there are no values of a type that has a kind $\STAR\to\STAR$.

\SFC supports a full fledged coercion calculus where each syntactic construct corresponds to a logical equation. The most basic type of coercion can be produced by using the reflexivity construct $\refl\tau$. It says that the type $\tau$ witness the (obvious) fact that it is equal to itself. The other constructs on coercions are symmetry '$\sym\gamma$', transitivity '$\trans {\gamma_1}{\gamma_2}$', which together with $\refl\tau$, makes type equality an equivalence class. Coercions can also be composed and decomposed using the '$\Co_1\Co_2$', and '$\Left\Co$' and '$\Right\Co$' constructs respectively. Finally, coercion abstraction '$\Forall {\alpha\co\kappa}\tau$' and coercion application '$\Co\At\tau$' aids equality reasoning on polytypes.

\subsection{Static Semantics}

\newcommand\TCast{
  \ib{\irule[\trule{cast}]
    {\Typing \TEnv {\Tm} {\tau}}
    {\CoKinding \TEnv \gamma {\tau \sim \sigma}};
    {\Typing \TEnv {\Cast \Tm \gamma} {\sigma}}
  }
}

\newcommand\KReflCo{
  \ib{\irule[\trule{co-refl}]
    {\TyKinding \TEnv \tau \kappa};
    {\CoKinding \TEnv {\refl \tau} {\tau \sim \tau}}
  }
}

\newcommand\KSymCo{
  \ib{\irule[\trule{co-sym}]
    {\CoKinding \TEnv \gamma {\tau \sim \sigma}};
    {\CoKinding \TEnv {\sym \gamma} {\sigma \sim \tau}}
  }
}

\newcommand\KTransCo{
  \ib{\irule[\trule{co-trans}]
    {\CoKinding\TEnv {\gamma_1} {\tau \sim \tau_2}}
    {\CoKinding\TEnv {\gamma_2} {\tau_2 \sim \sigma}};
    {\CoKinding\TEnv {\trans {\gamma_1} \gamma_2} {\tau \sim \sigma}}
  }
}

\newcommand\KInstCo{
  \ib{\irule[\trule{co-$\E\forall$}]
    {\CoKinding\TEnv \gamma {\Forall\alpha\tau_1 \sim \Forall\beta\tau_2}}
    {\Subst_1 = \Sub\alpha\sigma}{\Subst_2 = \Sub\beta\sigma};
    {\CoKinding\TEnv {\gamma\At\sigma} {\Subst_1\tau_1 \sim \Subst_2\tau_2}}
  }
}

\newcommand\KForallCo{
  \ib{\irule[\trule{co-$\I\forall$}]
    {\CoKinding {\TEnv,\alpha\co\kappa} \gamma {\tau_1 \sim \tau_2}}{\alpha \# \TEnv};
    {\CoKinding \TEnv {\Forall {\alpha\co\kappa} \gamma} {\Forall {\alpha\co\kappa}\tau_1 \sim \Forall {\alpha\co\kappa}\tau_2}}
  }
}


\newcommand\KCoComp{
  \ib{\irule[\trule{co-comp}]
    {\CoKinding \TEnv {\gamma_1} {\tau_1 \sim \tau_2}}
    {\CoKinding \TEnv {\gamma_2} {\sigma_1 \sim \sigma_2}}
    {\TyKinding \TEnv {\tau_i\App \sigma_i} \kappa};
    {\CoKinding \TEnv {\gamma_1\App \gamma_2} {\tau_1 \sigma_1 \sim \tau_2 \sigma_2}}
  }
}

\newcommand\KLeftCo{
  \ib{\irule[\trule{co-left}]
    {\CoKinding \TEnv {\gamma} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
    {\CoKinding \TEnv {\Left \gamma} {\tau_1 \sim \tau_2}}
  }
}

\newcommand\KRightCo{
  \ib{\irule[\trule{co-right}]
    {\CoKinding \TEnv {\gamma} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
    {\CoKinding \TEnv {\Right \gamma} {\sigma_1 \sim \sigma_2}}
  }
}

\newcommand\KCastCo{
  \ib{\irule[\trule{co-leftc}]
    {\CoKinding \TEnv \gamma {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
    {\CoKinding \TEnv {\Cast {\gamma_1} \gamma_2} {\tau_1 \sim \tau_2}}
  }
}

\newcommand\KCoAx{
  \ib{\irule[\trule{co-ax}]
    {\CoKinding \TEnv \gamma {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
    {\CoKinding \TEnv {\Cast {\gamma_1} \gamma_2} {\tau_1 \sim \tau_2}}
  }
}

\newcommand{\KTyVar}{
  \ib{\irule[\trule{ty-var}]
    {\alpha\co\kappa \in \TEnv};
    {\TyKinding \TEnv \alpha \kappa}
  }
}
\newcommand{\KTyApp}{
  \ib{\irule[\trule{ty-app}]
    {\TyKinding \TEnv \tau \kappa'}
    {\TyKinding \TEnv \sigma {\kappa' \to \kappa}};
    {\TyKinding \TEnv {\sigma\App\tau} \kappa}
  }
}
\newcommand{\KTyCon}{
  \ib{\irule[\trule{ty-con}]
    {F_n \co \many \kappa^n \to \kappa' \in \TEnv}
    {\many {\TyKinding \TEnv {\sigma} {\kappa}}^n};
    {\TyKinding \TEnv {F_n \many\sigma^n} {\kappa'}}
  }
}
\newcommand{\KTyAll}{
  \ib{\irule[\trule{ty-all}]
    {\TyKinding {\TEnv,\alpha\co\kappa} {\sigma} \star}
    {\fresh \alpha \TEnv};
    {\TyKinding \TEnv {\Forall {\alpha\co\kappa} \sigma} \star}
  }
}

\begin{figure}[t]
  \centering
  
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}\\
    \TCast
  \end{gather*}

\begin{gather*}
  \fbox{$\TyKinding \TEnv \tau \kappa$}\\
  \KTyVar \rsp \KTyApp \rsp \KTyCon \rsp \KTyAll
  \end{gather*}
  
  \begin{gather*}
    \fbox{$\CoKinding \TEnv \tau \kappa$}\\
    \KReflCo \rsp \KSymCo \rsp \KTransCo \\
    \KForallCo \rsp \KInstCo \\
    \KCoComp \\
    \KLeftCo \rsp \KRightCo \\
  \end{gather*}

  \caption{Excerpt of Static Semantics of \SFC}
  \label{fig:sf-typing}
\end{figure}

To formalize our intuitions of how the coercions ought to behave, we provide static semantics in the form of declarative style typing and kinding rules in \pref{fig:sf-typing}. To start off the \trule{cast} transforms a term $M\co\tau$ to a term $M\co\sigma$ with a witness coercion $\Co\co\tau\sim\sigma$

As all the coercions are types such that their kinds give us the type equality, the coercion calculus is part of the kinding rules. The kinding rules \trule{co-refl}, \trule{co-sym}  and \trule{co-trans} makes coercion an equivalence relation. The rule \trule{co-comp} enables lifting coercions for higher kinded types and reason equalities between them. As a simple example of why coercion composition is useful, consider a higher kinded algebraic type !Tree! and a coercion $\Co\co\sigma_1\sim\sigma_2$, then using $\tau_1$ and $\tau_2$ to be equal to !Tree!, we have that $\refl{\texttt{Tree}} \App\Co : \texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_2$.
On the other hand, if we have a coercion $\texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_1$ then we can recover the coercion components using the \trule{co-left} and \trule{co-right}, for the higher kinded type and its arguments repectively. In full generality, we can state the lifting property formally in \SFC using \pref{thm:sfc-coercion-lifting}.
It captures the intuitive idea that given equal sub-parts, an equality between whole entities can be constructed.
\begin{theorem}[Coercion Lifting]\label{thm:sfc-coercion-lifting}
  If $\TyKinding {\TEnv,\alpha\co\kappa'} \phi \kappa$, where $\alpha$ is free in $\phi$
  and does not appear free in $\TEnv$, 
  $\CoKinding \TEnv \Co {\sigma_1\sim\sigma_2}$, and $\TyKinding \TEnv {\sigma_i} \kappa'$
  then, $\CoKinding \TEnv {\Set{\alpha\mapsto \Co}\refl\phi} {\Set{\alpha\mapsto\sigma_1}\phi \sim \Set{\alpha\mapsto\sigma_2}\phi}$
\end{theorem}
\begin{proof}[Proof Sketch of \pref{thm:sfc-coercion-lifting}]
  Proof is by induction on the derivation of the well kinded type $\phi$. In each of the four cases, whenever in the original derivation the rule \trule{co-refl} was used, it is replaced by the derivation of $\CoKinding \TEnv \Co {\sigma_1 \sim \sigma_2}$ % TODO: I am not convinced some how
  % We have 4 cases: 
  % \begin{itemize}
  % \item[\trule{ty-var}] Either the type variable is $\alpha$ and we are done, or it is not and we use \trule{co-refl}.
  % \item[\trule{ty-app}] By induction hypothesis and using \trule{co-comp}.
  % \item[\trule{ty-con}] 
  % \item[\trule{ty-all}]
  % \end{itemize}
  % we have a derivation $\Typing {\TEnv, \alpha\co\kappa'} \phi \kappa$. Then using \trule{co-refl} we obtain $\Typing {\TEnv, \alpha\co\kappa} {\refl\phi} {\phi \sim \phi}$.
\end{proof}

The \trule{co-$\I\forall$} and \trule{co-$\E\forall$} justify coercions between polytypes and their instantations or in other words, if two types are equal, then their instantiations with equal types are also equal. 

\subsubsection{Consistency}
We want to make sure that we can never derive anything obviouly wrong such as !Int ~ Bool! in our language.

\subsection{Operational Semantics}
We will use call by name semantics as GHC uses lazy evaluation. Adjusting the operational semantics to call by value will not change the analysis in any significant manner. We assume Barendregt's convention throughout the presentation. 

\newcommand{\Beta}{
  \ib{\irule[\trule{$\beta$}]
    {};
    {$\stepsto {(\Lam {x\co\tau} M) \App N} {\Set{x\mapsto N}M}$}
  }
}
\newcommand{\TBeta}{
  \ib{\irule[\trule{Ty-$\beta$}]
    {};
    {$\stepsto {(\TLam \alpha M) \App \tau} {\Set{\alpha\mapsto \tau}M}$}
  }
}
\newcommand{\CaseE}{
  \ib{\irule[\trule{case}]
    {};
    {\stepsto {\Case {(K \many\sigma\many\phi\many\Tm)} {\Set{...;  K\App\many\beta\App\many x \to N; ...}}} {\Set{\many {\beta\mapsto\phi}, \many{x\mapsto\Tm}}N}}
  }
}
\newcommand{\CoTransE}{
  \ib{\irule[\trule{Co-Trans}]
    {};
    {$\stepsto {\Cast {(\Cast \Val \Co)} {\nu}} {\Cast \Val {(\trans{\Co} {\nu})}}$}
  }
}

\newcommand{\TyPush}{
  \ib{\irule[\trule{ty-push}];
    % {\Co : {\Forall {c\co\kappa} \tau} \sim \Forall {c\co\kappa} \tau'};
    {$\stepsto {(\Cast{\TLam {\alpha\co\kappa} M}\Co)\App \tau} {({\TLam {\alpha\co\kappa} (\Cast M {\Co\At\alpha})})\App \tau}$}
  }
}

\newcommand{\CoPush}{
  \ib{\irule[\trule{co-push}]
    {\LARGE\substack {\nu\co \sigma_1' \sim \sigma_2'\\
                      \Co_1 : \sigma_1 \sim \sigma_1' = \Left {(\Left \Co)}}}
    {\LARGE\substack {\Co\co (\sigma_1 \sim \sigma_2 \then \sigma_3) \sim (\sigma_1' \sim \sigma_2' \then \sigma_3')\\
                     {\Co_2: \sigma_2 \sim \sigma_2' = \Right{(\Left\Co)}\quad{\Co_3:\sigma_3\sim\sigma_3' = \Right\Co}}}};
    {$\stepsto {(\Cast{\TLam {\alpha\co(\sigma_1\sim\sigma_2)} M}\Co)\App \nu} {\Cast {(\TLam {\alpha\co(\sigma_1\sim\sigma_2)} M)\App (\Co_1 \circ \nu \circ \sym \Co_2)} {\Co_3}} $}
  }
}

\newcommand{\Push}{
  \ib{\irule[\trule{push}]
    {\Co : \tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}
    {\Co_1 = \Right (\Left \Co)}
    {\Co_2 = \Right \Co};
    {$\stepsto {({\Cast {\Lam x M} {\Co}}) \App N} {\Cast {(({\Lam x M})\App {(\Cast N {\sym \Co_1})})} \Co_2}$}
  }
}

\newcommand{\HPush}{
  \ib{\irule[\trule{h-push}]
    {\LARGE\substack{\Co : T\App\many\sigma \sim T\App\many\tau\\
        H : \Forall {\many{\alpha\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \rho \to T\App\many\alpha^n}}}
    {\LARGE\substack{{\Subst = \Set{\many{\alpha_i \mapsto \gamma_i}, \many{\beta_i \mapsto \phi_i}}}\\
        {\Tm'_i = \Cast {\Tm_i} \Subst\rho_i}}}
    {\Co_i = \Right (\Left^{i-1}\Co) }    
    {\phi' =
      \begin{cases}
        \Cast {\phi_i} \Subst(v_1 \sim v_2) &\text{if }\beta_i:v_1 \sim v_2\\
        \phi_i\quad &\text{otherwise}
      \end{cases}
    };
    {$\stepsto {\Case {(\Cast {H\App \many\sigma\App\many\phi\App\many\Tm} \Co)} {\many{\Ptrns \to N}} }
      {\Case {(H\App \many\tau\App\many{\phi'}\App\many{\Tm'})} {\many{\Ptrns \to N}} }$}
  }
}


\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Plain Values} && \Val  &::= \Lam {x\co\tau} M \bnfor \Forall {\alpha\co\kappa} M \bnfor H\\
    \text{CValues}      && C\Val &::= \Val \bnfor \Cast \Val \Co\\

    \text{Evaluation Contexts} &&  \EvalCtxt &::= \EvalCtxtHole{-} \bnfor \EvalCtxt\App M \bnfor \EvalCtxt \tau \bnfor \Cast \EvalCtxt \Co \bnfor \Case \EvalCtxt {\many{P}}\\
  \end{syntax}
  \begin{gather*}
    \fbox{$\stepsto M N$}\\
    \Beta \rsp \TBeta\\
    \CaseE \rsp \CoTransE\\
    \Push \rsp \TyPush\\
    \CoPush\\
    \HPush
  \end{gather*}
  \caption{Operational Semantics of \SFC}
  \label{fig:op-sem-sfc}
\end{figure}

The unusual feature of the system is that the values are stratified into normal values ($\Val$) and cvalues ($C\Val$) that are values with coercion casts respectively. Cvalues are needed to maintain type preservation, which would otherwise break in the presense of casts. The cvalues can step in one of the four ways \trule{push}, \trule{ty-push}, \trule{co-push} or \trule{h-push}. In \trule{push}, the coercion $\Co$, applied to the lambda term, is split so that it is applied to the argument ($\Co_1$) and to the redux reduction ($\Co_2$). Applying this rule exposes a \trule{$\beta$} redux. The rule \trule{ty-push} for type application that moves the coercion inside a type abstraction instantiated at the type variable.
The rule \trule{co-push} is just like \trule{push} but for moving coercions.

The most complex rule is \trule{h-push} which makes more sense with an example in hand. Consider the case scrutinee $\Cast {(Cons\App \texttt{Int}\App x\App y)} \Co$ where, !Cons : FORALL a. a -> [a] -> [a]! and $\Co : [Int] \sim [S\App Bool]$ where !S! is a type constructor. The cast transforms the scrutee into a type $[S\App Bool]$ by pushing the coercion into its subcomponents.
$$
\stepsto {\Cast {(\texttt{Cons}\App \texttt{Int}\App x\App y)} \Co} {\texttt{Cons} \App (S\App Bool) (\Cast x \Right\Co) (\Cast y {(\refl{[]}\Right\Co)})}
$$
Coercion lifting plays an important role here to make sure that the term subcomponents $\Cast M_i {\Subst}\rho_i$ is of the appropriate type. Each rule is derived in a systematic way by making sure the type of the term does not change after moving the cast.

This coercion operation calculus is ofcourse not needed during runtime as these coercions are erased after type checking phase. They are necessary to prove the imporant metatheoritic property of the calculus.

\begin{theorem}[Progress and Subject Reduction (Take 1)]
  If $\Typing \TEnv \Tm \tau$ then, either $\Tm$ is a cvalue or, $\stepsto \Tm \Tm'$ and
  $\Typing \TEnv {\Tm'} \tau$
\end{theorem}

However this is not entirely correct. If our system has a top level axiom such as $coBogus \co \texttt{Int} \sim \texttt{Bool}$, this coercion can be used in a cast, to convert an ill typed term to a well typed term thus breaking subject reduction property. The thoerm needs to be strengthened using an appropriate restriction on $\TEnv$. In general, checking consistency at top level is an undecidable. There exists is a conservative approximation of consistency which can be checked syntactically and it is sufficient for this purpose.

\begin{definition}[\Good $\TEnv$]
  A $\TEnv$ is \Good $\TEnv$ when
  \begin{itemize}
  \item If $\CoKinding \TEnv \Co {T \many\sigma \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $T\many\sigma'$
  \item If $\CoKinding \TEnv \Co {\Forall {\alpha\co\kappa} \sigma \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $\Forall {\alpha\co\kappa} \sigma'$
  \end{itemize}
\end{definition}

\begin{theorem}[Progress and Subject Reduction]\label{thm:progress}
  If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ then, either $\Tm \in C\Val$ or, $\stepsto \Tm \Tm'$ and
  $\Typing \TEnv {\Tm'} \tau$
\end{theorem}


\subsection{Soundness}
So do the static semantics effectively weed out all the programs that may fail at runtime?
  



\subsection{Encoding Language Features}

Armed with the new coercion infrastructure, it is now possible to encode most of the features discussed in \pref{sec:language-features}. \SFC is a conservative extension of \SF, type classes and algebraic datatypes can thus be encoded in \SFC without any extra machinery. GADTs and associated types demand some more details.

\subsubsection{GADTs}

Each GADT data constructor's type is elaborated with equality coercions as arguments in \SFC term.
For example the data constructor !App! from \pref{fig:gadt-example} is elaborated as:
\begin{code}
  App : FORALL a:* b:*. FORALL c:*. FORALL co: c ~ (a -> b). Lam c -> Lam a -> Lam b
\end{code}
A type directed elaboration converts a source level GADT into \SFC as shown in \pref{fig:encoding-gadts}
The source level types are stratified into three groups: polytypes, constrained types and monotypes.
\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Polytypes}         && \pi   &\bnfeq \eta \bnfor \Forall\alpha.\pi\\
    \text{Constrained Types} && \eta  &\bnfeq \tau \bnfor \tau\sim\tau \then \eta\\
    \text{Monotypes}         && v,\tau  &\bnfeq \alpha \bnfor \tau\to\tau \bnfor T\App\many\tau\\
    \text{Constraints}       && C     &\bnfeq \empt \bnfor C, c\co\tau\sim\tau'
  \end{syntax}
  \caption{GADT Surface level type syntax}
  \label{fig:gadt-type-syntax}
\end{figure}
The judgement $\GTranslate C \TEnv \Tm \pi \Tm'$ says that given a surface level term $\Tm$ of type $\pi$, it is elaborated to a term $\Tm'$ into \SFC under the constraints C and typing environment $\TEnv$. The key idea of the elaboration is that the type equality constraints, $\tau\sim\tau'$, are elaborated to coercions in \SFC. The constraint C is a collection of named type equalities. The rules \trule{g-var}, \trule{g-$\I\forall$} and \trule{g-$\E\forall$} are standard rules used for elaborating Hindley-Milner System to \SF while \trule{g-$\I C$} and \trule{g-$\E C$} reminescent of elaborating typeclass constraints. The complex looking \trule{g-alt} elaborates case statements into \SFC. Each surface data constructor is elaborated to the \SFC data constructor with explicit coercions as arguments. The \trule{g-eq} is the same as \trule{cast} rule. The important detail here is that the coercion, $\Co$, is inferred from the constraint context $C$. Constructing the appropriate $\Co$ algorithmically is possible by using a simple unification algorithm based on \cite{lassez_unification_1988}.

\newcommand\GADTVar{
  \ib{\irule[\trule{g-var}]
    {x\co\pi \in \TEnv};
    {\GTranslate C \TEnv x {\pi} x}
  }
}
\newcommand\GADTEq{
  \ib{\irule[\trule{g-eq}]
    {\GTranslate C \TEnv \Tm \tau \Tm'}
    {\CoKinding C \Co {\tau \sim \tau'}};
    {\GTranslate C \TEnv \Tm {\tau'} {\Cast {\Tm'} \Co}}
  }
}
\newcommand\GADTForallI{
  \ib{\irule[\trule{g-$\I\forall$}]
    {\GTranslate C \TEnv \Tm \pi \Tm'}
    {\fresh \alpha {C, \TEnv}};
    {\GTranslate C \TEnv \Tm {\Forall {\alpha\co\star} \pi} {\TLam {\alpha\co\star} \Tm'}}
  }
}
\newcommand\GADTForallE{
  \ib{\irule[\trule{g-$\E\forall$}]
    {\GTranslate C \TEnv \Tm {\Forall {\alpha\co\star} \pi} \Tm'};
    {\GTranslate C \TEnv \Tm {\Set{\alpha\mapsto\tau}\pi} {\Tm'\App \tau}}
  }
}
\newcommand\GADTCI{
  \ib{\irule[\trule{g-$\I C$}]
    {\GTranslate {C,c:\tau\sim\tau'} \TEnv \Tm {\eta} \Tm'};
    {\GTranslate C \TEnv \Tm {\tau\sim\tau'\then\eta} {\TLam {(c\co\tau\sim\tau')} \Tm'}}
  }  
}
\newcommand\GADTCE{
  \ib{\irule[\trule{g-$\E C$}]
    {\GTranslate {C} \TEnv \Tm {\tau\sim\tau'\then\eta} \Tm'}
    {\CoKinding C \Co \tau\sim\tau'};
    {\GTranslate C \TEnv \Tm {\eta} {\Tm'\App\Co}}
  }  
}
\newcommand\GADTAlt{
  \ib{\irule[\trule{g-alt}]
    {\LARGE\substack{
        {H\co \Forall {\many\alpha} {\Forall {\many\beta} {\many{\tau'\sim\tau''} \then \many\tau \to T\many\alpha}}}\quad
        {\many\alpha \cap \many\beta = \varnothing}\quad
        {\fvs{\many\tau, \many{\tau'}, \many{\tau''}} = \fvs{\many\alpha, \many\beta}}\quad
        {\Subst = \Set{\many{\alpha\mapsto v}}}\quad
        {\fresh {\many{c}} {C, \TEnv}}\\
        {\GTranslate {C,\many{c\co\Subst{\tau'}\sim\Subst\tau''}\,} {\,\TEnv,\many{x\co\Subst\tau}\,} \Tm {\tau'} \Tm'} }};
    {\GTranslate C \TEnv {H\App\many x \to \Tm} {T\App\many v \to \tau'}
                   {H\App(\many{\beta\co\star})\App(\many{c\co\Subst\tau'\sim\Subst\tau''})\App(\many{x\co\Subst\tau}) \to \Tm' }}
  }
}

\begin{figure}[ht]
  \centering
  \caption[Encoding GADTs]{Type directed Translation of GADTs into \SFC}
  \begin{gather*}
    \fbox{$\GTranslate C \TEnv {\Tm} {\pi} {\Tm'}$}\\
    \GADTVar \rsp \GADTEq\\
    \GADTForallI \rsp \GADTForallE\\
    \GADTCI \rsp \GADTCE
  \end{gather*}
  \begin{gather*}
    \fbox{$\GTranslate C \TEnv {p \to \Tm} {\pi \to \pi} {p' \to e'}$}\\
    \GADTAlt
  \end{gather*}
  \label{fig:encoding-gadts}
\end{figure}

\begin{lemma}[Type Preservation]
  If $\GTranslate C \empt \Tm \tau {\Tm'}$ then $\Typing C {\Tm'} \tau$
\end{lemma}
Type preservation is an important sanity check for the elaborations. It says that the the elaborated terms have the same type that was inferred at surface level.

\begin{theorem}[GADT Consistency]
  If $\dom\TEnv$ is type variable and coercion constant free, and $\CoKinding \TEnv \Co {\tau\sim\tau'}$ then $\tau = \tau'$ i.e. they are syntactically identical.
\end{theorem}

All GADT programs are sound in \SFC.

\begin{theorem}[GADT Soundess]
  If $\GTranslate \empt \empt \Tm \tau \Tm'$, then $\manystepsto {\Tm'} \Val$ iff $\manystepsto {\compile\Tm} \Val$ where $\Val$ is some value of ground type and $\compile\Tm$ is type erased term.
\end{theorem}

\subsubsection{Associated Types}
Elaborating associated types to \SFC is very similar to translating GADTs. The constraint context now contains class predicates along with equality predicates. This extension is also needed if GADT data constructors need to constrain certain type parameters. The equality constraints can now appear not only in the context of GADT data constructors but any term type.

\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Class Declarations} &&C_{ls} &\bnfeq \textbf{\texttt{class }} D\App\many\alpha \textbf{\texttt{ where }} \many{dsigs}; \many{sigs}\\
    \text{Instance Declarations} &&I_{nts} &\bnfeq \textbf{\texttt{instance }} D\App\many\tau \textbf{\texttt{ where }} \many{adata}; \many{val}\\
    \text{Associated types} &&dsigs &\bnfeq \textbf{\texttt{type }} \tau\\
    \text{Method signatures} &&vsigs &\bnfeq x\co\tau\\
    \text{Assoicated type instance} &&asigs &\bnfeq \tau = \sigma\\
    \text{Method bindings} &&asigs &\bnfeq x = \Tm
  \end{syntax}
  \caption[Class Syntax]{Class and Associated Types Surface Syntax}
  \label{fig:assoc-types-syntax}
\end{figure}


The judgement $\DTranslate C {D\App\tau} d$ elaborates the predicate $D\App\tau$ to a dictonary $d$ in \SFC.
The \trule{subst} allows replacing type class parameters with equal types. This is necessary to account for associated types that appear in the type class signature. $D\App\tau$ may contain an associated type, and depending on the instantiation of the free type variables in $\tau$ an appropriate coercion can be used to justify casting the dictonary $d$ to the corresponding actual type.

$$
\ib{\irule[\trule{subst}]
  {\DTranslate C {D\App\tau} d}
  {\CoKinding C \Co {D\App\tau \sim D\App\tau'}};
  {\DTranslate C {D\App\tau'} {\Cast d \Co}}
}
$$

Reconsider the !Coll c! typeclass and a function !sumColl :: (Coll c, Num (Elem c)) => c -> Elem c!, which sums up all the elements of the collection !c!. The predicate !Num (Elem c)! constraints the elements of the collection so that they can be summed up. A resonable use of !sumColl! would be at type ![Int]!, as integers can indeed be summed up. This would result in instantiating the type parameter !c! with ![Int]!. The rule \trule{subst} in this case justifies the use of !Num Int! dictonary as if it was a !Num (Elem [Int])! dictonary.

Each class declaration introduces a new class predicate name in the type environment along with its method names. With associated types, a new ``type function'' name---!Elem c! in the case of !Coll c!---is also added to the type environment.
Each instance introduces a new axiom into the typing environment. For the !Coll c! typeclass the instance !Coll [Int]! introduces the coercion !CoColl : Elem [Int] ~ Int!. In general, each instance generates an axiom of the form !co: (FORALL $\many{\alpha\co\star}$. F $\sigma$) $\sim$ (FORALL $\many{\alpha\co\star}$. $\sigma'$)! where $\fvs\sigma = \many\alpha$
and $\fvs{\sigma'} \subseteq \many\alpha$. These axioms induce a rewrite function on types and are also called as rewrite axioms.


\begin{theorem}[Associated Type Consistency]
If $\TEnv$ contains type rewrite axioms that are confluent and terminating, then $\TEnv$ is consistent.
\end{theorem}


\subsubsection{Newtypes}
Newtypes introduce a new coercion axiom in the system. For the generative type !newtype Fun = MkFun (Fun -> Fun)!
the associated coercion axiom will be !CoFun : Fun ~ (Fun -> Fun)!. In general, for any generative type declaration of the form !newtype T $\many\alpha$ = MkT $\tau$! will be of the form !CoT : T $\many\alpha$ ~ $\tau$! where $\tau$ may contain $\many\alpha$ as free type variables.


\subsubsection{Open Type Functions}
Each type family instance directly translates to axioms. Thus for !Add m n! type function, the two associated instances would introduce the following axioms !CoAddZ : Add Z n ~ n! and !CoAddSmn : Add (S m) n ~ S (Add m n)!


\section{\SFR}\label{sec:sfr} % R for roles

Feature of type safe generative types aka newtypes

The problem is mixing parametricity and type level dispatch.

Type families don't mix well with newtypes as type families make a distinction between types
even if they have the same representations.


\begin{figure}[ht]
  \caption{Terms and Types of \SFR as an extension of \SFC}
  \label{fig:system-fcr-syntax}
\end{figure}

\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}
  \end{gather*}
  \caption{Static Semantics of \SFR}
  \label{fig:sfr-typing}
\end{figure}




\section{\SFP}\label{sec:sfp} % P for promotion
We want to promote datatypes to kinds. What does it take?
\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Type Vars} &&\alpha,\beta,\gamma \\
    \text{Term Vars} &&x,y          \\
    \text{Kinds}     &&\kappa                  &::= \star \bnfor \kappa \to \kappa \bnfor \syntaxhl{\tau \sim \sigma}\\
    \text{Types}     &&\tau,\sigma,\gamma,\nu  &::= \alpha \bnfor T \bnfor \syntaxhl{F} \bnfor \tau \to \tau \bnfor \Forall {\alpha\co\kappa} \tau\\
    &&                        &\syntaxhl{\bnfor \sym \Co \bnfor \trans \nu \Co \bnfor \Co\At\tau \bnfor \Left \Co \bnfor \Right \Co} \\
    &&                        &\syntaxhl{\bnfor  \Cast \Co \Co}\\
    \text{Terms}     &&M,N                     &::= x \bnfor H \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau
  \end{syntax}
  \caption{Terms and Types of \SFP as an extension of \SFC}
  \label{fig:system-fcp-syntax}
\end{figure}


\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}
  \end{gather*}
  \caption{Static Semantics of \SFP}
  \label{fig:sfp-typing}
\end{figure}





\section{\SFK}\label{sec:sfk} % K for kind eq
We have type equalities, why not kind equalities?
But we would then have two kinds of equalities: type and kind.
So why not just squish types and kinds together, making it truly impredicative

\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Type Vars} &&\alpha,\beta,\gamma \\
    \text{Term Vars} &&x,y          \\
    \text{Kinds}     &&\kappa                  &::= \star \bnfor \kappa \to \kappa \bnfor \syntaxhl{\tau \sim \sigma}\\
    \text{Types}     &&\tau,\sigma,\gamma,\nu  &::= \alpha \bnfor T \bnfor \syntaxhl{F} \bnfor \tau \to \tau \bnfor \Forall {\alpha\co\kappa} \tau\\
    &&                        &\syntaxhl{\bnfor \sym \Co \bnfor \trans \nu \Co \bnfor \Co\At\tau \bnfor \Left \Co \bnfor \Right \Co} \\
    &&                        &\syntaxhl{\bnfor \Cast \Co \Co}\\
    \text{Terms}     &&M,N                     &::= x \bnfor H \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau
  \end{syntax}
  \caption{Terms and Types of \SFK as an extension of \SFC}
  \label{fig:system-fck-syntax}
\end{figure}


\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}
  \end{gather*}
  \caption{Static Semantics of \SFK}
  \label{fig:sfk-typing}
\end{figure}


\begin{figure}[ht]
  \centering
  \begin{tabular}[ht]{c | c}
    Parametric Features & Non-parametric Features \\
    \hline
    Modules             & Typeclasses (with Functional Dependencies)\\
    Algebriac Datatypes & Generalized Algebraic Datatypes\\
                        & Type Families (Open, Closed, Associated types)\\
                        & Generative Types (newtypes)
  \end{tabular}
  \caption{Features of Haskell}
  \label{fig:haskell-lang-features}
\end{figure}

\section{Conclusion and Future work}\label{sec:conclusion}
Using non-parametric features in a context that assumes parmetricity is a root cause of type unsoundess ``bugs''.

% \begin{figure}[ht]
%   \centering
%   \begin{tabular}[ht]{c | c | c | c | c | c | c}
%     Language & Decidable Type checking & ADTs & GADTs & Open/Closed Type functions & Generative Types & Kind Functions\\
%     \hline
%     \SF  & & & & &\\
%     \SFC & & & & &\\
%     \SFP & & & & &\\
%     \SFK & & & & &\\
%   \end{tabular}
%   \caption{Core languages and their Capabilities}
%   \label{fig:language-features}
% \end{figure}

%%%% Bibliography
\newpage
\bibliography{comp}


\end{document}

%%% Local Variables:
%%% mode: latex
%%% eval: (visual-line-mode 1)
%%% eval: (auto-fill-mode 0)
%%% TeX-master: t
%%% TeX-command-extra-options: "--synctex=1"
%%% End: