\newif\ifcomments\commentstrue

\RequirePackage[svgnames,dvipsnames,prologue,x11names]{xcolor}

\documentclass[manuscript,screen,nonacm]{acmart}

\usepackage{comp}

\title{Practical Functional Programming in \SFC and its extensions}
% \subtitle{Extensions to System F}

\author{Apoorv Ingle}
%\orcid{0000-0002-7399-9762}
\affiliation{%
  \institution{University of Iowa}
  \department{Department of Computer Science}
  \streetaddress{McLean Hall}
  \city{Iowa City}
  \state{Iowa}
  \country{USA}}
% \keywords{typeclass, type family}

\begin{document}

\begin{abstract}
  This report is a juxtaposition of \SFC and its extensions with its theoretical developments. We begin with some standard features of a strongly typed functional programming language, followed by a formalization of \SFC, which is in essense is \SF with explicit type equalities. The repercussions of adding type equalities as a core language feature is extensive as  it can efficiently encode a diverse set of language features which were previously not possible in \SF. We then study some extensions of \SFC: \SFR which makes type equality finer grained, \SFP which makes the kind system more expressive and finally, we discuss \SFK which enables expressing kind level equalities by squashing the distinction between types and kinds.
\end{abstract}

\maketitle
\pagestyle{plain}

\section{Features of Typed Functional Programming Languages}\label{sec:language-features}
%% The point to make here is type checker can enforce contraints!
\subsection{User Defined Datatypes}
Aiding organization of related data together is important feature of any programming language. The structures, which store information, represent the domain elements. The transformations on these structures then, models the program logic. Algebraic datatypes are a primary way to define such new structures in a functional programming language.

\subsubsection{Algebraic Datatypes}
Algebraic Datatypes (ADT) can be viewed as composite datatypes of named sums of products. They enhance the expressivity of the language by allowing users to declare their own structures by possibly re-using the existing ones. A declarative style of defining new datatypes was first introduced in the programming language HOPE\cite{burstall_hope_1980}. As an example, a binary tree in Haskell can be declared as:

\begin{CenteredBox}
\begin{code}
data Tree c = Leaf c
            | Branch (Tree c, Tree c)
\end{code}
\end{CenteredBox}

The keyword !data! introduces a new user defined agebraic datatype with name !Tree! parameterized over any type !c!. A value of type !Tree c! can be defined using the data constructors, !Leaf! and !Branch!. A !Leaf! is a terminal node that contains the value of type !c!, while a !Branch! contains a pair of sub trees. The data constructors themselves can be used as functions with the type of\, !Leaf :: FORALL a. a -> Tree a! and\, !Branch :: FORALL a. (Tree a, Tree a) -> Tree a!. It is important to observe that the structure the datatype does not dependent on the type parameter !c!, i.e. it is used parametrically, or the shape of !Tree Int! and !Tree Float! is the same. Parametricity allows programs to be general and hence reusable. In an untyped programming language, a predicate is necessary to check that certain structural invariants hold: The !Branch! case has two sub trees. In a typed setting the typechecker can check such invariants and reject ill-structured data: the term !Branch (Tree c)! is ill-typed. An alternative syntax to write the !Tree c! data type would be by type signature explication for each of the data constructors.

\begin{CenteredBox}
\begin{code}
data Tree c where
    Leaf   :: c                -> Tree c
    Branch :: (Tree c, Tree c) -> Tree c
\end{code}
\end{CenteredBox}

ADTs open avenues for writing a clean domain specific application. For example, consider a simple calculator application that performs addition operation on integers. In this case, the domain is the algebraic expression. One can declare an encoding for algrebraic expressions using an ADT as follows:

\begin{CenteredBox}
\begin{code}
data AlgExp a where
    Value  ::  a                        -> AlgExp a
    Plus   ::  AlgExp Int -> AlgExp Int -> AlgExp a
\end{code}
\end{CenteredBox}

The data constructor !Value! encodes values such as integers while !Plus! encodes the operation of adding the two expressions. Now, the function of the calculator, i.e. to compute algebraic expressions, is to evaluate the encoded expression. This can be perfromed by means of an !eval! funtion defined recursively as shown below.

\begin{CenteredBox}
\begin{code}
eval :: AlgExp Int -> Int
eval (Value i)  = i
eval (Plus x y) = (eval x) + (eval y)
\end{code}
\end{CenteredBox}

The declarative style of programming allows us to write the !eval! function on a case by case basis. The first case says that if we have a !Value!, we can deduce that it is an !Integer! and just return it unaltered. In the second case of !Plus! we first evaluate the expressions !x! and !y! and then return the addition of the evaluations. Another advantage of having a declarative style is that extending the calculator application, say to include more binary operations, such as multiplication and division operation, is a simple matter. It involves adding two new data constructors !Mult! and !Div!, similar to !Plus!, in the data declaration followed by extending the !eval! function with cases for !Mult!, which multiplies, and !Div! that divides.


\subsubsection{Generalized Algebraic Datatypes}
Consider an extension of the previously defined !AlgExp! to include an explicit predicate that checks if the expression is equal to zero, !IsZero!.

\begin{minipage}[ht]{0.6\linewidth}
\begin{code}
data AlgExp a where
    Value  ::  a                    -> AlgExp a
    Plus   ::  AlgExp a -> AlgExp a -> AlgExp a
    IsZero ::  AlgExp Int           -> AlgExp a
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
eval :: AlgExp Int -> Int
eval (Value i)  = i
eval (Plus x y) = (eval x) + (eval y)
eval (IsZero x) = (eval x) == 0 -- Type error!
\end{code}
\end{minipage}

We also need to extend the !eval! function that evaluates the !IsZero! case. How should the !eval! function handle the !IsZero! case? Using the staight forward definition of !(eval x) == 0! fails to type check as the eval function requires the return type of the expression to be an !Int! but the expression is of type !Bool!. Another possible solution is to change the return type of the the !eval! function to instead be !Either Int Bool! which means that the evaluator either returns an !Int! or a !Bool!. While this would work as expected, it necesitates a complete rewrite of the !eval! function. The situation of maintaining this !eval! function would become even more cumbersome if later on we had the need to add a facility to store and use user defined functions. Addition of each such new construct would mean nesting of !Either! datatype and then checking at each recursive call which value is returned.

All we really want is an evaluator that evaluates the expression. Ideally the type signature of the !eval! function
should be agnostic of the expression type: !eval :: FORALL a. AlgExp a -> a!. The crux of the problem is that the current data constructors !AlgExp! all have a generic return type of !AlgExp a!. The problem would disappear if we were able to constraint the return type of each of the data constructors.

\begin{minipage}[ht]{0.6\linewidth}
\begin{code}
data AlgExp a where
    Value  ::  a                        -> AlgExp a
    Plus   ::  AlgExp Int -> AlgExp Int -> AlgExp Int
    IsZero ::  AlgExp Int               -> AlgExp Bool
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
eval :: AlgExp a -> a
eval (Value i)  = i
eval (Plus x y) = (eval x) + (eval y)
eval (IsZero x) = (eval x) == 0 -- Okay!
\end{code}
\end{minipage}%


Here we are able to not just constrain the structure of the data but also its return type.



Reusablity due to parametricty however, comes at a cost of inexpressivity. Real world programs such as polymorphic serialization of data either requires the programmers to provide the cannonical forms of representation \cite{herlihy_value_1982} to be used during run-time or require runtime type analysis\cite{harper_compiling_1995}. Parametric algebraic datatypes are insufficent as they provide no facility to pass the type parameters at runtime. Generalized algebraic datatypes liberate this limitation by capturing type constraints with in the structure of the type. They have been studied under various names such as first class phantom types\citep{cheney_first-class_2003}, guarded recursive datatypes\cite{xi_guarded_2003}, equality qualified types\cite{sheard_meta-programming_2008}. Consider an example that models simply typed lambda calculus and a single step call by name evaluator as shown \pref{fig:gadt-example}.
\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
data LamTm a where
  Var :: String -> LamTm a
  Lam :: (LamTm a -> LamTm b) -> LamTm (a -> b)
  App :: LamTm (a -> b) -> LamTm a -> LamTm b
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      eval :: LamTm a -> LamTm a
      eval v@(Var _) = v
      eval v@(Lam _) = v
      eval (App (Lam fun) arg) = fun arg
      eval (App fun arg) = let fun' = eval fun
                           in App fun' arg
    \end{code}
  \end{minipage}
  \caption{Expressing Lambda Terms as GADT}
  \label{fig:gadt-example}
\end{figure}
Writing an !eval! function for a !LamTm a! would be cumbersome if it were not for the constrainted type arguments of data constructors !Lam! and !App!. The local type constraints on !Lam! are necessary to convince the type checker that the application term !fun arg! is well typed, with the type of !fun! being !LamTm a -> LamTm b! and the type of !arg! being !LamTm a!.

% \subsection{Modules and Data Abstraction}
% Code modularity aids writing bug free programs. Modules help in achieveing code modularity by allowing programmers write smallish independent programs. The complete program is then a composition of the modules. Modules were first implemented as extensions to Algol 68. They aid in maintaining separation of concerns by not just organizing related program units together, but by further allowing some definitions to be opaque to the programs that are not inside the module. A principled way of hiding certain subprograms aids in data abstraction and separate compilation.

\subsubsection{Generative Abstract Types}
Generative types provides a mechanism for the programmer to confine the visibility of the representation details of a type exclusively within a module. External to the module the client program cannot know how the type is represented or in other words, representation of the generative types is opaqaue to the client module. In ML generative types are defined by hiding the type declarations in the signatures. Consider an HTML module defined in ML as shown in \pref{fig:ml-generative-type}.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      module HTML (Html, mkHtml, unMkHtml)
      where

      data Html = Html String

      mkHtml :: String -> HTML
      mkHtml = Html . escapeString

      unMkHtml :: HTML -> String
      unMkHtml (Html s) = s
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      module Client
      where

      import HTML

      page :: HTML
      page = mkHTML "<html><body>Text</body></html>"



    \end{code}
  \end{minipage}
  \caption{HTML Signature and Structure in ML}
  \label{fig:ml-generative-type}
\end{figure}

Inside !HTMLImpl! the types !String! and !HTML! are synonymous, however any client using !HTML! will not be able to directly manipulate a value of type !Html!---unless of course by using the specific functions exposed in the signature. Inhibiting manipulations and views of the data representation can be useful for avoiding leaking information in secure computation contexts. This (naive) abstraction however comes with a runtime cost. !Html! and !String!, need to be explicitly converted from one form into another, although they have the same representation in memory.

% Haskell also provides generative types by the means of declaring a data type using a !newtype! keyword. Hiding the default data constructors of the newtype, would in effect declare a generative abstract type.


% \subsection{Intrinsic Typing vs Extrinsic Typing} % Principal typing
% Intrinsic typing: When the type is tied together with its term. This makes typechecking easy as there is not really any inference bit to peform but makes terms ``rigid''. This rigidity has philosophical implications.
% Extrinsic: The type is loosly tied with the term.

% Principal typing: Given a term one can infer the most general type for that term.
% as a decision problem, infering a principal type for a term in general is undecidable.
% System F has undecidable type inference\cite{wells_typability_1999}.

% But not all hope is lost. Hindley-Milner (HM) type system, which is a restriction of System F, has a decidable type inference algorithm. HM is expressive enough for most practical programming purposes; ML and Haskell are all based on HM type systems.
% Various works have tried to find the sweat spot of what is the minimal type annotation needed for the type inference.
% The idea is that we need to reduce the programmer burden of writing down the types where it is ``obvious''.
% Philosophically, if the system has a principal typing property then the distinction between intrinsic types and extrinsic types disappers as we can use an algorithm to infer the type of the term and decorate the original term with the infered types to recover the intrinsic term. In a sense, there is a galois connection between the intrinsic term and its extrinsic counterpart. The type inference mapping takes an term from extrinsic to an intrinsic while type erasure mapping takes an extrinsicly typed term to intrinsicly typed term

% \subsection{Treatment of Equalities}
% There are two ways of remembering type equalities that the typechecker would have proved or the ones provided by the user: Store them as terms or store them as types. Both of them seem to be equally attractive however they have significant computational differences

% Storing them as term makes the type equality coercions explicit in the code, helps type checking. For a proof assistant it would be an attractive feature, but for a programming language it would mean that we carry such
% type equality proofs into the compiled language. Dynamic type dispatch is a an important application for being able to carry proofs about type equalities.

% Storing them as types has an advantage that we can erase the equality proofs before compiling it to a more efficient runtime representation. After all, type equalities are only really needed by the type checker to prove that nothing will go wrong at runtime. At runtime we don't need it.
%  Intensional vs extensional % Intensional vs Extensional typing

\subsection{Polymorphism}
\subsubsection{Parametric Polymorphism}
Certain functions behave uniformly on the data that they might receive. An archetypical example is that of an identity function $id$ that just takes an value and returns it unmodified.

\begin{CenteredBox}
  \begin{code}
    id : FORALL a. a -> a
    id a = a
  \end{code}
\end{CenteredBox}
Such functions are said to be parametrically polymorphic over their type arguments\cite{strachey_fundamental_2000}.
Agebraic datatypes are also parametrically polymorphic over their type parameters.

\subsubsection{Ad-hoc Polymorphism}
In contrast to the term !id!, consider the following two terms:

\begin{CenteredBox}
  \begin{code}
    t1 : Int = 1 + 2
    t2 : Float = 1.1 + 2.3
  \end{code}
\end{CenteredBox}
In the first case we see that the operator !+! is applied to two integers, but in the second case it is applied to two floating point numbers. Although the programmer uses the same symbol, the meanings of each of the programs is completely different. One adds two !Int! and returns an !Int! while the other adds two !Float! and returns a !Float!. Further, the compiled code for each would also be different. The runtime code would make a call to two different built in constructs. Such kind of function reuse, which is dependent on the context or the type of arguments, is called as ad-hoc polymorphism\cite{strachey_fundamental_2000}. Implicit operator overloading is a general mechanism to impliment ad-hoc polymorphism where the compiler resolves the overloaded operator to the actual operator.

\subsection{Typeclasses}
\subsubsection{Operator overoading}
To make implicit overloading practical \citet{wadler_polymorphism_1989} proposed a dictonary passing style by using a mechanism. This technique became the foundation for Haskell\cite{haskell_2010} typeclasses. \cite{kaes_parametric_1988} had similar ideas related to implimentation aspects of ad-hoc polymorphism before Wadler et al. Implicit objects\cite{oliveira_typeclasses_2010} provide a mechanism for users to choose the intended behavior instance if there are multiple available options. This pushes the burden of choosing the right instance on programmer whenever the typechecker cannot figure out the right option or when the user wants to force the typechecker to resolve it to a specific instance.

Typeclasses can be viewed as relations over types and every instance declaration extends that relation. Consider a typeclass !Ord a! shown in \pref{fig:tc-ord}. It represents a unary relation for the types whose values can be compared. Instances of the typeclass !Ord! are written !Ord Int! and !Ord Float! say that !Float! and !Int! belong to the unary relation !Ord!.

%% Something about methods

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Ord a where
         (<)  :: a -> a -> Bool
         (<=) :: a -> a -> Bool
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Ord Int where
         (<)  = int_le
         (<=) = int_leq
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Ord Float where
         (<)  = le_float
         (<=) = leq_float
    \end{code}
  \end{minipage}
  \caption{\lstinline{Ord} typeclass and instances}
  \label{fig:tc-ord}
\end{figure}
\subsubsection{Multi-Parameter Typeclasses and Functional Dependencies}
A straightforward generalization of single parameter typeclases is multiparameter typeclasses. It naturally extends the idea of having n-ary propositions or relations over types. Such relations can be useful to express properties such as containment relations. For example, following\cite{jones_tcfd_2000}, a typeclass to unify different containers under a single interface can be defined using a !Coll e c! typeclass, as shown in \pref{fig:tc-collection}. One can imagine having instances for such a typeclass as \lstinline{Coll Int [Int]}, !Coll Float (BST Float)!, etc. Speaking in terms of relations, we say that !(Int, [Int]) $\in$ Coll! and similarly !(Float, BST Float) $\in$ Coll!.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Coll e c
      where
         empty :: c
         insert :: e -> c -> c
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Coll Int [Int]
      where
         empty = ...
         insert = ...
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.3\linewidth}
    \begin{code}
      class Coll Float (BST Float)
      where
         empty = ...
         insert = ...
    \end{code}
  \end{minipage}
  \caption[Coll typeclass]{\lstinline{Coll} typeclass and its instances}
  \label{fig:tc-collection}
\end{figure}

The use of !empty! in a polymorphic setting however interferes with the compilation decision. It is not enough to determine what the type of !e! should be just by knowing the type !c!. Such types are called ambiguous types. Formally, if a type variable only appears in the constraints of the type it is called an ambiguous type. The type of !empty!, !FORALL e c. Coll e c => c!, is ambiguous due to the occurance of !e! only the constraint. Such types do not have a well defined semantics as the compiler cannot make a unique choice of what the compiled code should be. The problem is here is that the typeclasses are relations and hence are too general for this use case. To make typeclasses behave in a more constrained way, Jones introduced functional dependencies\cite{jones_tcfd_2000}. The insight is to be able to specify the functional nature of relations, i.e. to be able to say if we \emph{know} certain type parameters of the typeclass, we can \emph{determine} the other type parameters as well.

\begin{figure}[ht]
  \begin{CenteredBox}
    \begin{code}
      class Coll e c | c ~> e where
      empty :: c
      extend :: e -> c -> c
    \end{code}
  \end{CenteredBox}
  \caption[Coll typeclass]{\lstinline{Coll} Typeclass with Functional Dependency}
  \label{fig:tc-collection-fd}
\end{figure}

The extra annotation !c ~> e! on the typeclass !Coll e c! as shown in \pref{fig:tc-collection-fd} says that !e! can be uniquely determined for given a !c!.
We call !c! to be the determiner and !e! to be the determinant of the functional dependency.

% improvement, simplification
%

\subsection{Type Functions}
\subsubsection{Associated Types}
The main reason to have multiparameter type classes with functional dependencies was to enable the type system to express type functions. In other words, if the determiner type variables of the functional dependency determine the determinant type variables, then it would be syntactically pleasing to write them in a function form.
Using our previous !Coll c e! example, if !c ~> e! then, it would be more obvious to the programmers to instead write !Elem c! instead of !e!, where !Elem c! is a special type function that takes in a type and returns a type. This effectively gives an alternative route to express functional dependencies.
\begin{figure}[ht]
  \begin{center}
    \begin{minipage}[ht]{0.4\linewidth}
      \begin{code}
        class Coll c where
           type Elem c
           empty :: c
           extend :: Elem c -> c -> c
      \end{code}
    \end{minipage}%
    \begin{minipage}[ht]{0.4\linewidth}
      \begin{code}
        instance Coll [Int] where
           type Elem [Int] = Int
           empty = ...
           extend = ...
      \end{code}
    \end{minipage}
  \end{center}
  \caption[Coll typeclass]{\lstinline{Coll} Typeclass with Type Function}
  \label{fig:type-fam}
\end{figure}

In \pref{fig:type-fam}, the typeclass !Coll! has an associated type function !Elem! that depends on the type parameter !c! of the typeclass.
% \subsubsection{Closed Type Functions}
% There maybe cases where the type functions have to be restricted extensions. For example The previously defined
% !Add! type function is supposed to be defined only on !Z! and !S n! types. Closed type families help in this situtation.
% \begin{figure}[ht]
%   \begin{minipage}[ht]{0.4\linewidth}
%     \begin{code}
%       type family Add m n where
%           Add Z n = n
%           Add (S m) n = S (Add m n)
%     \end{code}
%   \end{minipage}
%   \caption{Addition with type functions}
%   \label{fig:closed-type-fun-add}
% \end{figure}

\section{\SFC}\label{sec:sfc}
The Glasgow Haskell Compiler (GHC)\cite{ghc_2020}, is a widely used Haskell\cite{haskell_2010} compiler. Its inception was a result of a lack of a test bed for lazy strongly typed functional programming languages. It is an industry grade compiler, meaning the compiler generated code can perform with efficiency comparable to other language compilers like Java or C++. The original core language of GHC was based on \SF. However, it soon became clear that with the addition of GADTs and generative types, using pure \SF would be too cumbersome, if not impossible to express in some constructs. For example consider a generative type declaration:

\begin{CenteredBox}
  \begin{code}
    newtype Fun = MkFun (Fun -> Fun)
  \end{code}
\end{CenteredBox}
The intension of this declaration is to make !Fun! and !Fun -> Fun! be isomorphic, however, it there was no good way to encode this in pure \SF. GHC relied on a hack to express this in the core language.
\SFC\cite{sulzmann_system_2007} was desiged and implimented to solve this problem. In the following sections we take a look of the core language, followed by how it can be used to encode all the features discussed in \pref{sec:language-features}.
% In one statment: \SFC is an intensional intrinsically typed programming language.
% It is intensional, meaning each term encodes its typing derivation, and it is intrinsically typed, meaning all the terms are index by types.

% Some key points to cover:

% Store the equality between types explicitly in the AST during type checking.

% New feature: coercion is a type and its kind tells us what types does the coercion equate.

% Features that can be directly expressed in \SFC: New types or generative types, associated types, functional dependencies, generalized algebraic datatypes (GADTs).

% Brand new feature user defined open type functions.

% Makes type rewriting complicated. But makes the type system more expressive by making it extensible.
% Proving type soundness is a bit more involved now.

% why don't we have a rule that says: if $\sigma_1 \sim \sigma_2$ then

\subsection{Syntax}
\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Type Vars} &\TyVar,\beta,\Co  &\qquad\text{Type constants} &T \\
    \text{Term Vars} &x,y                  &\qquad\text{Indices}        &i,n \in \mathbb{N} \\
    \text{Coercion Vars} &c & &
  \end{syntax}
  \begin{syntax}
    \text{Kinds}     &&\kappa       \bnfeq& \star \bnfor \kappa \to \kappa \bnfor \sigma \sim \tau\\
    \text{Types}     &&\tau,\sigma  \bnfeq& \TyVar \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor F_n\\
    \text{Coercions} &&\nu,\Co      \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\nu\Co % equiv relation
                                        \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
                                        \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co\\  % compose/decompose
    \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
    \text{Patterns}  &&P    \bnfeq& H\App \many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
    \text{Terms}     &&M,N  \bnfeq& x \bnfor  \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co\\

    \end{syntax}
    \begin{syntax}
    \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
    \text{Substitutions}  &&\Subst       \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
  \end{syntax}

  \begin{syntax}
    \text{Program} &&P_{gm} \bnfeq& \many{D_{cl}} \mathrel{;} \many{\Tm}\\
    \text{Data Declarations} &&D_{cl} \bnfeq& \textbf{\texttt{data }}\App T\co\many{\kappa} \to \star\App \textbf{\texttt{ where }}\App \many{C_{trs}(T)} \\
                             &&       \bnfor& \textbf{\texttt{type }}\App F_n : \many\kappa^n \to \kappa\\
                             &&       \bnfor&  \textbf{\texttt{axiom }}\App C\App \many{\TyVar\co\kappa} : \sigma_1 \sim \sigma_2\\
    \text{Data Constructors} &&C_{trs}(T) \bnfeq& H : \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \many{(\tau \sim \tau)} \then \many\sigma \to T\many\TyVar}\\
  \end{syntax}

  \caption{The Syntax of \SFC}
  \label{fig:system-fc-syntax}
\end{figure}

\SFC is impredicative; there is no stratification between polytypes and monotypes. Unlike \SFw, there are no lambdas at type level this makes the type level calculus less expressive by disallowing certain types such as $\Lam a {T\App a\App Int}$. The system does allow higher kinded types---either built or user defined---that can be used by the type application form $\tau\App\tau$. \AI{what is the difference between $\Forall a (a, a)$ and $\Lam a. (a, a)$?} The value type constructors $T$ ranges over built-in types such as !Int! and user defined types such as algebraic datatypes. Declaration of algebraic datatypes introduces data constructors $H$, the types of which are of the form:
$$
H \co \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa}} \many{\sigma} \to T \many\TyVar}
$$
Here the type variables $\many\TyVar$ appear in the same order as in the algebraic datatype delcarations,  and type variables $\many\beta$ are the existential; they do not appear in the return type. This enforcement is necessary to avoid any type variable to escape its scope. The existential type variables play a crucial role in encoding GADTs.

The type equality coercion, $\Co$, is the essential additional construct. Coercions are types and have a ``cannonical'' form of $\tau\sim\sigma$. \AI{TODO: Are coercions types or are types coercions?}. They can be constructed, applied, passed around as arguments using the special coercion infrastructure at the level of types. They appear at term level in the form of casts $\Cast M \Co$. For type theory enthusiasts, type coercions should be thought of as extensional type equality, meaning, if a term $M$ has a type $\tau$ and we have a type coercion $\tau\sim\sigma$, then the type cast $\Cast M  {\tau\sim\sigma}$ says that $M$ can be treated as if it has type $\sigma$. The soundness property of the type system guarantees that after the types (including casts) have been erased, the program will not crash or get stuck. Unlike other regular types, however, coercions do not classify values, i.e. there are no term level constructs that have a type $\tau\sim\sigma$. This is similar to how in \SFw there are no values of a type that has a kind $\STAR\to\STAR$.

\SFC supports a full fledged coercion calculus where each syntactic construct corresponds to a logical equation. The most basic type of coercion can be produced by using the reflexivity construct $\refl\tau$. It says that the type $\tau$ witness the (obvious) fact that it is equal to itself. The other constructs on coercions are symmetry '$\Sym\Co$', transitivity '$\trans {\Co_1}{\Co_2}$', which together with $\refl\tau$, makes type equality an equivalence class. Coercions can also be composed and decomposed using the '$\Co_1\Co_2$', and '$\Left\Co$' and '$\Right\Co$' constructs respectively. Finally, coercion abstraction '$\Forall {\TyVar\co\kappa}\tau$' and coercion application '$\Co\At\tau$' aids equality reasoning on polytypes.

\subsection{Static Semantics}

\newcommand\TCast{
  \ib{\irule[\trule{cast}]
    {\Typing \TEnv {\Tm} {\tau}}
    {\CoKinding \TEnv \Co {\tau \sim \sigma}};
    {\Typing \TEnv {\Cast \Tm \Co} {\sigma}}
  }
}

\newcommand\KReflCo{
  \ib{\irule[\trule{co-refl}]
    {\TyKinding \TEnv \tau \kappa};
    {\CoKinding \TEnv {\refl \tau} {\tau \sim \tau}}
  }
}

\newcommand\KSymCo{
  \ib{\irule[\trule{co-sym}]
    {\CoKinding \TEnv \Co {\tau \sim \sigma}};
    {\CoKinding \TEnv {\Sym \Co} {\sigma \sim \tau}}
  }
}

\newcommand\KTransCo{
  \ib{\irule[\trule{co-trans}]
    {\CoKinding\TEnv {\Co_1} {\tau \sim \tau_2}}
    {\CoKinding\TEnv {\Co_2} {\tau_2 \sim \sigma}};
    {\CoKinding\TEnv {\trans {\Co_1} \Co_2} {\tau \sim \sigma}}
  }
}

\newcommand\KInstCo{
  \ib{\irule[\trule{co-$\E\forall$}]
    {\CoKinding\TEnv \Co {\Forall\TyVar\tau_1 \sim \Forall\beta\tau_2}}
    {\Subst_1 = \Sub\TyVar\sigma}{\Subst_2 = \Sub\beta\sigma};
    {\CoKinding\TEnv {\Co\At\sigma} {\Subst_1\tau_1 \sim \Subst_2\tau_2}}
  }
}

\newcommand\KForallCo{
  \ib{\irule[\trule{co-$\I\forall$}]
    {\CoKinding {\TEnv,\TyVar\co\kappa} \Co {\tau_1 \sim \tau_2}}{\TyVar \# \TEnv};
    {\CoKinding \TEnv {\Forall {\TyVar\co\kappa} \Co} {\Forall {\TyVar\co\kappa}\tau_1 \sim \Forall {\TyVar\co\kappa}\tau_2}}
  }
}


\newcommand\KCoComp{
  \ib{\irule[\trule{co-comp}]
    {\CoKinding \TEnv {\Co_1} {\tau_1 \sim \tau_2}}
    {\CoKinding \TEnv {\Co_2} {\sigma_1 \sim \sigma_2}}
    {\TyKinding \TEnv {\tau_i\App \sigma_i} \kappa};
    {\CoKinding \TEnv {\Co_1\App \Co_2} {\tau_1 \sigma_1 \sim \tau_2 \sigma_2}}
  }
}

\newcommand\KLeftCo{
  \ib{\irule[\trule{co-left}]
    {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
    {\CoKinding \TEnv {\Left \Co} {\tau_1 \sim \tau_2}}
  }
}

\newcommand\KRightCo{
  \ib{\irule[\trule{co-right}]
    {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
    {\CoKinding \TEnv {\Right \Co} {\sigma_1 \sim \sigma_2}}
  }
}

\newcommand\KCastCo{
  \ib{\irule[\trule{co-leftc}]
    {\CoKinding \TEnv \Co {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
    {\CoKinding \TEnv {\Cast {\Co_1} \Co_2} {\tau_1 \sim \tau_2}}
  }
}

\newcommand\KCoAx{
  \ib{\irule[\trule{co-ax}]
    {\CoKinding \TEnv \Co {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
    {\CoKinding \TEnv {\Cast {\Co_1} \Co_2} {\tau_1 \sim \tau_2}}
  }
}

\newcommand{\KTyVar}{
  \ib{\irule[\trule{ty-var}]
    {\TyVar\co\kappa \in \TEnv};
    {\TyKinding \TEnv \TyVar \kappa}
  }
}
\newcommand{\KTyApp}{
  \ib{\irule[\trule{ty-app}]
    {\TyKinding \TEnv \sigma {\kappa' \to \kappa}}
    {\TyKinding \TEnv \tau \kappa'};
    {\TyKinding \TEnv {\sigma\App\tau} \kappa}
  }
}
\newcommand{\KFCon}{
  \ib{\irule[\trule{ty-fcon}]
    {F_n \co \many \kappa^n \to \kappa' \in \TEnv}
    {\many {\TyKinding \TEnv {\sigma} {\kappa}}^n};
    {\TyKinding \TEnv {F_n \many\sigma^n} {\kappa'}}
  }
}
\newcommand{\KTyCon}{
  \ib{\irule[\trule{ty-con}]
    {T \co \kappa \in \TEnv};
    {\TyKinding \TEnv {T} {\kappa}}
  }
}
\newcommand{\KTyAll}{
  \ib{\irule[\trule{ty-all}]
    {\TyKinding {\TEnv,\TyVar\co\kappa} {\sigma} \star}
    {\fresh \TyVar \TEnv};
    {\TyKinding \TEnv {\Forall {\TyVar\co\kappa} \sigma} \star}
  }
}

\begin{figure}[t]

  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}\\
    \TCast
  \end{gather*}

\begin{gather*}
  \fbox{$\TyKinding \TEnv \tau \kappa$}\\
  \KTyVar \rsp \KTyApp \\
  \KTyCon \rsp \KFCon \rsp \KTyAll
  \end{gather*}

  \begin{gather*}
    \fbox{$\CoKinding \TEnv \tau \kappa$}\\
    \KReflCo \rsp \KSymCo \rsp \KTransCo \\
    \KForallCo \rsp \KInstCo \\
    \KCoComp \\
    \KLeftCo \rsp \KRightCo \\
  \end{gather*}

  \caption{Excerpt of Static Semantics of \SFC}
  \label{fig:sfc-typing}
\end{figure}

To formalize our intuitions of how the coercions ought to behave, we provide static semantics in the form of declarative style typing and kinding rules in \pref{fig:sfc-typing}. To start off the \trule{cast} transforms a term $M\co\tau$ to a term $M\co\sigma$ with a witness coercion $\Co\co\tau\sim\sigma$

As all the coercions are types such that their kinds give us the type equality, the coercion calculus is part of the kinding rules. The kinding rules \trule{co-refl}, \trule{co-sym}  and \trule{co-trans} makes coercion an equivalence relation. The rule \trule{co-comp} enables lifting coercions for higher kinded types and reason equalities between them. As a simple example of why coercion composition is useful, consider a higher kinded algebraic type !Tree! and a coercion $\Co\co\sigma_1\sim\sigma_2$, then using $\tau_1$ and $\tau_2$ to be equal to !Tree!, we have that $\refl{\texttt{Tree}} \App\Co : \texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_2$.
On the other hand, if we have a coercion $\texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_1$ then we can recover the coercion components using the \trule{co-left} and \trule{co-right}, for the higher kinded type and its arguments repectively. In full generality, we can state the lifting property formally in \SFC using \pref{thm:sfc-coercion-lifting}.
It captures the intuitive idea that given equal sub-parts, an equality between whole entities can be constructed.
\begin{theorem}[Coercion Lifting]\label{thm:sfc-coercion-lifting}
  If $\TyKinding {\TEnv,\TyVar\co\kappa'} \phi \kappa$, where $\TyVar$ is free in $\phi$
  and does not appear free in $\TEnv$,
  $\CoKinding \TEnv \Co {\sigma_1\sim\sigma_2}$, and $\TyKinding \TEnv {\sigma_i} \kappa'$
  then, $\CoKinding \TEnv {\Set{\TyVar\mapsto \Co}\refl\phi} {\Set{\TyVar\mapsto\sigma_1}\phi \sim \Set{\TyVar\mapsto\sigma_2}\phi}$
\end{theorem}
\begin{proof}[Proof Sketch of \pref{thm:sfc-coercion-lifting}]
  Proof is by induction on the derivation of the well kinded type $\phi$. In each of the four cases, whenever in the original derivation the rule \trule{co-refl} was used, it is replaced by the derivation of $\CoKinding \TEnv \Co {\sigma_1 \sim \sigma_2}$ % TODO: I am not convinced some how
  % We have 4 cases:
  % \begin{itemize}
  % \item[\trule{ty-var}] Either the type variable is $\TyVar$ and we are done, or it is not and we use \trule{co-refl}.
  % \item[\trule{ty-app}] By induction hypothesis and using \trule{co-comp}.
  % \item[\trule{ty-fcon}]
  % \item[\trule{ty-all}]
  % \end{itemize}
  % we have a derivation $\Typing {\TEnv, \TyVar\co\kappa'} \phi \kappa$. Then using \trule{co-refl} we obtain $\Typing {\TEnv, \TyVar\co\kappa} {\refl\phi} {\phi \sim \phi}$.
\end{proof}

The \trule{co-$\I\forall$} and \trule{co-$\E\forall$} justify coercions between polytypes and their instantations or in other words, if two types are equal, then their instantiations with equal types are also equal.

\subsubsection{Consistency}
We want to make sure that we can never derive anything obviouly wrong such as !Int ~ Bool! in our language.

\subsection{Operational Semantics}
We will use call by name semantics as GHC uses lazy evaluation. Adjusting the operational semantics to call by value will not change the analysis in any significant manner. We assume Barendregt's convention throughout the presentation.

\newcommand{\Beta}{
  \ib{\irule[\trule{$\beta$}]
    {};
    {$\stepsto {(\Lam {x\co\tau} M) \App N} {\Set{x\mapsto N}M}$}
  }
}
\newcommand{\TBeta}{
  \ib{\irule[\trule{Ty-$\beta$}]
    {};
    {$\stepsto {(\TLam \TyVar M) \App \tau} {\Set{\TyVar\mapsto \tau}M}$}
  }
}
\newcommand{\CaseE}{
  \ib{\irule[\trule{case}]
    {};
    {\stepsto {\Case {(K \many\sigma\many\phi\many\Tm)} {\Set{...;  K\App\many\beta\App\many x \to N; ...}}} {\Set{\many {\beta\mapsto\phi}, \many{x\mapsto\Tm}}N}}
  }
}
\newcommand{\CoTransE}{
  \ib{\irule[\trule{Co-Trans}]
    {};
    {$\stepsto {\Cast {(\Cast \Val \Co)} {\nu}} {\Cast \Val {(\trans{\Co} {\nu})}}$}
  }
}

\newcommand{\TyPush}{
  \ib{\irule[\trule{ty-push}];
    % {\Co : {\Forall {c\co\kappa} \tau} \sim \Forall {c\co\kappa} \tau'};
    {$\stepsto {(\Cast{\TLam {\TyVar\co\kappa} M}\Co)\App \tau} {({\TLam {\TyVar\co\kappa} (\Cast M {\Co\At\TyVar})})\App \tau}$}
  }
}

\newcommand{\CoPush}{
  \ib{\irule[\trule{co-push}]
    {\substack {\mathlarger{\nu\co \sigma_1' \sim \sigma_2'}\\
                      \mathlarger{\Co_1 : \sigma_1 \sim \sigma_1' = \Left {(\Left \Co)}}}}
    {\substack {\mathlarger{\Co\co (\sigma_1 \sim \sigma_2 \then \sigma_3) \sim (\sigma_1' \sim \sigma_2' \then \sigma_3')}\\
                \mathlarger{{\Co_2: \sigma_2 \sim \sigma_2' = \Right{(\Left\Co)}\quad{\Co_3:\sigma_3\sim\sigma_3' = \Right\Co}}}}};
    {$\stepsto {(\Cast{\TLam {\TyVar\co(\sigma_1\sim\sigma_2)} M}\Co)\App \nu} {\Cast {(\TLam {\TyVar\co(\sigma_1\sim\sigma_2)} M)\App (\Co_1 \circ \nu \circ \Sym \Co_2)} {\Co_3}} $}
  }
}

\newcommand{\Push}{
  \ib{\irule[\trule{push}]
    {\Co : \tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}
    {\Co_1 = \Right (\Left \Co)}
    {\Co_2 = \Right \Co};
    {$\stepsto {({\Cast {\Lam x M} {\Co}}) \App N} {\Cast {(({\Lam x M})\App {(\Cast N {\Sym \Co_1})})} \Co_2}$}
  }
}

\newcommand{\HPush}{
  \ib{\irule[\trule{h-push}]
    {\substack{\mathlarger{\Co : T\App\many\sigma \sim T\App\many\tau}\\
        \mathlarger{H : \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \rho \to T\App\many\TyVar^n}}}}
    {\substack{\mathlarger{{\Subst = \Set{\many{\TyVar_i \mapsto \Co_i}, \many{\beta_i \mapsto \phi_i}}}}\\
        \mathlarger{\Tm'_i = \Cast {\Tm_i} \Subst\rho_i}}}
    {\Co_i = \Right (\Left^{i-1}\Co) }
    {\phi' =
      \begin{cases}
        \Cast {\phi_i} \Subst(v_1 \sim v_2) &\text{if }\beta_i:v_1 \sim v_2\\
        \phi_i\quad &\text{otherwise}
      \end{cases}
    };
    {$\stepsto {\Case {(\Cast {H\App \many\sigma\App\many\phi\App\many\Tm} \Co)} {\many{\Ptrns \to N}} }
      {\Case {(H\App \many\tau\App\many{\phi'}\App\many{\Tm'})} {\many{\Ptrns \to N}} }$}
  }
}


\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Value Types}  && T\Val &::= T \bnfor \tau \to \tau \bnfor \Forall {\TyVar\co\kappa}\\
    \text{Plain Values} && \Val  &::= H \bnfor \Lam {x\co\tau} M \bnfor \TLam {\TyVar\co\kappa} M \\
    \text{CValues}      && C\Val &::= \Val \bnfor \Cast \Val \Co\\

    \text{Evaluation Contexts} &&  \EvalCtxt &::= \EvalCtxtHole{-} \bnfor \EvalCtxt\App M \bnfor \EvalCtxt \tau \bnfor \Cast \EvalCtxt \Co \bnfor \Case \EvalCtxt {\many{P}}\\
  \end{syntax}
  \begin{gather*}
    \fbox{$\stepsto M N$}\\
    \Beta \rsp \TBeta\\
    \CaseE \rsp \CoTransE\\
    \Push \rsp \TyPush\\
    \CoPush\\
    \HPush
  \end{gather*}
  \caption{Operational Semantics of \SFC}
  \label{fig:op-sem-sfc}
\end{figure}

The unusual feature of the system is that the values are stratified into normal values ($\Val$) and cvalues ($C\Val$) that are values with coercion casts respectively. CValues are needed to maintain type preservation, which would otherwise break in the presense of casts. The cvalues can step in one of the four ways \trule{push}, \trule{ty-push}, \trule{co-push} or \trule{h-push}. In \trule{push}, the coercion $\Co$, applied to the lambda term, is split so that it is applied to the argument ($\Co_1$) and to the redux reduction ($\Co_2$). Applying this rule exposes a \trule{$\beta$} redux. The rule \trule{ty-push} for type application that moves the coercion inside a type abstraction instantiated at the type variable.
The rule \trule{co-push} is just like \trule{push} but for moving coercions.

The most complex rule is \trule{h-push} which makes more sense with an example in hand. Consider the case scrutinee $\Cast {(Cons\App \texttt{Int}\App x\App y)} \Co$ where, !Cons : FORALL a. a -> [a] -> [a]!, !$\Co$ : [Int] \sim [T Bool]! and !T! is a type constructor. The cast transforms the scrutee into a type ![T Bool]! by pushing the coercion into its subcomponents.
$$
\stepsto {\Cast {(\texttt{Cons}\App \texttt{Int}\App \texttt{x}\App \texttt{y})} \Co} {\texttt{Cons} \App (\texttt{T}\App \texttt{Bool}) (\Cast {\texttt{x}} \Right\Co) (\Cast {\texttt{y}} {(\refl{[]}\Right\Co)})}
$$
Coercion lifting plays an important role here to make sure that the term subcomponents $\Cast M_i {\Subst}\rho_i$ is of the appropriate type. Each rule is derived in a systematic way by making sure the type of the term does not change after moving the cast.

This coercion operation calculus is not needed during runtime as these coercions are erased after type checking phase. They are necessary to prove the imporant metatheoritic property of the calculus.

\subsubsection{Soundness}
So do the static semantics effectively weed out all the programs that may fail at runtime. One way to show soundness for a system is by proving subject reduction property.

\begin{theorem}[Progress and Subject Reduction (Take 1)]
  If $\Typing \TEnv \Tm \tau$ then, either $\Tm$ is a cvalue or, $\stepsto \Tm \Tm'$ and
  $\Typing \TEnv {\Tm'} \tau$
\end{theorem}

However this is not entirely correct. If our system has a top level axiom such as $coBogus \co \texttt{Int} \sim \texttt{Bool}$, this coercion can be used in a cast, to convert an ill typed term to a well typed term thus breaking subject reduction property. The thoerm needs to be strengthened using an appropriate restriction on $\TEnv$. In general, checking consistency at top level is an undecidable. There exists is a conservative approximation of consistency which can be checked syntactically and it is sufficient for this purpose.

\begin{definition}[\Good $\TEnv$]
  A $\TEnv$ is \Good $\TEnv$ when
  \begin{itemize}
  \item If $\CoKinding \TEnv \Co {T \many\sigma \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $T\App\many\sigma'$
  \item If $\CoKinding \TEnv \Co {(\sigma' \to \sigma) \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $\tau' \to \tau''$
  \item If $\CoKinding \TEnv \Co {\Forall {\TyVar\co\kappa} \sigma \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $\Forall {\TyVar\co\kappa} \sigma'$
  \end{itemize}
\end{definition}

\begin{theorem}[Progress and Subject Reduction]\label{thm:progress}
  If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ then, either $\Tm \in C\Val$ or, $\stepsto \Tm \Tm'$ and
  $\Typing \TEnv {\Tm'} \tau$
\end{theorem}

\subsection{Encoding Language Features}

Armed with the new coercion infrastructure, it is now possible to encode most of the features discussed in \pref{sec:language-features}. \SFC is a conservative extension of \SF, type classes and algebraic datatypes can thus be encoded in \SFC without any extra machinery. GADTs and associated types demand some more details.

\subsubsection{GADTs}

\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Polytypes}         && \pi   \bnfeq& \eta \bnfor \Forall\TyVar.\pi\\
    \text{Constrained Types} && \eta  \bnfeq& \tau \bnfor \tau\sim\tau \then \eta\\
    \text{Monotypes}         && v,\tau  \bnfeq& \TyVar \bnfor \tau\to\tau \bnfor T\App\many\tau\\
    \text{Constraints}       && C     \bnfeq& \empt \bnfor C, c\co\tau\sim\tau'
  \end{syntax}
  \caption{GADT Surface level type syntax}
  \label{fig:gadt-type-syntax}
\end{figure}

\newcommand\GADTVar{
  \ib{\irule[\trule{g-var}]
    {x\co\pi \in \TEnv};
    {\GTranslate C \TEnv x {\pi} x}
  }
}
\newcommand\GADTEq{
  \ib{\irule[\trule{g-eq}]
    {\GTranslate C \TEnv \Tm \tau \Tm'}
    {\CoKinding C \Co {\tau \sim \tau'}};
    {\GTranslate C \TEnv \Tm {\tau'} {\Cast {\Tm'} \Co}}
  }
}
\newcommand\GADTForallI{
  \ib{\irule[\trule{g-$\I\forall$}]
    {\GTranslate C \TEnv \Tm \pi \Tm'}
    {\fresh \TyVar {C, \TEnv}};
    {\GTranslate C \TEnv \Tm {\Forall {\TyVar\co\star} \pi} {\TLam {\TyVar\co\star} \Tm'}}
  }
}
\newcommand\GADTForallE{
  \ib{\irule[\trule{g-$\E\forall$}]
    {\GTranslate C \TEnv \Tm {\Forall {\TyVar\co\star} \pi} \Tm'};
    {\GTranslate C \TEnv \Tm {\Set{\TyVar\mapsto\tau}\pi} {\Tm'\App \tau}}
  }
}
\newcommand\GADTCI{
  \ib{\irule[\trule{g-$\I C$}]
    {\GTranslate {C,c:\tau\sim\tau'} \TEnv \Tm {\eta} \Tm'};
    {\GTranslate C \TEnv \Tm {\tau\sim\tau'\then\eta} {\TLam {(c\co\tau\sim\tau')} \Tm'}}
  }
}
\newcommand\GADTCE{
  \ib{\irule[\trule{g-$\E C$}]
    {\GTranslate {C} \TEnv \Tm {\tau\sim\tau'\then\eta} \Tm'}
    {\CoKinding C \Co \tau\sim\tau'};
    {\GTranslate C \TEnv \Tm {\eta} {\Tm'\App\Co}}
  }
}
\newcommand\GADTAlt{
  \ib{\irule[\trule{g-alt}]
    {\substack{
        \mathlarger{H\co \Forall {\many\TyVar} {\Forall {\many\beta} {\many{\tau'\sim\tau''} \then \many\tau \to T\many\TyVar}}}\quad
        \mathlarger{\many\TyVar \cap \many\beta = \varnothing}\quad
        \mathlarger{\fvs{\many\tau, \many{\tau'}, \many{\tau''}} = \fvs{\many\TyVar, \many\beta}}\quad
        \mathlarger{\Subst = \Set{\many{\TyVar\mapsto v}}}\quad
        \mathlarger{\fresh {\many{c}} {C, \TEnv}}\\
        \mathlarger{\GTranslate {C,\many{c\co\Subst{\tau'}\sim\Subst\tau''}\,} {\,\TEnv,\many{x\co\Subst\tau}\,} \Tm {\tau'} \Tm'} }};
    {\GTranslate C \TEnv {H\App\many x \to \Tm} {T\App\many v \to \tau'}
                   {H\App(\many{\beta\co\star})\App(\many{c\co\Subst\tau'\sim\Subst\tau''})\App(\many{x\co\Subst\tau}) \to \Tm' }}
  }
}

\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\GTranslate C \TEnv {\Tm} {\pi} {\Tm'}$}\\
    \GADTVar \rsp \GADTEq\\
    \GADTForallI \rsp \GADTForallE\\
    \GADTCI \rsp \GADTCE
  \end{gather*}
  \begin{gather*}
    \fbox{$\GTranslate C \TEnv {p \to \Tm} {\pi \to \pi} {p' \to e'}$}\\
    \GADTAlt
  \end{gather*}
  \caption[Encoding GADTs]{GADTs' Type-directed Translation in \SFC}
  \label{fig:encoding-gadts}
\end{figure}
Each GADT data constructor's type is elaborated with equality coercions as arguments in \SFC term.
For example the data constructor !App! from \pref{fig:gadt-example} is elaborated as:
\begin{code}
  App : FORALL a:* b:*. FORALL c:*. FORALL co: c ~ (a -> b). Lam c -> Lam a -> Lam b
\end{code}
A type directed elaboration converts a source level GADT into \SFC as shown in \pref{fig:encoding-gadts}.
The source level types are stratified into three groups: polytypes, constrained types and monotypes.


The judgement $\GTranslate C \TEnv \Tm \pi \Tm'$ says that given a surface level term $\Tm$ of type $\pi$, it is elaborated to a term $\Tm'$ into \SFC under the constraints C and typing environment $\TEnv$. The key idea of the elaboration is that the type equality constraints, $\tau\sim\tau'$, are elaborated to coercions in \SFC. The constraint C is a collection of named type equalities. The rules \trule{g-var}, \trule{g-$\I\forall$} and \trule{g-$\E\forall$} are standard rules used for elaborating Hindley-Milner System to \SF while \trule{g-$\I C$} and \trule{g-$\E C$} reminescent of elaborating typeclass constraints. The complex looking \trule{g-alt} elaborates case statements into \SFC. Each surface data constructor is elaborated to the \SFC data constructor with explicit coercions as arguments. The rule \trule{g-eq} is the same as \trule{cast} rule. The important detail here is that the coercion, $\Co$, is inferred from the constraint context $C$. Constructing the appropriate $\Co$ algorithmically is possible by using a simple unification algorithm based on \cite{lassez_unification_1988}.

\begin{lemma}[Type Preservation]
  If $\GTranslate C \empt \Tm \tau {\Tm'}$ then $\Typing C {\Tm'} \tau$
\end{lemma}
Type preservation is an important sanity check for the elaborations. It says that the the elaborated terms have the same type that was inferred at surface level.

\begin{theorem}[GADT Consistency]
  If $\dom\TEnv$ does not contain type variables and coercion constants, and $\CoKinding \TEnv \Co {\tau\sim\tau'}$ then, $\tau$ and $\tau'$ are syntactically identical.
\end{theorem}
GADT consistency says that if two types are provably equal, then they must be syntactically identical. All GADT programs are sound in \SFC.

\begin{theorem}[GADT Soundess]
  If $\GTranslate \empt \empt \Tm \tau \Tm'$, then $\manystepsto {\Tm'} \Val$ iff $\manystepsto {\compile\Tm} \Val$ where $\Val$ is some value of ground type and $\compile\Tm$ is type erased term.
\end{theorem}

\subsubsection{Associated Types}
Elaborating associated types to \SFC is very similar to translating GADTs. The constraint context now contains class predicates along with equality predicates. This extension is also needed if GADT data constructors need to constrain certain type parameters. The equality constraints can now appear not only in the context of GADT data constructors but any term type.

\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Class Declarations} &&C_{ls} \bnfeq& \textbf{\texttt{class }} D\App\many\TyVar \textbf{\texttt{ where }} \many{dsigs}; \many{sigs}\\
    \text{Instance Declarations} &&I_{nts} \bnfeq& \textbf{\texttt{instance }} D\App\many\tau \textbf{\texttt{ where }} \many{adata}; \many{val}\\
    \text{Associated types} &&dsigs \bnfeq& \textbf{\texttt{type }} \tau\\
    \text{Method signatures} &&vsigs \bnfeq& x\co\tau\\
    \text{Assoicated type instance} &&asigs \bnfeq& \tau = \sigma\\
    \text{Method bindings} &&asigs \bnfeq& x = \Tm
  \end{syntax}
  \caption[Class Syntax]{Class and Associated Types Surface Syntax}
  \label{fig:assoc-types-syntax}
\end{figure}


The judgement $\DTranslate C {D\App\tau} d$ elaborates the predicate $D\App\tau$ to a dictonary $d$ in \SFC.
The rule \trule{subst} allows replacing type class parameters with equal types. This is necessary to account for associated types that appear in the type class signature. $D\App\tau$ may contain an associated type, and depending on the instantiation of the free type variables in $\tau$ an appropriate coercion can be used to justify casting the dictonary $d$ to the corresponding actual type.

$$
\ib{\irule[\trule{subst}]
  {\DTranslate C {D\App\tau} d}
  {\CoKinding C \Co {D\App\tau \sim D\App\tau'}};
  {\DTranslate C {D\App\tau'} {\Cast d \Co}}
}
$$

Reconsider the !Coll c! typeclass and a function !sumColl :: (Coll c, Num (Elem c)) => c -> Elem c!, which sums up all the elements of the collection !c!. The predicate !Num (Elem c)! asserts that the elements of the collection can be summed up. A resonable use of !sumColl! would be at type ![Int]!, as integers can indeed be summed up. This would result in instantiating the type parameter !c! with ![Int]!. The rule \trule{subst} in this case justifies the use of !Num Int! dictonary as if it was a !Num (Elem [Int])! dictonary.

Each class declaration introduces a new class predicate name in the type environment along with its method names. With associated types, a new ``type function'' name---!Elem c! in the case of !Coll c!---is also added to the type environment.
Each instance introduces a new axiom into the typing environment. For the !Coll c! typeclass the instance !Coll [Int]! introduces the coercion !CoColl : Elem [Int] ~ Int!. In general, each instance generates an axiom of the form !co: (FORALL $\many{\TyVar\co\star}$. F $\sigma$ $\sim$ $\sigma'$)! where $\fvs\sigma = \many\TyVar$
and $\fvs{\sigma'} \subseteq \many\TyVar$. These axioms induce a rewrite function on types and are also called as rewrite axioms.


\begin{theorem}[Associated Type Consistency]
If $\TEnv$ contains type rewrite axioms that are confluent and terminating, then $\TEnv$ is consistent.
\end{theorem}
For non-overlapping instances, it is indeed the case that the rewrite axioms are confluent and terminating.

\subsubsection{Open Type Functions}
An unusual feature of the system, which also makes the type system expressive is the ability to write open type functions. The intension is to generalize associated types to be independent of typeclasses. Type functions are able to express complex type level computations. Further, they are also open, meaning, just like class instances, the open type functions can be extended. An example of how type computations are expressed by type functions that define addition of naturals at type level, shown in \pref{fig:open-type-fun-add}
\begin{figure}[ht]
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      data Z
      data S n
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      type family Plus m n
      type instance Plus Z n = n
      type instance Plus (S m) n = S (Plus m n)
    \end{code}
  \end{minipage}
  \caption{Addition with type functions}
  \label{fig:open-type-fun-add}
\end{figure}
Open type functions thus give type computation a flavor of rewrite rules.
Each type family instance directly translates to axioms. Thus for !Plus m n! type function, the two associated instances would introduce the following axioms !CoAddZ : Plus Z n ~ n! and !CoAddSmn : Plus (S m) n ~ S (Plus m n)!

\subsection{Implicit vs Explicit Evidence}
\SFC is to be used as a core language with fairly complex features. It would be worth while to investigate if, as an optimization step, the explicit coercions can be elided and then by means of typechecking they could be reconstructed. We call the system with implicit coercion calculus, \SFCi. The key difference between \SFC and \SFCi is that where ever \SFC has a coercion type $\Co$ of kind $\tau\sim\tau'$, \SFCi only gives the equality kind in curly braces $\tau\sim\tau'$. Hence, for terms
\begin{itemize}
\item Type casts, $\Cast \Tm \Co$, turns into $\Cast \Tm \Set{\tau \sim \tau'}$ and,
\item Coercion applications, $\Tm\App\Co$ turns into $Tm\App\Set{\tau \sim \tau'}$
\end{itemize}


\begin{theorem}[Undecidability of coercion reconstruction of \SFCi]
  If $\Tm_i$ is an expression in \SFCi and $\TEnv$ is a typing environment, then reconstrucing a \SFC term $\Tm$ such that $\Typing \TEnv {\stepsto {\Tm_i} \Tm} \sigma$, where $\Typing \TEnv \Tm \sigma$ holds is undecidable.
\end{theorem}

If there exists a restriction of \SFCi which is sufficient to encode all the language features while enjoying decidable type checking is an open question.

% \subsubsection{Generative Abstract Types}

\section{\SFR}\label{sec:sfr} % R for roles
\subsection{Generative types}
GHC uses the keyword newtype to declare generative types. For the type declaration !newtype Fun = MkFun (Fun -> Fun)!, its translation to \SFC amounts to an introduction of a coercion axiom !CoFun : Fun ~ (Fun -> Fun)! where !Fun! is a type constant. In general, for any generative type declaration of the form !newtype T $\many\TyVar$ = MkT $\tau$! will be of the form !CoT : T $\many\TyVar$ ~ $\tau$! where $\tau$ may contain $\many\TyVar$ as free type variables. The advantage of introducing coercion axioms for newtype declarations is that they provide abstraction with zero runtime cost. However,
a naive translation can result in a type soundess bug\cite{TODO}.

Consider two modules: !AgeLib! where the library writer defines a newtype !Age! and the other, !AgeClient!, a client module that uses the library module !AgeLib!. The !AgeLib! module does not expose the newtype constructor !MkAge! and forces the client module to use the smart constructors !mkAge! to create values of the type !Age! instead.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      module AgeLib (Age, mkAge, addAge) where
      newtype Age = MkAge Int

      mkAge :: Int -> Age
      mkAge n = MkAge n

      addAge :: Age -> Int -> Age
      addAge (MkAge n) p = MkAge (n + p)
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      module Client where
      import Lib (Age, mkAge, addAge)

      a1, a2 :: Age
      a1 = mkAge 3
      a2 = addAge a1 4


    \end{code}
  \end{minipage}
  \caption{Newtype Module: Definition and Use}
  \label{fig:newtype-modules}
\end{figure}
While on the surface level the functions !mkAge! and !AddAge! seem to pack and unpack
the !Int! value stored in the !Age! type, the compiler generates code where all the packing and unpacking of the values are removed using explicit type casts. Further during runtime, the explicit casts are completely removed. This optimization step is not performed on types declared using !data! keyword. So the runtime cost characteristics of types declared using  !newtype!s and !data! are different. Due to the newtype definition
we know that !Age! \emph{is} !Int! as !Int! is the concrete runtime representation of !Age!. This means that we should not pay any runtime cost for coercing !Age! to !Int! and vice versa. We notionally represent this type equality as !Age ~ Int!.
\begin{figure}[ht]
  \centering
  \begin{minipage}[h]{0.4\linewidth}
    \begin{code}
      co :: Int ~ Age
      mkAge :: Int -> Age
      mkAge x = x /> co
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.4\linewidth}
    \begin{code}
      addAge x a :: Int -> Age -> Age
      addAge x a = (x + a /> (sym co)) /> co
    \end{code}
  \end{minipage}
  \caption{\lstinline{AgeLib} compiled}
  \label{fig:compiled-code}
\end{figure}

The compiled code of !AgeLib! is shown in \pref{fig:compiled-code}. The coercion !co! introduced by the newtype definition asserts that !Int! is the same as !Age!. !x /> co! casts the type of !x! from !Int! to !Age! and !sym co! reverses the type equality to !Age ~ Int!.

Now, consider an indexed type family !F! that maps !Age! to !Bool! and !Int! to !Char!. This declaration adds two new type equalities namely !F Age ~ Bool! and !F Int ~ Char!. But in the presense of the equality !Age ~ Int!, we can make type unsound coercions by deriving !Char ~ Bool!. The reasoning would be that !Char ~ F Int!, then !F Int ~ F Age! (as !Int ~ Age!), and finally, obtaining !Char ~ Bool! (by using !F Age ~ Bool!).

\begin{code}
  type family F a :: *
  type instance F Age = Bool
  type instance F Int = Char
\end{code}

What went wrong? The problem stems from the fact that we are using unconstrained coercion lifting; the use non-parametric features of the language in the context that assumes parametricity. Type functions interacting with newtypes is just one case where seemingly innocuous features interacting with each other causes type unsoundess. Just limiting the use of type funcions is not enough; similar bugs can be cooked up by using GADTs as well.

\SFC needs to be extended to ensure that no runtime cost generative types without introducing type unsoundness.
There are 2 main changes in extended \SFR: (i) The equality is a type and not a kind as in \SFC, and (ii) The equality type is indexed by a role which captures how the two types are equal. The strictest role of equality is nominal equality that says that two types are equal only if they are syntactically identical in names. A more relaxed version of equality is if the the types are representationaly equal where they have the same runtime representation.
\subsection{Syntax}
The key ingredient needed to express the finer grained equality is via addition of roles. All type parameters to a type constructor is decorated with a role that says whether the type parameter equality is supposed to be compared nominaly or representationly.

  \begin{figure}[h]
    \begin{syntax}
    \text{Type Vars} &\TyVar,\beta,\Co  &\qquad\text{Type constants} &T \\
    \text{Term Vars} &x,y                  &\qquad\text{Newtypes}       &N \\
    \text{Coercion Vars} &c                &\qquad\text{Indices}        &i,n \in \mathbb{N} \\
  \end{syntax}
  \begin{syntax}
    \text{Kinds}     &&\kappa       \bnfeq& \star \bnfor \kappa \to \kappa \bnfor \sigma \sim \tau\\
    \text{Types}     &&\tau,\sigma  \bnfeq& \TyVar \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor F_n\\
    \text{Coercions} &&\nu,\Co      \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\nu\Co % equiv relation
                                        \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
                                        \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co \bnfor \Nth i \Co \bnfor H\App\many\Co \bnfor F_n\many\Co \bnfor \SubCo \Co
                                        \\  % compose/decompose
    \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
    \text{Roles}           &&\rho  \bnfeq& \texttt{N} \bnfor \texttt{R} \bnfor \texttt{P}\\
    \text{Patterns}  &&P    \bnfeq& H\App \many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
    \text{Terms}     &&M,N  \bnfeq& x \bnfor  \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co
    \end{syntax}
    \begin{syntax}
      \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
      \text{Role Context} &&\REnv        \bnfeq& \empt \bnfor \REnv, \TyVar\co\rho\\
    \text{Substitutions}  &&\Subst       \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
  \end{syntax}

    \caption{Syntax of \SFR as an extension of \SFC}
    \label{fig:sfr-syntax}
  \end{figure}

\subsection{Static Semantics}
The crux of the matter is in the judgement for coercions $\CoKinding \TEnv \Co {\tau\sim^\kappa_\rho\sigma}$. This is read as ``in the type environment $\TEnv$ the coercion $\Co$, witness the equality between types $\tau$ and $\sigma$ that have the same kind $\kappa$ at role $\rho$''. The ``equality at role $\rho$'' being the novel feature. There can be three different kinds of roles:
\begin{itemize}
\item\textbf{Nomial Equality} This is the strictest kind of equality denoted by $\sim_N$ which holds exactly wnen the two types are the ``same''. For example, an type function axiom !F Int = Bool! makes !F int!$~\sim_N~$!Bool!
\item\textbf{Representational Equality} This equality holds when the two types have the same runtime representation. For example, a new type declarations !newtype Age = Int! makes !Age!$~\sim_R~$!Int!
\end{itemize}
The roles form a natural relation where $N < R$ which is reflected explicitly in rule $\trule{co-sub}$.

A type safe cast can now be expressed using a typing rule:
$$
\ib{\irule[\trule{Cast}]
  {\Typing \TEnv \Tm \tau}
  {\CoKinding \TEnv \Co {\tau \teqR \tau'}};
  {\Typing \TEnv {\Cast \Tm \Co} \tau'}
}
$$
This means that a type cast is only possible if the types are representationally equal. This coincides with our intuition of a type cast: An expression of a type can be treated as an expression of another type, only if they have the same representation at runtime.

\newcommand\KSubCo{
  \ib{\irule[\trule{co-sub}]
    {\CoKinding \TEnv {\Co} {\tau \teq\rho \tau'}}
    {\rho < \rho'};
    {\CoKinding \TEnv {\SubCo \Co} {\tau \teq{\rho'} \tau'}}
  }
}

\newcommand\KNthCo{
  \ib{\irule[\trule{co-nth}]
    {\CoKinding \TEnv {\Co} {T \App \many\sigma \sim T\App\many{\tau'}}}
    {\many\rho = \text{roles}(T)}
    {T~\text{is not a newtype}};
    {\CoKinding \TEnv {\Nth i \Co} {\tau_1 \teq{\rho_i} \tau_2}}
  }
}

\newcommand\KLeftCoR{
  \ib{\irule[\trule{co-left}]
    {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \teqN \tau_2 \App \sigma_2}};
    {\CoKinding \TEnv {\Left \Co} {\tau_1 \teqN \tau_2}}
  }
}

\newcommand\KRightCoR{
  \ib{\irule[\trule{co-right}]
    {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \teqN \tau_2 \App \sigma_2}};
    {\CoKinding \TEnv {\Right \Co} {\sigma_1 \teqN \sigma_2}}
  }
}

\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\CoKinding \TEnv \tau \kappa$}\\
    \KLeftCoR \rsp \KRightCoR \\
    \KNthCo \rsp \KSubCo\\
  \end{gather*}
  \caption{Static Semantics of \SFR (Excerpt)}
  \label{fig:sfr-typing}
\end{figure}


\section{\SFP}\label{sec:sfp} % P for promotion
Consider a standard GADT implementation of a vector !Vec! that also stores its length at the type level.

\begin{minipage}[ht]{0.4\linewidth}
  \begin{code}
    data Z
    data S n
  \end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
  data Vec : * -> * -> * where
      Nil  : Vec Z elem
      Cons : elem -> Vec n elem -> Vec (S n) elem
\end{code}
\end{minipage}

The types !Z! and !S n! encode the size of the vector. The data constructor !Nil! is a zero length vector which is asserted by the type !Z!, while !Cons! appends an element of type !elem! to a vector of size !n! to return a vector of size !S n!. With primitive kind system, there is no enforcement that the type argument to !Vec! has to be !Z! or !S n!. Any type of kind $\STAR$ is a valid argument. A more desirable alternative would be to restrict the kind of the type that is used to describe the size of the vector.

\begin{minipage}[ht]{0.4\linewidth}
  \begin{code}
    data Nat = Z
             | S Nat
  \end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
  data Vec : Nat -> * -> * where
      Nil  : Vec Z elem
      Cons : elem -> Vec n elem -> Vec (S n) elem
\end{code}
\end{minipage}

In this setting, the type checker has enough information to reject semantically absurd types like !Vec Char Int!.
Similar problems arise while writing addition function on type level naturals. The situation is worse as writing type functions does not include any kind information; the argument and return types are defaulted to kind $\STAR$, making it seem unkinded. The intention of the programmer is not that any type of kind $\STAR$ is a valid argument to the function !Plus!. A better alternative would be to allow specifying the concrete kind specification to the function---in this case specifying that the arguments and return kind of !Plus! is a !Nat!.

\begin{minipage}[ht]{0.4\linewidth}
  \begin{code}
    type family Plus m n
    type instance Plus Z m     = m
    type instance Plus (S n) m = S (Plus n m)
  \end{code}
\end{minipage}
\begin{minipage}[ht]{0.4\linewidth}
  \begin{code}
    type family Plus (m:Nat) (n:Nat) : Nat
    type instance Plus Z m     = m
    type instance Plus (S n) m = S (Plus n m)
  \end{code}
\end{minipage}

Allowing user defined kinds, albeit more expressive, is just a half baked pie without kind polymorphism. Consider a higher kinded datatype !TApp! which is useful for generic programming that encodes type application.
\begin{code}
  data TApp f a = MkTApp (f a)
\end{code}

The type argument !f! is defaulted to kind $\STAR \to \STAR$ making the kind of !TApp! to be $(\STAR \to \STAR) \to \STAR \to \STAR$. This limits encoding only those types that have a kind $\STAR$. A more appropriate kind for !TApp! would be $\Forall k {(k\to\STAR) \to k \to \STAR}$, allowing a wider range of types to benefit from generic programming techniques.

There are two birds to kill: allowing user defined kinds and kind polymorphism. An unimaginative solution for the former would involve adding a new syntax to the language that accepts kind definitions similar to the type definition. For example !kind Nat = Z | S Z!. An improvement, which the paper proposes, is by reusing the datatype declaration syntax. For any algebraic datatype that the user defines, each of its data constructors will be promoted along with the type constructor. To encode type level naturals, the user declares the datatype, as they normally would, while the compiler automatically generates new type and kind level bindings. For example, in case of datatype !Nat!, the data constructor !S : Nat -> Nat! gets promoted to the type level while !Nat! get promoted to the kind level. Kind polymorphism is straight forward by allowing kind quantification only in prenex form. For example for a datatype !T : FORALL $\many{a}$. $\many{k}$ -> *!, all the kind variables $\many{a}$ are quantified before the type arguments of kind $\many{k}$. This simplifies the semantics of the type constants.

\subsubsection{Syntax}

\begin{figure}[h]
  \centering
  \begin{syntax}
    \shl{\text{Kind Vars}} &\shl{\chi}   &                            & \\
    \text{Type Vars} &\TyVar,\beta,\Co  &\qquad\text{Type constants} &T,\shl{H} \\
    \text{Term Vars} &x,y                  &\qquad\text{Indices}        &i,n \in \mathbb{N} \\
    \text{Coercion Vars} &c & &
  \end{syntax}
  \begin{syntax}
    \text{Sort}      &&\square & \\
    \text{Kinds}     &&\kappa,\eta       \bnfeq& \shl{\chi} \bnfor \STAR \bnfor \shl{\texttt{CONSTRAINT}} \bnfor \kappa \to \kappa \bnfor \shl{T\App\many\kappa}\\
    \text{Types}     &&\tau,\sigma  \bnfeq& \TyVar \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor F_n \bnfor \shl{\Forall \chi \tau} \bnfor \shl{\tau\App\kappa} \bnfor \shl{H}\\
    \text{Coercions} &&\nu,\Co      \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\nu\Co % equiv relation
                                      \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
                                      \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co  % compose/decompose
                                      \bnfor \shl{\Forall \chi \Co} \bnfor \shl{\Co\App\chi} %
                                      \bnfor \shl{\Co\At\chi}\\
    \text{Predicates} && \phi \bnfeq& \sigma \sim \tau\\
    \text{Patterns}  &&P    \bnfeq& H\App\shl{\many\chi}\App\many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
    \text{Terms}     &&M,N  \bnfeq& x \bnfor  \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co %
                             \bnfor \shl{\TLam \chi \Tm} \bnfor \shl{\Tm\App\kappa} \\

    \end{syntax}
    \begin{syntax}
    \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
    \text{Substitutions}  &&\Subst       \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
  \end{syntax}
  \caption{The Syntax of \SFP as an extension fo \SFR}
  \label{fig:sfp-syntax}
\end{figure}

The stone, \SFP, that kills both these birds is a relatively small extension of \SF. The syntax for the core language is shown in \pref{fig:sfp-syntax}. The key changes in the language of types---highlighted to compare with \SFC---are allowing kind variables ($\chi$), kind abstraction ($\Forall \chi \kappa$) and application ($\tau\App \kappa$). These changes are reflected in the coercion language which needs additional constructs: kind poly-type congruence ($\Forall \chi \Co$), kind application ($\Co\App\chi$) and kind instantiations ($\Co\At\kappa$).

Other noteworthy addition to the kind and type language are promoted constructors $T\App\many\kappa$ and $H\App\many\tau$, respectively. The data constructors are now explicit in their kind polymorphism by expecting all the kind variables $\many\chi$ to be instantiated before any type variables are instantiated, evident in their type signatures, which now expects the kinds before the type as formal parameters.
\footnote{As a notational convenience, $\Forall {\many\TyVar} \tau$ is just a shorthand for $\Forall {\TyVar_1} {\Forall {\TyVar_2} {... \Forall {\TyVar_i} \tau}}$, while $\many\tau \to \tau'$ is a short hand for $\tau_1 \to \tau_2 \to ... \to \tau_i \to \tau'$
}{$$ H : \Forall {\shl{\many\chi}} {\Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\eta}} {\many\tau \to T \App\many{\chi}\App\many{\TyVar}}}} $$
}
Promotion of type and data constructors may result in namespace issues. It, however, can be disambiguated from the context. Another mechanism is to add explicit quotation marks as prefix to aid disambiguation.
For example ``!T!'' is a type constructor but ``!'T!'' is a kind constructor.
Finally, to stop the buck on the syntax classification hierarchy \emph{all} kinds are classified by a unique sort, $\square$.


\subsection{Promotion}
It is not clear which constructors can be promoted without threatening either the consistency, or usability of the language. Backwards compatibility is an important aspect while adding a new feature, as the code that compiled without promotion should also work after promotion.

\begin{itemize}
\item For type constructors, $T\App\many\kappa$ is a valid kind only if $T$ is fully saturated and has a kind $\many\STAR \to \STAR$
\item For data constructors, $H\App\many\tau$ is a valid type if all the type arguments $\many\tau$ to the constructor can be promoted and the type of the data constructor can be promoted.
\end{itemize}

The restriction on the promotion of type constructors is because promoting higher kinded types, such as $(\STAR \to \STAR) \to \STAR$ would not be possible without having a richer kind classification system. Further, promotion of type constructors that themselves accept promoted types such as !Vec : * -> 'Nat -> *! are not promoted as that would involve double promotion of the type !Nat! or would require type and kind levels to be fully dependent, making the system overcomplex. Kind polymorphic type constructors are also not promoted as that would require polymorphic sorts, which is absent from the system to keep it simple.

\subsection{Type and Kind Inference}
The user is expected to provide minimal (if any) kind annotations to the programs. This necessitates changing the type inference algorithm. While instantiating a kind polymorphic term, fresh kind unification variables are generated, and during type unification, the kinds of the types are also unified. This is necessary due to the design decision: there are no kind equalities. The unification of kinds does not produce any evidences, unlike for type unification. This helps in solving kind unification on the fly in contrast to solving type unification where they are collected as equality constraints and then solved separately, generating coercion as evidences.

The datatype declaration is type checked in a manner very similar to mutually dependent terms:
\begin{itemize}
\item The type declarations are sorted into strongly connected components;
\item The kind inference assigns a new unification kind variable to each of the type constructors and then walks over the definitions solving for kind constraints that may arise;
\item Finally, all the constructors are kind generalized at the end.
\end{itemize}


In \SFP, there are no kind coercions nor there are kind equality constraints. This keep the design and implementation simple but at the expense of leaving out certain programs that can be well typed. Promotion of GADTs is useful for generic programming.


\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}
  \end{gather*}
  \caption{Static Semantics of \SFP}
  \label{fig:sfp-typing}
\end{figure}





\section{\SFK}\label{sec:sfk} % K for kind eq
% We have type equalities, why not kind equalities?
% But we would then have two kinds of equalities: type and kind.
% So why not just squish types and kinds together, making it truly impredicative
GADTs are expressed using explicit type equality predicates in the type system. For example, consider the following GADT that encodes type representations:

\begin{minipage}[ht]{0.5\linewidth}
  \begin{lstlisting}
  data TyRep : * -> * where
    TyInt  : TyRep Int
    TyBool : TyRep Bool
  \end{lstlisting}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
  \begin{lstlisting}
  data TyRep : * -> * where
    TyInt  : a ~ Int => TyRep a
    TyBool : a ~ Bool => TyRep a
  \end{lstlisting}
\end{minipage}

The right hand side is just an elaborated version of the left hand side with explicit type equalities equalities.
Now, we can use local type equalities to pattern match on the different values of \lstinline{TyRep} to produce appropriate values. The function \lstinline{zero} computes a default value for each type representation.

\begin{minipage}{0.5\linewidth}
  \begin{codef}
    zero : forall a. TyRep a -> a
    zero TyInt = 0
    zero TyBool = False
  \end{codef}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
  \begin{codef}
    zero : forall a. TyRep a -> a
    zero (TyInt (co : a ~ Int))   = 0 /> sym co
    zero (TyBool (co : a ~ Bool)) = False /> sym co
  \end{codef}
\end{minipage}

The operator \lstinline|sym| reverses the direction of the type equality \lstinline|co : a ~ Int| to \lstinline|Int ~ a| and then the cast operator, \lstinline{/>}, uses the flipped coercion to justify the return type of the function.

It may be tempting to extend \lstinline{TyRep} datatype to represent more complex types such as lists. However, \SFC is not expressive enough to achieve this without auxiliary definitions. The type system is not expressive enough allow higher kinded types. Consider a new definition of \lstinline{TyRep} extended with two new data constructors \lstinline{TyList}, which represents a list type, and \lstinline{TyApp} which represents type application.

\begin{minipage}[ht]{0.4\linewidth}
  \begin{lstlisting}
    data TyRep :: forall k. k -> * where
      TyInt  :: TyRep Int
      TyBool :: TyRep Bool
      TyList :: TyRep []
      TyApp  ::
                TyRep a -> TyRep b
             -> TyRep (a b)
  \end{lstlisting}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
  \begin{lstlisting}
data TyRep :: forall k. k -> * where
  TyInt  :: ((k ~ *),(a : k) ~ Int) => TyRep k Int
  TyBool :: ((k ~ *),(a : k) ~ Bool) => TyRep k Bool
  TyList :: ((k ~ * -> *), (a : k) ~ []) => TyRep k []
  TyApp  :: (k1 ~ * -> *) => (k2 ~ *)
         => TyRep k1 a -> TyRep k2 b
         -> TyRep k2 (a b)
  \end{lstlisting}
\end{minipage}

The new \lstinline{TyRep} accepts two parameters, the first is the kind and the second is the type that has the kind of the first parameter. Pattern matching on the data constructor now exposes kind equalities along with type equalities. The new \lstinline{zero} function that now also returns the zero case for list would look as follows:
% \begin{minipage}[ht]{0.5\linewidth}
\begin{codef}
  zero : forall (a:*). TyRep a -> a
  zero TyInt            = 0
  zero TyBool           = False
  zero (TyApp TyList _) = []
\end{codef}

With explicit kind equalities allowed in the system, while the case for \lstinline{TyInt} and \lstinline{TyBool} in \lstinline{zero} is the same as in \SFC, the case for \lstinline{TyApp} can now be allowed and type checked. The first argument to \lstinline{TyApp} is inferred to have kind \lstinline{* -> *} and the second argument is inferred to have kind \lstinline{*}, making the result to be of kind \lstinline{*}. As a side note, because we have constrained the kind of the type parameter to \lstinline{zero} to be a \lstinline{*}, we cannot write \lstinline{zero (TyApp ty b) = zero ty} as \lstinline{ty} would be inferred to have kind \lstinline{k -> *}.

Adding kind equalities to the system is non-trivial owing to its interaction with type equalities. A few challenges are:
\begin{itemize}
  \item \SFK squashes the types and kinds to be the same by adding the axiom \lstinline{* : *}, or ``type is of kind type''. This necessitates a new formalism that proves that the metatheoritic properties of \SFK are carried over from \SFC. In dependently type languages \lstinline{* : *} axiom amounts to adding inconsistency, however this is not a problem in Haskell as all kinds are already inhabited;
  \item in \SFC, equalities could only exist between types of the same kind, but now, due to explicit kind equalities, it is possible to construct equalities between heterogenous types. The matters get more complicated with polymorphic types;
  \item coercions should not interfere with the operational semantics of the language;
  \item Kind indexed GADTs need to be able to abstract over coercions and use them in the types, a feature missing in \SFC.
\end{itemize}


\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Type Vars} &\TyVar,\beta,\Co  &  & \\
    \text{Term Vars} &\TmVar,y                  &\qquad\text{Indices}        &i,n \in \mathbb{N} \\
    \text{Coercion Vars} &c & &
  \end{syntax}
  \begin{syntax}
    \text{Type Constants}      &&T                    \bnfeq& (\to) \bnfor \star \bnfor H\\
    \text{Type level names}    &&w                    \bnfeq& \TyVar \bnfor F_n \bnfor T\\
    \text{Propositions}        &&\Prop                 \bnfeq& \tau\sim\sigma\\
    \text{Types and Kinds}     &&\tau,\sigma,\kappa   \bnfeq& w \bnfor \tau\App\tau %
                                                       \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor %
                                                       \Forall {c\co\Prop}\tau \bnfor \Cast\tau\Co \bnfor \tau\App\Co\\
    \text{Coercions}    &&\MCo,\Co   \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\MCo\Co \\ % equiv relation
                        && \bnfor& \shl{\ForallC {\MCo} {(\TyVar_1, \TyVar_2, c)} \Co} \bnfor \shl{ \MCo\App(\Co, \Co')} %
                                        \bnfor \shl{\ForallC {(\MCo_1, \MCo_2)} {(c_1, c_2)} \Co} %
                                        \bnfor \Co\At\MCo \bnfor \shl{\Co\At(\MCo, \MCo')}\\ % abstraction instanst
                        && \bnfor& \MCo\App\Co \bnfor \Left \Co \bnfor \Right \Co %
                                        \bnfor \Nth i \Co \bnfor \shl{\Kind \Co} \bnfor T\App\many\phi \\  % compose/decompose
    \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
    \text{Patterns}  &&P    \bnfeq& H\App \shl{\many{\Telescope}}\App{\many{x\co\tau}} \\
    \text{Telescopes} &&\Telescope \bnfeq& \empt \bnfor \Telescope, \TyVar\co\kappa \bnfor \Telescope, c\co\Prop\\
    \text{Terms}     &&M,N  \bnfeq& x \bnfor  \Lam {x\co\tau} M \bnfor M\App N %
                             \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \\
                     &&     \bnfor& \Lam {c\co\Prop} {\Tm} \bnfor \Tm\App\Co \bnfor \shl{\Contra \Co \tau}%
                             \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co\\

    \end{syntax}
    \begin{syntax}
    \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
    \text{Substitutions}  &&\Subst       \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
  \end{syntax}

  \caption{The Syntax of \SFK}
  \label{fig:system-fck-syntax}
\end{figure}

\newcommand\TContra{
  \ib{\irule[\trule{t-contra}]
    {\CoKinding \TEnv {\Co} {T\App\many\phi \sim T'\App\many{\phi'}}}
    {T \neq T'}
    {\Kinding \TEnv {\tau} {\star}};
    {\Typing \TEnv {\Contra \Co\tau} {\tau}}
  }
}

\newcommand\KCAppCo{
  \ib{\irule[\trule{co-capp}]
    {\CoKinding \TEnv {\Co} {\tau\sim\tau'}}
    {\Typing \TEnv {\tau\App\MCo} {\kappa}}
    {\Typing \TEnv {\tau'\App\MCo'} {\kappa'}};
    {\CoKinding \TEnv {\Co\App(\MCo, \MCo')} {\tau\App\MCo \sim \tau'\App\MCo'}}
  }
}

\newcommand\KCAllT{
  \ib{\irule[\trule{co-$\I{\forall\tau}$}]
    {\substack{ \mathlarger{\CoKinding {\TEnv,\TyVar\co\kappa,\TyVar'\co\kappa',c\co\TyVar\sim\TyVar'} {\Co} {\tau\sim\tau'}}\\
      \mathlarger{\Kinding \TEnv {\Forall {\TyVar\co\kappa} {\tau}} {\star}}}}
    {\substack{ \mathlarger{\CoKinding \TEnv \MCo {\kappa\sim\kappa'}}\\
      \mathlarger{\Kinding \TEnv {\Forall {\TyVar'\co\kappa'} {\tau'}} {\star}}}};
    {\CoKinding \TEnv {\ForallC\MCo{(\TyVar,\TyVar',c)}{\Co}} {\Forall {\TyVar\co\kappa}{\tau} \sim \Forall {\TyVar'\co\kappa'}{\tau'}}}
  }
}

\newcommand\KCAllC{
  \ib{\irule[\trule{co-$\I{\forall\Co}$}]
    {\substack{\mathlarger{\CoKinding {\TEnv,c\co\Prop,c'\co\Prop'} {\Co} {\tau\sim\tau'}} \\
                     \mathlarger{\Kinding \TEnv {\Forall {c\co\Prop} {\tau}} \star}%
                     \quad\mathlarger{\Kinding \TEnv {\Forall {c'\co\Prop'} {\tau'}} \star}
                        }}
    {\mathlarger{\fresh {\Set{c, c'}}{\Erased\Co}}}
    {\substack{\mathlarger{\CoKinding \TEnv {\MCo_1} {\sigma_1 \sim \sigma_1'}}\\
                  \mathlarger{\CoKinding \TEnv {\MCo_2} {\sigma_2 \sim \sigma_2'} }}}
    {\substack{\mathlarger{p = \sigma_1\sim\sigma_2}\\
                     \mathlarger{p' = \sigma_1'\sim\sigma_2'}}};
    {\CoKinding \TEnv {\ForallC{(\MCo_1,\MCo_2)}{(c, c')} {\Co}} {\Forall {c\co\Prop}{\tau} \sim \Forall {c'\co\Prop'}{\tau'}}}
  }
 }

\newcommand\KCInstCo{
  \ib{\irule[\trule{co-$\E\forall\Co$}]
    {\CoKinding \TEnv {\Co} {\Forall {c\co\Prop} {\tau} \sim \Forall {c'\co\Prop'} {\tau'}}}
    {\CoKinding \TEnv \MCo \Prop}
    {\CoKinding \TEnv {\MCo'} \Prop'};
    {\CoKinding \TEnv {\Co\At(\MCo, \MCo')} {\Set{c\mapsto\MCo}\tau \sim \Set{c'\mapsto\MCo'}\tau'}}
  }
}

\newcommand\KExtCo{
  \ib{\irule[\trule{co-ext}]
    {\CoKinding \TEnv {\Co} {\tau \sim \tau'}}
    {\CoKinding \TEnv \tau \kappa}
    {\CoKinding \TEnv {\tau'} \kappa'};
    {\CoKinding \TEnv {\Kind \Co} {\kappa \sim \kappa'}}
  }
}

% \newcommand\KNthOneCo{
%   \ib{\irule[\trule{co-cast}]
%     {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
%     {\CoKinding \TEnv {\Co\App(\MCo, \MCo')} {\tau}}
%   }
% }

% \newcommand\KNthTwoCo{
%   \ib{\irule[\trule{co-cast}]
%     {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
%     {\CoKinding \TEnv {\Co\App(\MCo, \MCo')} {\tau}}
%   }
% }


\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}\\
    \TContra
  \end{gather*}
  \begin{gather*}
    \fbox{$\CoKinding \TEnv \Co {\tau \sim \tau}$}\\
    \KCAllT\\
    \KCAllC\\
    \KCAppCo \rsp \KCInstCo\\
    \KExtCo
  \end{gather*}
  \caption{Static Semantics of \SFK (Excerpt)}
  \label{fig:sfk-typing}
\end{figure}

% \section{\HMX}
% \subsection{Absence of Principal Types}
% %% How is principality lost
% In the presence of GADTs terms can no longer have principal types. Consider a program fragment as shown below:
% \begin{minipage}[ht]{0.4\linewidth}
% \begin{codef}
% data Exp a where
%     Var :: String -> Exp String
%     Add :: Exp a -> Exp a -> Exp a
% \end{codef}
% \end{minipage}
% \begin{minipage}[ht]{0.3\linewidth}
%   \begin{codef}
%     qq (Var s) _ = s
%     qq (Add _ _) s = s
%   \end{codef}
% \end{minipage}%
% \begin{minipage}[ht]{0.3\linewidth}
%   \begin{codef}
%     qq2 (Var s) _ = s
%     qq2 (Add _ _) s = "exp" ++ s
%   \end{codef}
% \end{minipage}

% Now !qq! can be assigned two valid types:

% \begin{minipage}[ht]{0.5\linewidth}
% \begin{codef}
% qq :: $\forall$ a. Exp a -> a -> a             -- (1)
% qq :: $\forall$ a. Exp a -> String -> String   -- (2)
% \end{codef}
% \end{minipage}%
% \begin{minipage}[ht]{0.4\linewidth}
%     qq2 :: $\forall$ a. Exp a -> String -> String
% \end{minipage}

% Both of these types (1) and (2) are also incomparable; meaning, it is not the case that one of them is more general than the other. On the other hand, qq2 does have a principal type. The goal of the paper is to design a type system which when given a term---like !qq!---that does not have a principal type fails with an error while, a term that does have a principal type---like !qq2!---does not. Does this mean that this type system never accepts terms that do not have a principal type? No, in such cases, we want the users to specify the type that they expect the term to have and the system will check if the type annotation agrees with the definition. The challenge is to identify which definitions should be accepted and which definitions should be rejected.

% \subsection{Constraint Based Type Systems}
% Constraint based type systems are type systems where the constraints are part of the type system specification and the implementation generates and solves for the constraints. The implementations usually operate in two phases. In the first phase all the constraints are generated and then in the second phase they are solved. For example, consider a Haskell program as given below:
% % isPalindrome :: Eq a => [a] -> Bool
% \begin{codef}
% isPalindrome = \ xs -> xs == reverse xs
% \end{codef}
% The function !isPalindrome! accepts a list of type !a! and returns !True! if it is a palindrome. While checking the type of !isPalindrome!, the type system would emit a ``wanted'' constraint !Eq [a]! due to the use of !(==)! on the list xs. This ``wanted'' constraint then needs to be solved, (possibly) using the ``given'' constraint !Eq a!. For the constraint generation phase, we write
% \newcommand\GenConstraints[4]{#1 \vdash #2 : #3 \rightsquigarrow #4}
% $$
% \GenConstraints \TEnv e \tau C
% $$
% to denote that the constraints $C$ are generated when we infer the type $\tau$ for the expression $e$ in the type environment $\TEnv$. For example, for \lstinline{isPalindrome}, we would infer the type and generate the constraints from the body of the function as given below:
% $$
% \TEnv \vdash \text{(\lstinline {\\ xs -> xs == reverse xs})} : \TyVar \rightsquigarrow (\TyVar \sim (\Co \to Bool)) \land (\Co \sim [\Co_1]) \land (\text{\lstinline{Eq}}~ [\Co_1])
% $$
% where the type environment $\TEnv$ contains the bindings $\text{\lstinline{reverse}}\co\forall \TyVar. [\TyVar] \to [\TyVar]$ and $\text{\lstinline{(==)}}\co\forall \beta. Eq~\beta \then \beta \to Bool$.
% For an ML like language, that contains lambdas ($\Lam x e$), applications ($e \App e'$), variables ($x$) and pattern matching on algebraic datatypes ($\Case e {\Set{\overline{K \overline x \to e}}}$ ) the rules for constraint generation are given in \pref{fig:constraint-gen}.

% \newcommand\SubstMap[3]{\Set{#2 \mapsto #3}#1}
% \begin{figure}[ht]
%   \centering
%   \begin{gather*}
%     \fbox{$\GenConstraints \TEnv e \tau C$}\\
%     \ib{\irule[\trule{var}]{(x\co\forall a. Q \then \tau) \in \TEnv}
%       {\Subst= \Set{{\overline{a \mapsto \TyVar}}}}
%       {\fresh {\overline{\TyVar}} \TEnv};
%       {\GenConstraints \TEnv x {\Subst\tau} {\Subst Q}}}
%     \rsp
%     \ib{\irule[\trule{app}]
%       {\GenConstraints \TEnv e {\tau_1} {Q_1}}
%       {\GenConstraints \TEnv {e'} {\tau_2} {Q_2}}
%       {\fresh \TyVar \TEnv};
%       {\GenConstraints \TEnv {e \App e'} {\TyVar}  {Q_1 \land Q_2 \land (\tau_1 \sim \tau_2 \to \TyVar)}}}\\
%     \ib{\irule[\trule{abs}]
%       {\GenConstraints {\TEnv, x\co\TyVar} {e} {\tau} Q}
%       {\fresh \TyVar \TEnv};
%       {\GenConstraints \TEnv {\Lam x e} {\TyVar \to \tau} Q}}
%     \rsp
%     \ib{\irule[\trule{case}]
%       {\LARGE\substack{{\GenConstraints \TEnv {e} {\tau} Q} \quad {\fresh {\TyVar,\overline\Co} \TEnv} \quad {Q' = Q \land T\overline\Co \sim \tau}\\
%         {\text{For each } K_i\overline x_i \to e_i} \text{ do }\qquad\qquad\qquad\qquad\\
%         (K_i\co\forall\overline{a}.~\overline\sigma_i \to T\overline{a}) \in \TEnv \quad {\GenConstraints{\TEnv, \overline{x_i:\Set{\overline{a \mapsto \Co}}\sigma_i}} {e_i} {\tau_i} {C_i}}\\
%         {Q'_i = C_i \land \tau_i \sim \TyVar}
%       }};
%       {\GenConstraints \TEnv {\Case e {\Set{\overline{K \overline x \to e}}}} {\TyVar} (Q' \land (\land \overline{Q_i})) }}
%   \end{gather*}
%   \caption{Constraint Generation Rules}
%   \label{fig:constraint-gen}
% \end{figure}

% The rule \trule{var} says that for a variable $x$ with the type scheme $\forall \TyVar. Q \then \tau$ in the type environment $\TEnv$, all the meta type variables $\overline{a}$ are instantiated to fresh unification variables $\overline{\TyVar}$, ($\fresh {\overline{\TyVar}} \TEnv$). The rule \trule{app} says that for term application $e \App e'$ $Q_1$ are the generated constraints from $e$, $Q_2$ from $e_2$, and further, the type of $e$ should be of the shape $\tau_2 \to \TyVar$ where $\tau_2$ is the type of the argument, $e'$, and $\TyVar$ is a fresh unification variable. The rule \trule{abs} says that for a lambda term $\Lam x e$ the inferred type is $\TyVar \to \tau$, where $\TyVar$ is a fresh unification variable and the body of the lambda term $e$ with the extended binding for $x\co\TyVar$ returns the type $\tau$ and generates the constraints $Q$. For, the \texttt{case} statement, we first infer the the type ($\tau$) of the expression subject expression ($e$) and generate constraints ($C$) for it, we then infer types and generate constraints of each match statement with an additional constraint that the return type ($\TyVar$) should be the same for all the cases. The final constraint set is just a conjunction of the constraints generated by the main expression and the cases.

% \newcommand\SolConstraints[5]{\ensuremath{#1 ; #2 \vdash_{simp} #3 \rightsquigarrow #4 ; #5}}
% In the second phase, the type system solves the constraints generated by phase one. This solving, or simplification of the constraints, depends on the specific constraint system X. We write the simplification as given below:
% $$
% \SolConstraints {\mathcal Q} {Q_{given}} {Q_{wanted}} {Q_{residual}} \Subst
% $$
% This means that under some global constraints $\mathcal{Q}$ and ``given'' constraints $Q_{given}$ the constraint solver (or simplifier) reduces the ``wanted'' constraints $Q_{wanted}$ to ``residual'' constraints $Q_{residual}$, along with a substitution $\Subst$ that maps unification variables to types. The residual constraints are the constraints that could not be solved by the simplifier. The constraint solving is performed for each top level binding. For \lstinline{isPalindrome}, the generated constraints can be solved
% by having a global constraint $\forall a.~ Eq~ a \then Eq~ [a]$.
% $$
% \SolConstraints {\Set{\forall a.~ Eq~ a \then Eq~ [a]}} {\varepsilon} {(\TyVar \sim (\Co \to Bool)) \land (\Co \sim [\Co_1]) \land (\text{\lstinline{Eq}}~ [\Co_1])} {\Set{Eq~\Co_1}} {\Set{\TyVar \mapsto ([\Co_1] \to Bool)}}
% $$

% \newcommand\TopLevel[3]{\ensuremath{#1 ; #2 \vdash #3}}
% Both of these phases are orchestrated at the top level for each binding. It is denoted by,
% $$
% \TopLevel {\mathcal{Q}} \TEnv {prog}
% $$
% where the $prog$ is a collection of bindings, $\TEnv$ is the type environment, and $\mathcal Q$ are the global constraints. There are two rules, \trule{binda} and \trule{bind}, depending on whether the binding is accompanied by a type annotation or not respectively. They are shown in \pref{fig:top-level-rules}.

% \begin{figure}[ht]
%   \centering
%   \begin{gather*}
%     \fbox{\TopLevel {\mathcal{Q}} \TEnv {prog}}\\
%     \ib{\irule[\trule{empt}]
%       {};{\TopLevel {\mathcal{Q}} \TEnv \varepsilon}}\\
% %%
%     \ib{\irule[\trule{bind}]
%       {\LARGE
%         \substack{{\GenConstraints \TEnv e \tau {Q_{wanted}}} \qquad {\SolConstraints {\mathcal Q} \varepsilon {Q_{wanted}} {Q} \Subst}\\
%           \overline{\TyVar} = fv(\Subst\tau, Q) \qquad \fresh{\overline{a}}\TEnv \qquad \Subst' = \Set{\overline{\TyVar \mapsto a}}\\
%         {\TopLevel {\mathcal Q} {\TEnv, (f\co\forall\overline a.~ \Subst'Q \then \Subst\tau)} {prog}}}
%       };
%       {\TopLevel {\mathcal{Q}} \TEnv {f = e, prog}}}
% %%
%     \rsp
% %%
%     \ib{\irule[\trule{bindA}]
%       {\LARGE
%         \substack{{\GenConstraints \TEnv e \sigma {Q_{wanted}}} \qquad {\SolConstraints {\mathcal Q} Q {Q_{wanted} \land \sigma \sim \tau} {\varepsilon} \Subst}\\
%         {\TopLevel {\mathcal Q} {\TEnv, (f\co\forall a. Q \then \tau)} {prog}}}
%       };
%       {\TopLevel {\mathcal{Q}} \TEnv {(f\co\forall a. Q \then \tau) = e, prog}}}
%   \end{gather*}
%   \caption{Top Level Rules}
%   \label{fig:top-level-rules}
% \end{figure}
% The rule \trule{bind} takes in a top level binding $f = e$ of the program $prog$, infers a type $\tau$ and also generates constraints $Q_{wanted}$. The simplifier, then solves $Q_{wanted}$ using global quantified constraints to residual constraints $Q$. The type of the binding is then obtained by simply quantifying over all the free unification variables that appear in $\tau$ and $Q$. The simplifier is not provided with any given constraints as the binding is at top level. The rule \trule{binda} handles the case of a user provided type annotation for a binding. Here, the constraint generation phase is similar to the rule \trule{bind}, but the constraint simplification phase takes in an additional constraint $\sigma \sim \tau$ where $\sigma$ is the inferred type and $\tau$ is the user provided type. The simplifier should also be able to solve all the wanted constraints. If the simplifier cannot, it would mean that the user provided type annotation is under-specified which would be reported as an error.

% In the \lstinline{isPalindrome} example, the simplifier could, alternatively, have returned a $Eq~[\Co_1]$ instead of $Eq~\Co_1$ by not using the global constraint. But, this would mean that the function would not have had the most general, or principal type. Thus, it is necessary for the simplifier to be able to compute principal types so that the type inference algorithm can infer the most general type.

% \subsection{The Constraint System with Local Assumptions}
% The key feature of GADTs is that pattern matching on a GADT data constructor introduces local equality constraints into scope. For example, in the previous, example of \lstinline{Exp}. The data constructor \lstinline{Var} has the type $\forall a. (a \sim String). String \to Exp~ a$ where the constraint $a \sim String$ is local. When a value of type \lstinline{Exp} is constructed using \lstinline{Var}, this local constraint needs to be satisfied by providing an evidence along with the value level arguments. This evidence becomes available when the value is de-constructed in a pattern match. The type of the data constructor $K$ of a type $T$ in this full glory is given below:
% $$
% K \co \forall \overline{a}\overline{b}.~ Q \then \overline{\tau} \to T \overline{a}
% $$
% Here, $\overline{b}$ and $Q$ are the local type variables and the local assumptions respectively, which were previously absent from the type. These local variables have a flavor of existential types as they do not appear in the return type, $T \overline a$
% \begin{figure}[ht]
%   \centering
%   \begin{gather*}
%     \fbox{$\GenConstraints \TEnv e \tau C$}\\
%     \ib{\irule[\trule{casela}]
%       {\LARGE\substack{
%           {\GenConstraints \TEnv e \tau C} \quad \fresh {\TyVar,\overline\Co} \TEnv\\
%           {\text {For each } K_i \overline{x_i} \to e_i \text{ do }\qquad\qquad\qquad}\\
%           {(K_i\co\forall \overline{a}\overline{b}.~ Q \then \sigma_i \to T\overline{a}) \in \TEnv} \qquad {\fresh\beta\TEnv} \\
%           {\GenConstraints {\TEnv, \overline{(x_i\co\Set{\overline{\Co \mapsto a}}})} {e_i} {\tau_i} {C_i}} \qquad {\overline{\delta_i} =  fv(ruben sandwitch) - fv}
%         }};
%       {\GenConstraints {\TEnv} {\Case e {\overline{\Set{K_i\overline{x_i}\to e_i}}}} {\TyVar} {C \land \overline{C'_i}}}}
%   \end{gather*}
% \caption[Constraint Generation]{Constraint Generation with Local Assumptions}
%   \label{fig:constraint-gen-la}
% \end{figure}
% %% Local constraints

% %% Implication constraints

% % The typing judgements in constraint based type systems are of the form
% % $$
% % \QTyping {Q} {\TEnv} {e} {\Ty}
% % $$
% % which means that given a typing environment $\TEnv$ and some given constraints $Q$ the term $e$ has type $\Ty$.
% % The structure of !Q! here is flat; meaning it does not contain quantified schemes. However, in Haskell, the instance declarations are quantified constraints. For example, an instance declaration such as:
% % \begin{codef}
% % instance Eq a => Eq [a] where {...}
% % \end{codef}
% % introduces a global top level constraint scheme: !$\forall$ a. Eq a => Eq [a]!.
% % To treat these kinds of global constraint schemes we need to rearrange the type system. We denote $\mathcal Q$ to contain all the top level constraint schemes that can be used by the solver to solve for wanted constraints.
\section{Related Work}
\subsection{Modules with Typeclasses}
Modules and typeclasses are similar in certain ways

They are however different.

Their co-existence is incoherent

Various attempts\cite{dreyer_modular_2007, wehr_ml_2008, white_modular_2014} have been made to enable a happy co-existence of parametric and implicit ad-hoc polymorphism together in a language by encoding typeclasses as modules and vice versa. However, in presense of modularity to support separate code compilation, it becomes impossible to avoid incoherence without cripling the language expressibility.
ML and its varients\cite{milner_definition_1997,leroy_ocaml_2023} disallow allow operator overloading to enable modular compilation.

\section{Conclusion and Future work}\label{sec:conclusion}
\begin{figure}[ht]
  \centering
  \begin{tabular}[ht]{c | c}
    Parametric Features & Non-parametric Features \\
    \hline
    Modules             & Typeclasses (with Functional Dependencies)\\
    Algebriac Datatypes & Generalized Algebraic Datatypes\\
                        & Type Families (Open, Closed, Associated types)\\
                        & Generative Types (newtypes)
  \end{tabular}
  \caption{Features of Haskell}
  \label{fig:haskell-lang-features}
\end{figure}


Using non-parametric features in a context that assumes parmetricity is a root cause of type unsoundess ``bugs''.
Using roles to define a nuanced way to describe equality between types was a way to separate out non-parametric behavior from parametric one. The non-parametric behaviour in this case arose from the fact that type functions can behave differently on types that may have the same run time representation.
A sound type system design for typeclasses with modules without crippling the expressiveness of the system is still an active area of research.
\AI{TBH i don't know where i'm going with this}
% \begin{figure}[ht]
%   \centering
%   \begin{tabular}[ht]{c | c | c | c | c | c | c}
%     Language & Decidable Type checking & ADTs & GADTs & Open/Closed Type functions & Generative Types & Kind Functions\\
%     \hline
%     \SF  & & & & &\\
%     \SFC & & & & &\\
%     \SFP & & & & &\\
%     \SFK & & & & &\\
%   \end{tabular}
%   \caption{Core languages and their Capabilities}
%   \label{fig:language-features}
% \end{figure}

%%%% Bibliography
\bibliography{comp}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% eval: (visual-line-mode 1)
%%% eval: (auto-fill-mode 0)
%%% eval: (tex-source-correlate-mode 1)
%%% TeX-master: t
%%% TeX-command-extra-options: "--synctex=1"
%%% End: