\newif\ifcomments\commentstrue

\RequirePackage[svgnames,dvipsnames,prologue,x11names]{xcolor}
\documentclass[screen,nonacm,manuscript,review]{acmart} % TODO make it 10pt

\usepackage{comp}

\title{Practical Functional Programming with \SFC and its Extensions}
% \subtitle{Extensions to System F}

\author{Apoorv Ingle}
%   \orcid{0000-0002-7399-9762}
\affiliation{%
  \institution{University of Iowa} \department{Department of Computer Science} \streetaddress{McLean Hall} \city{Iowa City} \state{Iowa} \country{USA}}
% \keywords{typeclass, type family}

%%% TODO:
%%% [ ]. Shorten Sentences
%%% [ ]. Fix Puntuations: commas and semicolons
%%% [ ]. How do the articles work?


\begin{document}
\begin{abstract}
  Programming languages are a primary interface between humans
  and computers. This makes it crucial to understand their principles of design
  and implementation. Programming languages need to encode unambiguous yet
  expressive instructions for computers. The
  features of a typed programming language strive to provide machine
  verified guarantees about the programs by following the  ``correct
  by construction'' philosophy. In this monograph, we study the practical
  (type system) and meta-theoretical (formalization) aspects of a
  modern, statically typed, declarative, and functional
  programming language. We focus on how one single construct,
  explicit type equality, gives a unified account of several seemingly
  unrelated language features. As a consequence we obtain a typed
  intermediate language that is an easy to
  reason about and implement. We also
  study important extensions of this language and identify notable open
  research problems.
\end{abstract}

\maketitle
\section{Introduction}\label{sec:introduction}
Constructive mathematics and computer programming have an elegant
correspondence: they both involve solving a problem by
identifying and then exploiting the right level of abstraction.
The difference is in the direction.
Mathematicians work from top to bottom; they first build the abstractions
and then enrich them to represent real world
problems. Programmers turn the process on its head.
They first write a software program that solves a very specific
task. They then build the right level of abstraction by means of
iterative code refactoring. This phenomenon is a well established
observation in computer science: Curry-Howard or the
Brouwer–Heyting–Kolmogorov correspondence~\cite{wadler_propositions_2015,han_deep_2023}
and it forms the basis for the field of type
theory~\cite{barendregt_lambda_2013,hottbook_2013,nordstrom_programming_1990}.

From a programmers perspective, a programming language compiler is
a software that evolves by means of extensions. A new language feature
is implemented in two steps. First, the abstract syntax tree (AST)---the
data structure that represents the surface syntax of the language---is
extended to represent the new feature, and second, the operations
performed on the AST are extended to cater for this new feature. These
operations transform the AST into an intermediate language which is internal to the
compiler~\cite{siek_compilation_2023}. It is a more suitable
representation of the surface syntax AST to perform code analysis and
runtime optimizations~\cite{aho_compilers_1986}.
If there is a way of encoding the new language feature by using the
existing language features, the second step---of the operation extension---can be
skipped. However, not all language features can be encoded
into existing language features. In such cases, the compiler writer, needs
to enrich the intermediate language to support the
new language feature. Extensions to the intermediate language,
however, do not scale. Multiple extensions to the intermediate
language causes a feature clutter, making it difficult to
maintain and reason about its correctness. The intermediate language
requires a reformulation, which amounts to finding the right
abstraction that can encode the different high level language features
into a simplified intermediate language.

In a compiler for statically typed programming languages, the
typechecker, is the gate keeper. Its purpose is to ensure that it does
not let \emph{bad} programs pass, and flag the appropriate offending
parts of programs to the programmer. This gate keeping is desirable;
it aids the programmer, by guaranteeing that a class of program
errors---such as runtime null pointer exceptions due to incorrect
function arguments---can never occur. They can then concentrate on the
other, more important aspects of the program, which cannot be verified
by the typechecker. On the other hand, the typechecker needs to allow
the \emph{good} programs---the ones that are definitely bug free---to
pass without the programmer's intense struggle. For
this reason, the structure of types needs to be sufficiently rich
to express programmer's complex ideas, while rejecting incorrect
programs. The examples below illustrate a couple use-cases and
their associated semantic invariants which the programmers may like to
capture within the type structure:
\begin{enumerate}
\item In an AST where the compiler writer wants to enforce the
  sub-term to always be of a fixed typed. This avoids the need
  explicitly checking them in the code.
  More concretely, the AST representing the control structures, such
  as !if! or !while! loop, is well formed only when the sub-term
  representing the condition has a !Boolean! type.
\item In a multi-stage compiler, the AST is gradually enriched with
  more information after each analysis stage. The type structure can
  enforce that the information is accessed only after it is populated,
  otherwise, the typechecker raises a type
  error~\cite{peyton_jones_trees_2017}.
\end{enumerate}

The thesis of this monograph is that \emph{using explicit type
  equality, a novel construct, in the intermediate language is an
  appropriate abstraction to encode semantic invariants which
  compromises neither on safety nor on efficiency.}
Type equalities denote explicit proofs in the system which assert when
two types can be considered are synonymous. The monograph
is arranged in four parts:
\begin{enumerate}
\item[Part I] serves as a reminder of the essential features
  of a statically typed functional programming language. This gives
  the \emph{programmers' view} of the language motivated using real
  world examples.
\item[Part II] formalizes the syntax and operational semantics of
  \SFC which in essence is \SF extended with typed equality. This part
  gives the \emph{compiler writers' view} of the language. To this
  effect, we show how the language features described in Part I can be
  trivially encoded within \SFC.
\item[Part III] describes three important extensions of vanilla \SFC
  which make it even more expressive:
  (1) \SFR makes type equality finer grained while
  providing stronger type soundness guarantees, (2) \SFP makes the kind
  system more expressive, and (3) \SFK squashes the distinction between
  types and kinds making the type (and kind) level computation even
  more expressive. Each of these extensions are formalized along with
  an overview its correctness argument.
\item[Part IV] describes notable open problems in the area.
  It is followed by important work adjacent to the principal topic
  of this monograph, and concluding remarks.
\end{enumerate}

\part{I: The Landscape}\label{part:I}%%%%%%%%%
\section{Program Behavior}
\subsection{Functions}
There can be no functional programming language without a support for first class
functions. Functions capture the essence of the program execution by encoding the logic of
transforming data. An example of a function declaration in Haskell is that of an identity
function, !idI! over integers and !idC! over characters shown below.

\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
idI :: Int -> Int
idI i = i
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
idC :: Char -> Char
idC i = i
\end{code}
\end{CenteredBox}
\end{minipage}

The type signature !idI :: Int -> Int! specifies the input-output behavior of the
function: !idI! accepts an argument of type !Int! and returns a value of type !Int!. The
function definition !idI i = i! specifies how the function satisfies the typing
specification: it takes an argument !i! and returns it unmodified. Functions declared in a
functional programming language resemble mathematical functions in two ways: first, they
are referentially transparent, i.e. a call to the same function twice with the same
arguments will always return the same result. The expression, !idI 3!, will always
evaluate to !3!. Second, they are declarative, in a sense they abstract away the
implementation and execution details of how the function executes on the actual underlying
hardware; there is no mention of allocating and freeing program memory or even an explicit
return.

\subsection{Parametric Polymorphism}
Functions like !idI!, while enough to build large programs, are too cumbersome in
practice to program with. It is necessary to define the same functionality over
different types to aid re-usability of programs. For example, an
identity function over !Char!, !idC! has the exact same
definition as !idI!. The functions that work on base types, such as !Int!, are called as
monomorphic functions. To enhance re-usability of programs, it is necessary to be able to
abstract over types.


\begin{minipage}{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
id :: t -> t
id a = a
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
hotm :: t -> t
hotm = id id
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
($\circ$) :: (a -> b) -> (b -> c) -> (a -> c)
f $\circ$ g  = \ x -> f (g x)
\end{code}
\end{CenteredBox}
\end{minipage}


The function !id!, shown above, abstracts over all types by an implicit universal
quantification over the type variable !t!. Such functions are called \emph{parametrically
polymorphic} functions~\cite{strachey_fundamental_2000}. The compiler
can deduce which instance of the polymorphic function is required when
an argument of a concrete type is passed as an argument to a
polymorphic function. Parametric polymorphism also aids the language
feature for defining and using higher order functions as functions can
now be passed as arguments to other functions. For example, the
typechecker will infer the type of the left hand side !id! in the
definition of !hotm! as !(t -> t) -> (t -> t)!. The language is said
to support functions as first class when the functions can be
used as arguments to other functions, new functions can be declared
locally and also be able to compose without saturating them. For
example, in the function declaration, which composes any two suitable
functions, ($\circ$), illustrates all these three properties. The
arguments !f! and !g! are functions used as arguments, it declares an
local anonymous function which binds the parameter !x!, !\ x -> f (g x)!.
ML~\cite{milner_logic_1975,milner_theory_1978}
was the pioneer among the functional programming languages in
introducing a declarative style implicit parametric polymorphism.

\subsection{Adhoc Polymorphism}
Consider the following two terms, !addI! and !addF!:

\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
addI :: Int = 1 + 2
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
addF :: Float = 1.1 + 2.3
\end{code}
\end{CenteredBox}
\end{minipage}

In the definition of !addI!, the operator (!+!) is applied to two
integers, but in the definition of !addF!, the operator (!+!) is
applied to two floating point numbers (!Float!). Although the
programmer uses the same symbol, the meanings of the two terms are
distinct: !addI! adds two !Int!s and returns an !Int! while the !addF!
adds two !Float!s and returns a !Float!. The low level compiler generated code
for each of them would also differ as they would make a call to two
different built-in subroutines. This name punning technique, which is dependent on the
type of arguments, or the context of where the function appears, is called as \emph{adhoc
polymorphism}~\cite{strachey_fundamental_2000}, while the functions
themselves are referred to as adhoc polymorphic functions. Implicit
operator overloading is a mechanism to support adhoc polymorphism.
The compiler resolves the overloaded operator (!+!) to
the actual operator (!int_plus! or !float_plus!) during the typechecking phase.

\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
class Num a where
  (<=) :: a -> a -> Bool
  (==) :: a -> a -> Bool
  (+) :: a -> a -> a
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
instance Num Int where
  (<=) = int_le
  (==) = int_eq
  (+) = int_plus
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
instance Num Float where
  (<=) = float_le
  (==) = float_eq
  (+) = float_plus
\end{code}
\end{CenteredBox}
\end{minipage}
\caption{\lstinline{Num} typeclass and instances}
\label{fig:tc-num}
\end{figure}

To make implicit overloading usable in functional programming
languages, the type checker should be able to rule out terms such
!id + id!---summing 2 parametrically polymorphic functions is meaningless.
Functions like !(+)! act only on some specific class of constrained or qualified
types~\cite{jones_qualified_1994}:  The addition function
(!+!), more precisely, has the type !Num t => t -> t -> t!. This means that the
function can only be used on those types !t! which satisfy the !Num t!
constraint. This !Num t! constraint is declared by the programmer as a
typeclass. The instances of the typeclass specify when the typeclass
constraint holds, and what is the intended behavior at that type. For
example, the !Num! typeclass and its instances are shown in the
\pref{fig:tc-num}. The use of the function (!+!), say for
example in the definition of !addF!, the typechecker needs to justify
its type correctness. This amounts to satisfying the constraint !Num Float!.
which in turn can only be satisfied, if the typechecker can find a declaration of the
the instance declaration !Num Float!.

The idea of typeclasses originates from
~\citet{wadler_polymorphism_1989} which modeled typeclass instances as
dictonaries. The implicit class constraints---such as !Num t!---are
elaborated into special data called dictonaries~\cite{hall_type_1994} internally by the
compiler. Mathematically, typeclasses can be viewed as relations
over types~\cite{morris_simple_2014}. Every typeclass instance
declaration extends that relation. The typeclass !Num! represents a unary relation
for those types whose values behave like numbers, they can be compared
(!<=!, !==!) and added together (!+!). The declarations in the
\pref{fig:tc-num} can be interpreted as an evidence of fact that
!Float! and !Int! belong to the unary relation !Num!,
$\{$ !Int!, !Float! $\} \subseteq$ !Num!, and
also !$\tau \to \sigma \not \subseteq$ Num!.


\section{Program Data}
\subsection{Algebraic Datatypes}
The other important aspect of any programming language, the first
being transformations of data described in the previous section, is
that of organizing related data in a logical fashion for the ease of
access and update. The data structures store the information by
encoding the domain elements via an appropriate representation in
memory. The transformation on these structures models the program
logic. Algebraic datatypes (ADT) are a primary way
to define such new structures in a functional programming language. They are called
algebraic because they can be viewed as composite datatype of (named)
sums of products. As an example, consider modeling a simple calculator
application which performs addition operation on integers. The domain,
in this case, are arithmetic expressions and can be represented using an
ADT in Haskell as:

\begin{CenteredBox}
\begin{code}
data AlgExp where
   Value :: Int    -> AlgExp
   Plus  :: AlgExp -> AlgExp -> AlgExp
\end{code}
\end{CenteredBox}

The !data! keyword defines a new user defined datatype with name
!AlgExp!. The data constructor !Value! stores evaluated !Int! values,
while !Plus! encodes the operation of adding the two expressions. Now,
the function of the calculator---to compute expressions---is simulated
by evaluating the encoded expression. This is performed by an !eval!
function defined recursively as shown below:

\begin{CenteredBox}
\begin{code}
eval :: AlgExp -> Int
eval (Value i) = i
eval (Plus x y) = eval x + eval y
\end{code}
\end{CenteredBox}

The declarative style of programming allows us to write the !eval!
function on a case by case basis via pattern
matching. There are exactly two ways in which we could have obtained a
value of type !AlgExp! by virtue of this definition. The first case says
that if we have a !Value i!, we can deduce that
!i! is an !Int! and return it. In the second case, !Plus!, we first
evaluate the expressions !x! and !y! recursively, and then return the
addition of the two results. Another advantage of having a declarative
style is that extending the calculator application, say to include
operations such as multiplication and division operation, is
straightforward. The required code changes would be to add the
associated data constructors, !Mult! and !Div!, in the datatype
declaration followed by extending the !eval! function with cases for
!Mult!, which multiplies, and !Div! that divides.
In a typed setting the typechecker can identify and reject programs
with ill-structured data. The pattern expression !eval (Plus x)! is
ill-typed as !Plus! requires 2 arguments of type !AlgExp!.

The declarative style of defining algebraic datatypes and pattern matching was introduced in
Hope~\cite{burstall_proving_1969, burstall_hope_1980} and has become an
essential feature of all modern typed functional programming languages.

\subsection{Generalized Algebraic Datatypes}
Consider extending the previously defined !AlgExp! to now
include a data constructor !IsZero!. The intention of the constructor is to
check if the expression of type !AlgExp! is evaluated to zero.

\begin{figure}[ht]
\centering
\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
data AlgExp where
  Value  :: Int              -> AlgExp
  Plus   :: AlgExp -> AlgExp -> AlgExp
  IsZero :: AlgExp           -> AlgExp
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
eval :: AlgExp -> Int
eval (Value i) = i
eval (Plus x y) = (eval x) + (eval y)
eval (IsZero x) = (eval x) == 0 -- type error!
\end{code}
\end{CenteredBox}
\end{minipage}
\caption{\texttt{AlgExp} datatype and \texttt{eval} function}
\label{fig:algexp-eval}
\end{figure}

How should the !eval! function handle the !IsZero! case? The naive
definition, !(eval x) == 0!, fails to type check as the !eval!
function requires the return type of the expression to be an !Int! but
it is of type !Bool!. A possible solution is to change the return type
of the the !eval! function to be !Either Int Bool!. This would mean
that the evaluator either returns an !Int! or a !Bool!. While this would work as
expected, it requires a complete rewrite of the !eval! function. The
situation of maintaining this !eval! function would become even more
cumbersome if later we wanted to add a new facility, say, to store and use user
defined functions. Extending !AlgExp! with new constructs would mean
nesting of !Either! datatype and then checking at each recursive
call the value returned is the one that we expect.

To reflect on the situation: the programmer
expects the !eval! function to compositionally evaluate an
expression, then why should they be bothered to wrap the
evaluated values of the !AlgExp! into a data type? This certainly
looks like a code smell, and we should be able to find an appropriate
abstraction to handle this wrapping and unwrapping automatically.
It is important to note that, in full generality, the evaluation of the expression
may not result in the same type. In our case, the !IsZero! evaluates
an expression to a Boolean value, while the !Plus! operator evaluates
to an Integer. Thus, the type signature of the !eval! function should
abstract over the expression result type:
!eval :: FORALL a. AlgExp a -> a!. The problem
would resolve if we were to constrain the return type of each of the
data constructors to agree to the type of the value expected after
evaluating the value of type !AlgExp!. This constrained return type
can be used as a tag to convince the typechecker that !eval! function
is indeed type safe. This formulation is shown in
\pref{fig:galgexp-eval} with the datatype !GAlgExp! along with the !eval!
function which evaluates it.


\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.6\linewidth}
\begin{CenteredBox}
\begin{code}
data GAlgExp a where
  Value  :: Int                        -> GAlgExp Int
  Plus   :: GAlgExp Int -> GAlgExp Int -> GAlgExp Int
  IsZero :: GAlgExp Int                -> GAlgExp Bool
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{CenteredBox}
\begin{code}
eval :: GAlgExp a -> a
eval (Value i) = i                    --(1)
eval (Plus x y) = (eval x) + (eval y) --(2)
eval (IsZero x) = (eval x) == 0       --(3)
                   -- typechecks okay!
\end{code}
\end{CenteredBox}
\end{minipage}%
\caption{\texttt{GAlgExp} datatype and \texttt{eval} function}
\label{fig:galgexp-eval}
\end{figure}

In each of the case alternative branches, the type of the right hand side of a
pattern match agrees with the type constraint introduced by the
pattern. For example, in the alternative case branch labeled (2)
in the \pref{fig:galgexp-eval}, the
pattern !Plus x y! is of type !GAlgExp Int! and the type of the
resultant on the right hand side of the equation is of type !Int!.
Similarly, in the alternative branch labeled (3) with the pattern
!IsZero x! is of type !AlgExp Bool! is on the left hand of the
equation, has a right hand side of the equation
of type !Bool!, which agrees with the type of its right hand side.

GADTs can further help in identifying dead code and code
optimizations~\cite{xi_dead_1998,graf_lower_2020,nilsson_dynamic_2005}.
For example, consider the function !isZero! shown below:

\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
isZero :: GAlgExp Int -> Bool
isZero (Value i) = i == 0
isZero (Plus x y) = isZero x && isZero y
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
isZero :: AlgExp Int -> Bool
isZero (Value i) = i == 0
isZero (Plus x y) = isZero x && isZero y
isZero (IsZero x) = error "impossible"
\end{code}
\end{CenteredBox}
\end{minipage}

The typechecker has the necessary information to identify that the case
!IsZero x! is impossible as it has a type !GAlgExp Bool! while the
function only expects an argument of type !GAlgExp Int!. If we were to
define the same function for !AlgExp Int!, the function
would have to include a bogus case for !IsZero x!.

\begin{minipage}[ht]{0.6\linewidth}
\begin{CenteredBox}
\begin{code}
data GAlgExp a where
  ...
  Equals :: GAlgExp a -> GAlgExp a -> GAlgExp Bool
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{CenteredBox}
\begin{code}
eval :: GAlgExp a -> a
...
eval (Equals x y) = (eval x) == (eval y)
                           -- Type error!
\end{code}
\end{CenteredBox}
\end{minipage}

Consider a generalization of !IsZero x! namely, !Equals x y! as shown
in the above code block. The intention of !Equals x y! to
encode comparing the arguments !x! and !y! and decide if they are
equal. The !eval! function case that evaluates !Equals x y! fails
to type check. The problem is not that the return type does not match,
it is indeed !Bool!, but there is no reason to believe that
!eval x! and !eval y! can be compared using the !(==)! operator. The
only information we can infer from the pattern match is that
the type of !eval x! and !eval y! are of some generic type variable
!a!. Such types are called existential as they do not appear in
the return type. We need to constrain the types of the
arguments to !Equal! to only those that can be compared.
Haskell already supports such a mechanism: \emph{typeclasses}.
By constraining the type variable to only those which satisfy
the !Eq! typeclass, we can convince the typechecker that the
term !(eval x) == (eval y)! is well typed.


\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
data GAlgExp a where
...
   Equals :: Eq a => GAlgExp a -> GAlgExp a
                  -> GAlgExp Bool
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{CenteredBox}
\begin{code}
eval :: GAlgExp a -> ac
...
eval (Equals x y) = (eval x) == (eval y)
                              -- Okay!
\end{code}
\end{CenteredBox}
\end{minipage}

To summarize GADTs generalize the notion of algebraic datatypes in two
distinct ways by uniformly supporting:
\begin{enumerate}
\item phantom or tagged types, where the return type of the data
  constructor is no longer a generic type variable and it is refined
  to be a fixed type;
\item existential types, where it is possible for the type variables
  that appear in the types of the arguments to the data
  constructors to not appear in the return type.
\end{enumerate}

While there is a rich literature of GADTs (inductive type families)
for dependently typed languages~\cite{dybjer_inductive_1991,
  dybjer_inductive_1994}, the idea of GADTs for non-dependently typed
languages appeared under different names such as indexed
types~\cite{zenger_indexed_1997}, first class phantom
types~\cite{cheney_first-class_2003}, guarded recursive
datatypes~\cite{xi_guarded_2003}, equality qualified
types~\cite{sheard_meta-programming_2008}. GHC/Haskell was the only
language compiler that supported GADTs~\cite{peyton_jones_wobbly_2004}
until recently OCaml 4.0~\cite{garrigue_gadt_2011}, Scala 3~\cite{xu_implementing_2021},
and other languages started following the lead of Haskell.


\subsection{Generative Abstract Types}\label{subsubsec:gen-abs-types}
Generative types provide a mechanism for the programmer to confine the
visibility of the representation details of a type
exclusively within a module. External to the module the client program
cannot know how the type is represented or in other words,
representation of the generative types is opaque to the client
module. In Haskell generative types are defined by hiding the type
declarations in the signatures. Consider an HTML module defined in
Haskell as shown in the \pref{fig:html-generative-type}.

\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
module HTML (Html, mkHtml, unMkHtml)
where

newtype Html = Html String

mkHtml :: String -> HTML
mkHtml = Html . escapeString

unMkHtml :: HTML -> String
unMkHtml (Html s) = s
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
module Client
where

import HTML

page :: HTML
page = mkHTML "<html><body>Text</body></html>"





\end{code}
\end{CenteredBox}
\end{minipage}
\caption{HTML as a generative type}
\label{fig:html-generative-type}
\end{figure}

Within the scope of !Html! module, the types !String! and !HTML! are
synonymous, however the type !Html! will be opaque for any
external client using the type !HTML!. The !Client! module will not be
able to directly manipulate a value of type !Html!---unless
of course by using the specific functions exposed in the
signature. Inhibiting manipulations and views of the data
representation can be useful for avoiding leaking information in
secure computation contexts. A naive abstraction, however,
will incur a runtime cost. !Html! and !String!, need to be explicitly
converted from one form into another using pattern matching.
This explicit conversion is redundant as they have the same
representation in memory, but the compiler cannot use this
information to perform any optimization on the generated code.
Is it possible for the runtime semantics of the code be liberated from
the extra overhead?

The idea of generative abstract types first appeared in work of
~\citet{leroy_applicative_1995} and ~\citet{milner_definition_1997}
in context to ML modules systems. Generative abstract types in the
context of Haskell are similar to ~\cite{montagu_modeling_2009}.
While they have a hint of existential types, they exhibit different
semantics.

\section{Type Computation}\label{sec:type-computation}
\subsection{Multiparameter Typeclasses}\label{sec:multiparam-typeclasses}
While single parameter type classes model unary
relations over types, multiparameter typeclasses generalize single
parameter typeclasses by modeling n-ary relations
over types. For example, ~\citet{jones_tcfd_2000} uses the
typeclass !Con e c! to unify the treatment of
different containers and their contained elements as shown in
the \pref{fig:tc-collection}. The type variable !e! stands in for the
element type of the container while !c! stands in for the container
type. The behavior of the containers is captured by the typeclass methods:
!empty!, which returns the empty container, and !insert!,
which inserts the element of type !e! in to the container of type !c!.
We can imagine having instances for such a typeclass as
!Con Int [Int]! that says that the container list of integers has
integers as its elements, !Con Word (Tree Chars)! that says that a
container tree of !Char! contains !Word!s as its
elements. Viewing them as relations,
we have $\{$ !(Int, [Int])!, !(Int, Tree Int)! $\}$ $\in$ !Con!.

\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
class Con e c
where
  empty :: c
  insert :: e -> c -> c
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
instance Con Int [Int]
where
  empty = ...
  insert = ...
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
instance Con Word (Tree Char)
where
  empty = ...
  insert = ...
\end{code}
\end{CenteredBox}
\end{minipage}
\caption[\lstinline{Con} typeclass]{\lstinline{Con} typeclass and its instances}
\label{fig:tc-collection}
\end{figure}

The use of !empty! in a polymorphic setting, however, leads to a type ambiguity during
compilation. For example, consider the term !single3! as below:

\begin{CenteredBox}
\begin{code}
single3 = insert 3 empty
\end{code}
\end{CenteredBox}

the typechecker infers the type of !empty! as !Con e [Int] => [Int]!. The type inferred by
the typechecker is the right one, but we have a free type variable
!e! which appears only in the constraint. We do not have enough
information to determine what the concrete type of !e! should be as
we cannot rule out the existence of another instance such as !Con Float [Int]!.
This results in an ambiguity in compilation step; the compiler cannot
resolve the typeclass constraint to a unique instance and hence cannot
choose the right implementation to use. Types that
give rise to such ambiguity in compilation are called ambiguous types. Formally, if a
type variable only appears in the constraints of the type it is called
an \emph{ambiguous type}. The type of !empty!,
!FORALL e c. Con e c => c!, is ambiguous due to the occurrence of !e! in the
constraint and no where else. Terms with ambiguous types need to be rejected by the compiler
precisely because they do not have a well defined semantics.
A possible solution to this problem, is to resort to assigning local
type annotations, for example,

\begin{CenteredBox}
\begin{code}
single3 :: [Int] = insert (3 :: Int) (empty :: [Int])
\end{code}
\end{CenteredBox}

One might think that the typechecker can now infer the type of the term unambiguously by
instantiating !e! to !Int!, but it still cannot$\bang$ In the
original type inference algorithm, the type constraints are propagated from
the leaves of the AST~\cite{lee_proofs_1998} to its root. Even if it
did solve the problem, the type annotation solution is unsatisfactory for the following reasons:
\begin{itemize}
\item it increases programmers' cognition overhead:
  it relies on the assumption that the programmer knows where to add the
  type annotations by having a deep understanding of how the typechecker
  uses the type annotation information, and worse;
\item it is fragile:
  any changes to the typechecker algorithm may result in unpredictable
  type inference causing previously typecheck-able programs to now fail.
\end{itemize}
% The problem is that the typeclasses are relations making them too
% general for this use case.

\subsection{Functional Dependencies}
The problem with multiparameter type classes is that there is a
mismatch between the programmers intention and the expressivity of the
typeclass machinery. Continuing with the above example, the
programmers intention might be to have a functional relation between !c! and
!e! of the typeclass !Con e c!: fixing the type
parameter !c! also fixes the the type parameter !e!. In general,
there can be arbitrary functional relation between the type parameters
of the typeclass.
\begin{figure}[ht]
\begin{CenteredBox}
\begin{code}
class Con e c | c ~> e where
  empty :: c
  extend :: e -> c -> c
\end{code}
\end{CenteredBox}
\caption[\lstinline{Con} typeclass]{The \lstinline{Con} Typeclass with Functional Dependency}
\label{fig:tc-collection-fd}
\end{figure}
As seen in the \pref{fig:tc-collection-fd}, the new annotation !c ~> e!
on the typeclass definition captures the functional dependency of the typeclass
!Con e c!. It is read as: \emph{for a given type parameter} !c!,
\emph{the type parameter} !e! \emph{can be uniquely
determined}. The parameter !c! is called the determiner, while
the parameter !e! is called the determinant of the
functional dependency. This extension to typeclasses has a good power
to weight ratio; with a minimal change to the typeclass syntax and
complete backwards compatibility with no change to typeclass instance declarations.
Typechecker needs to now verify that the new instance declarations do
not violate the functional dependency: We cannot allow both the typeclass instances,
!Con Int [Int]! and !Con Float [Int]! to exist together.

~\citet{jones_tcfd_2000} introduced functional dependencies as a
conservative extension to the typeclass language feature. His
inspiration stems from relational algebra. Later,
~\citet{jones_language_2008} raise questions about language designs in
presence of functional dependencies, answer some of them while also
clarifying the misunderstandings which had seeped deep into the
functional programming community.

\subsection{Associated Types}
Another way of introducing a functional relation on types, or simply
type functions, would be via associating the determinant of the
functional dependency within the class definition in a more direct fashion.
Using the previous !Con c e! example, if we have a functional
dependency !c ~> e! then, another style to capture the programmers
intention is to use a special type function, say !Elem c!, where ever
the type parameter !e! appears. The !Elem! is a special type function
that takes in a type and returns a type, and is associated with the
typeclass.

\begin{figure}[ht]
\begin{center}
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
class Con c where
  type Elem c
  empty :: c
  insert :: Elem c -> c -> c
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
instance Con [Int] where
  type Elem [Int] = Int
  empty = ...
  insert = ...
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{CenteredBox}
\begin{code}
instance Con (Tree Char) where
  type Elem (Tree Char) = Word
  empty = ...
  insert = ...
\end{code}
\end{CenteredBox}
\end{minipage}
\end{center}
\caption[Con typeclass]{\lstinline{Con} Typeclass with an Associated Type}
\label{fig:assoc-types}
\end{figure}

In the \pref{fig:assoc-types}, the typeclass !Con! needs only one
parameter, !c!, for the container type, with an additional field for
the type function !Elem! that depends on the type parameter !c! of the
typeclass. As a cascading effect, the type signature of !insert!, now only depends on
the type parameter !c!, and the !Elem c! stands for the element
type of container. In the instance declaration, the !Elem [Int]! is mapped to
!Int!, meaning, the element of the list of integers is of type !Int!.
Every multiparameter typeclass, with a functional dependencies can be
rewritten by representing all the determinants of the functional
dependency associated with it to be the associated type. This provides
a mechanism to circumvent the problem of ambiguous types.

Associated types were introduced by ~\citet{chakravarty_associated_2005}
as an alternative for functional dependencies because of a more
direct syntax: functional programmers are acustomend to writing
functions and not relationals as in logic programming.

\part{II: \SFC}\label{part:II}
\section{History and Motivation}
The Glasgow Haskell Compiler (GHC)~\cite{ghc_2020}, is the de facto
Haskell language~\cite{haskell_2010} compiler. It operates in
three major phases. First, the \emph{parsing phase}, ingests the
programmer writing text code, also known as
the surface syntax, in an appropriate abstract syntax tree
representation, second, the \emph{typechecking phase},
checks for type correctness of the surface syntax, and finally the
third \emph{desugaring phase}, compiles the program into a core intermediate
language. Colloquially, these three major phases--parsing,
typechecking, desugaring---are lumped together and called
the front end of the compiler. Haskell is statically typed,
which has practical and theoretical implications;
in practice, only those terms whose types can be verified by the
typechecker can be compiled to the core intermediate language, and in theory,
terms without types have no meanings. The first iteration of GHC
compiler, used \SF as its core intermediate language with
separate extensions to support different language features. In the
rest of this section, We will briefly re-visit the extensions  from
the previous section and motivate why \SF with extensions was
insufficient to encode the advanced language features.

To achieve decidable and complete type inference, Haskell follows the suit
of Hindley-Milner type system~\cite{milner_theory_1978}; the
programmers are not burdened with decorating the terms with explicit
type annotations as the type checking algorithm can infer the most
general type of the term. Implicit parametric~\cite{reynolds_user-defined_1978}, and adhoc
polymorphism~\cite{hall_type_1994} in Haskell can be
translated into \SF as Hindley-Milner type system is just a restricted
version of \SF. The functions from the surface
level language are elaborated to have explicit type arguments. This
means, at the function use site, explicit types need to be applied
before their value arguments can be applied.
For example, the elaborated function !id! in \SF will have a type
!id : FORALL a. a -> a!, while for the term, !id 3!, it will be elaborated
to !id Int 3!. The typeclasses are converted into record types called
dictionaries~\cite{wadler_polymorphism_1989} which contain the
associated methods. The instances of typeclasses are
values of this type. They contain a reference to the method instances.
Elaboration of implicit adhoc polymorphism amounts to finding
the right dictionary to be applied to the function as an explicit
argument in the elaborated version of the function application.

Elaboration of algebraic datatypes into \SF has also been widely
studied in literature and various elaboration (or encoding) schemes
such as Church~\cite{jansen_efficient_2005,jansen_programming_2013},
Mogensen-Scott~\cite{mogensen_efficient_1992}, or Mendler-Parigot
~\cite{parigot_representation_1990, stump_efficiency_2016} can be used.
The data constructors introduced by user defined datatypes are treated as
term constants with its type and appropriate argument are specified in
the declaration. Consider a pathological datatype declaration as shown
below:

\begin{CenteredBox}
\begin{code}
  data T a where MkT :: a -> T a
\end{code}
\end{CenteredBox}

The data constructor !MkT! has type !FORALL a. a -> T a! in the
core language. The programmer written term !MkT 42! for the type
!T Int! will be elaborated to !MkT Int 42!. The example illustrates
that types need to be explicitly applied in the core language and the
elaboration procedure needs to find unique (type) values to fill into the
placeholder arguments for core level functions. In the previous
example, the type !Int! had to be inferred by the typechecker so as
to include it in the elaborated term. The uniqueness criteria for
elaboration is important for the same type ambiguity reason that we saw
in the previous section for multiparameter typeclasses and functional
dependencies.

However, the co-existence of GADTs, generative abstract
types, and associated types would be too cumbersome, if not impossible
to express using orthogonal extensions of pure \SF. Further, the
interaction of each of these seemingly different features together would
be painful for the compiler writer to implement, and programming
language theorists to argue about its
correctness. \SFC~\cite{sulzmann_system_2007} was designed as an
alternative to having separate extensions for
each language feature. The key insight is that each language feature
is, in a sense, trying to encode some notion of type equality
($\tau\sim\sigma$). GADTs introduce type refinements, as seen in
!eval! example, are type equality constraints exposed after a pattern
is matched. Instances of associated types introduce a type equality
between the instantiated associated type and the concrete type
(!Elem [Int] ~ Int!). The generative abstract type introduces an axiom,
instantiations of which are type equalities between the defined type
and its representation. In the case of !newtype Html! declaration:
!Html ~ String!. % By introducing a single construct for type equality
% within the system, all the features described previously can be
% encoded in a fairly straightforward fashion.

In the following sections we describe the meta-theory of the core intermediate
language followed by details of how each of the language features
GADTs (in the \pref{sec:fc-encodes-gadts}), generative abstract
types (\pref{sec:fc-encodes-newtypes}), and associated
types (\pref{sec:fc-encodes-assoctypes}), and also a unique feature of
the system, open type functions (\pref{sec:fc-encodes-opentypefun}) are
encoded in \SFC using \emph{coercions} and \emph{coercion
  axioms} which embody the notion of type equality.

% In one statment: \SFC is an intensional intrinsically typed programming language.
% It is intensional, meaning each term encodes its typing derivation, and it is intrinsically typed, meaning all the terms are index by types.

% Some key points to cover:

% Store the equality between types explicitly in the AST during type checking.

% New feature: coercion is a type and its kind tells us what types does the coercion equate.

% Features that can be directly expressed in \SFC: New types or generative types, associated types, functional dependencies, generalized algebraic datatypes (GADTs).

% Brand new feature user defined open type functions.

% Makes type rewriting complicated. But makes the type system more expressive by making it extensible.
% Proving type soundness is a bit more involved now.

% why don't we have a rule that says: if $\sigma_1 \sim \sigma_2$ then

\section{Syntax}\label{sec:sfc-syntax}
\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Type Variables} &\TyVar,\beta,\Co &\qquad\text{Type Constants} &T \\
 \text{Term Variables} &x,y &\qquad\text{Type Functions} &F\\
 \text{Coercion Vars} &c &\qquad\text{Indices} &i,n \in \mathbb{N}
 \end{syntax}
 \begin{syntax}
 \text{Kinds} &&\kappa \bnfeq& \star \bnfor \kappa \to \kappa \bnfor \shl{\sigma \sim \tau}\\
 \text{Types} &&\tau,\sigma \bnfeq& \TyVar \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor \shl{F\App\many\tau} \bnfor \shl{\Co}\\
 \shl{\text{Coercions}} &&\nu,\Co \bnfeq& c \bnfor \Refl\tau \bnfor \Sym\Co \bnfor \Trans\nu\Co % equiv relation
 \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instantiation
 \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co\\  % compose/decompose
 \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
 \text{Patterns} &&P \bnfeq& H\App \many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\phi\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \shl{\Cast \Tm \Co}\\

 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv,F\co\tau \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Substitutions} &&\Subst \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
 \end{syntax}

 \begin{syntax}
 \text{Program} &&P_{gm} \bnfeq& \many{D_{cl}} \mathrel{;} \many{x = \Tm}\\
 \text{Data Declarations} &&D_{cl} \bnfeq& \textbf{\texttt{data }}\App T\co\many{\kappa} \to \star\App \textbf{\texttt{ where }}\App \many{C_{trs}(T)} \\
 && \bnfor& \textbf{\texttt{type }}\App F : \many\kappa \to \kappa\\
 && \bnfor& \textbf{\texttt{axiom }}\App C\App \many{\TyVar\co\kappa} : \sigma_1 \sim \sigma_2\\
 \text{Data Constructors} &&C_{trs}(T) \bnfeq& H : \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \many\sigma \to T\many\TyVar}\\
 \end{syntax}

 \caption{The Syntax of \SFC}
 \label{fig:sfc-syntax}
\end{figure}

The complete syntax of \SFC is shown in \pref{fig:sfc-syntax}. The
system is classified into three sub-languages or levels: (1) kinds,
(2) types, and (3) terms. The kind language sits at the highest
abstraction level. It consists of base kind ($\STAR$), higher kinds
($\kappa \to \kappa$), and the novel construct for expressing type
equality ($\shl{\tau \sim \sigma}$). Kinds classify types. The type
language contains type variables ($\alpha$), type constructors or
constants $T$, which ranges over built-in types such as !Int!, !(->)!
and other user defined types, and polytypes ($\Forall \alpha \tau)$
for supporting polymorphic functions. Function types ($\tau \to
\sigma$), can very well be considered as a type constructor $T$, but
we give it a special status for the sake of presentation. The absence
of type level lambdas make the type level calculus less expressive by
disallowing certain types such as $\Lam a {T\App a\App Int}$. This is
a cautious decision to ensure type checking remains tractable and
efficient. To make up for the lack of type level lambdas, the system
allows higher kinded types. This allows programmers to define
inductive algebraic datatypes such as !List!s and !Tree!s. \SFC is
impredicative; there is no stratification between the polytypes---the
ones with explicit type abstraction---and
the monotypes---the types with no explicit type abstraction, and they
both have the kind $\STAR$.


The type equality coercion, $\shl{\Co}$, is the novel construct.
It is classified by kind level type equality predicate,
$\shl{\tau\sim\sigma}$. Coercions are first class at the level of types:
they can be constructed, applied to and passed in as arguments, and also
abstracted over by using the special coercion infrastructure.
\SFC supports a full fledged coercion calculus where
each syntactic construct corresponds to a logical equation between two
types. The first basic coercion construct is the
reflexivity construct `$\Refl\tau$': the type $\tau$
witness the (obvious) fact that it is equal to itself. The two other
basic constructs on coercions are symmetry `$\Sym\Co$', which flips
the direction of equality, transitivity `$\Trans {\Co_1}{\Co_2}$',
which chains two coercions $\Co_1$ and $\Co_2$. Reflexivity, symmetry
and transitivity together makes type equality an equivalence class in the
system. Coercions can also be composed and decomposed using the
`$\nu\App\Co$', and `$\Left\Co$' and `$\Right\Co$' constructs
respectively. Finally, coercion abstraction `$\Forall
{\TyVar\co\kappa}\Co$' and coercion application `$\Co\At\tau$' aids
equality reasoning on polytypes. To stress, it is important to notice
that the \pref{fig:sfc-syntax}, places coercions and types in two
different rows only for the sake of a clear presentation, and in
principal \emph{coercions are types}, and we will use $\phi$ to mean
either when the distinction is immaterial. Coercions can also
be introduced as equality axioms which can be thought as a templatized type
equation. For example, an axiom $\Forall \alpha {F\App a \sim a}$
says that for all types, !F a! can be treated exactly as !a!, and vice
versa. We distinguish user defined datatypes $T$ from type function constructors
$F$. Only the saturated application of $F$ can appear on the left hand
side of a coercion axiom head.

Finally, types classify the terms of the system.
The novel term construct of the system is $\shl{\Cast \Tm \Co}$.
It denotes that if a term $M$ has a type $\tau$
and we have a coercion, $\Co$, which says that $\tau\sim\sigma$
then, the term, $\Cast M \Co$, justifies treating $M$ as if it has type
$\sigma$. Alternatively, the type equality coercion can be thought of
as: if $\Co$ justifies $\tau \sim \sigma$ then, within a context that
expects a term $\Tm$ of type $\sigma$, we can freely use the term $\Cast M \Co$,
without worrying that the term will get stuck or crash when we run the
program. The ``will not crash'' guarantee is called as the \emph{type
  soundness property} of the system.
The type coercions introduced in the system should be
thought of as axiomatic type equality, similar to axiomatizing functional
extensionality. However, the type soundness argument of the system, as we
will see later, does not rely on any specific
semantic notion of type equality.

The system has the usual term constructs: term variables `$x$',
term abstractions `$\Lam {x\co\tau} \Tm$' and term applications `$M \App N$',
type level abstraction `$\TLam \TyVar \Tm$' and type applications
`$\Tm\App\sigma$'. Declaration of algebraic datatypes introduces data
constructors $H$, the types of which are of the form:
\[
H \co \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa}} \many{\sigma} \to T\App\many\TyVar}
\]
Here, the type variables, $\many\TyVar$, appear in the same order as in
the algebraic datatype declarations, $T\App\many\TyVar$.
The type variables $\many\beta$ are special; they
do not appear in the return type, $\many\TyVar \cap \many\beta = \emptyset$. This
characterization of splitting the type variables into $\many\alpha$
and $\many\beta$ plays a crucial role in encoding GADTs and
existential types. We will describe this encoding scheme in more
detail in the next \pref{sec:fc-encodes-gadts}.
The $\Case M {\overline{P \to N}}$
discriminates on the shape of the term $M$ and chooses one of the
alternatives $N$ after a successful match on the one of the patterns
$P$.

% The
% soundness property of the type
% system guarantees that after the
% types (including casts) have been
% erased, the program will not crash
% or get stuck. Unlike other regular
% types, however, coercions do not
% classify values, i.e. there are no
% term level constructs that have a
% type $\tau\sim\sigma$.


\subsection{Intermettzo: Notations}\label{sec:notations}
Before we dive deep into the system formalization any further, for the
sake of presentation, we need some helper notations.
The type environment $\TEnv$ contains type mapping for free term variables
`$x\co\tau$', kind mapping for free type variables,
`$\alpha\co\kappa$', along with the term constants `$H\co\tau$', type
function constants `$F\co \tau$', and coercion axioms
`$coAx:\sigma\sim\tau$'. The convention of naming
coercion axioms will be with a $co$ prefix to distinguish them from
non-coercion types. The freshness condition, or that $\alpha$ does not appear in the
domain of $\TEnv$, is denoted by $\fresh \alpha \TEnv$: $\alpha$,, or
$(\TyVar\co\kappa) \not\in \TEnv$. Substitutions, $\Subst$, are mappings from term variables to
types and by abuse of notation, we will also use it to denote type
variables to their kind mappings. Due to naming conventions, we need
not worry about mistakenly substituting a term into a type. We will
use the type constant $(\to)$ and kind constant $(\sim)$ in its infix
form rather than in a prefix form: $\tau \to \sigma$ instead of
$\to \App\tau\App\sigma$, and similarly $\tau\sim\sigma$ instead of
$\sim \tau\App\sigma$. We use the over bar notation $\many\alpha$ to
mean a list of items, in this case a list of $\alpha$s. If the size of
the list is not obvious from the context, we will use explicit
superscript to specify it, $\many\alpha^n$

A program, $P_{gm}$, is a list of programmer written type
declarations, $D_{cl}$, followed by term bindings, $x = M$. Each
$D_{cl}$ introduces a new user defined datatype, a type function, or
an axiom. We assume the convention that the declarations are correctly
ordered, i.e. defined or at least declared before their use. We use
!M :: t! (stress on double colons) to assign (or check) type
!t! for actual program expression !M!, and $\Tm \co \tau$ (stress
on single colon) to mean $\Tm$ has type $\tau$ in the formalization.

\subsection{Static Semantics}\label{sec:sfc-static-sem}

\newcommand\KReflCo{
 \ib{\irule[\trule{co-refl}]
 {\TyKinding \TEnv \tau \kappa};
 {\CoKinding \TEnv {\Refl \tau} {\tau \sim \tau}}
 }
}

\newcommand\KSymCo{
 \ib{\irule[\trule{co-sym}]
 {\CoKinding \TEnv \Co {\tau \sim \sigma}};
 {\CoKinding \TEnv {\Sym \Co} {\sigma \sim \tau}}
 }
}

\newcommand\KTransCo{
 \ib{\irule[\trule{co-trans}]
 {\CoKinding\TEnv {\Co_1} {\tau \sim \tau_2}}
 {\CoKinding\TEnv {\Co_2} {\tau_2 \sim \sigma}};
 {\CoKinding\TEnv {\Trans {\Co_1} \Co_2} {\tau \sim \sigma}}
 }
}

\newcommand\KInstCo{
 \ib{\irule[\trule{co-$\E\forall$}]
 {\CoKinding\TEnv \Co {\Forall\TyVar\tau_1 \sim \Forall\beta\tau_2}}
 {\Subst_1 = \Sub\TyVar\sigma}{\Subst_2 = \Sub\beta\sigma};
 {\CoKinding\TEnv {\Co\At\sigma} {\Subst_1\tau_1 \sim \Subst_2\tau_2}}
 }
}

\newcommand\KForallCo{
 \ib{\irule[\trule{co-$\I\forall$}]
 {\CoKinding {\TEnv,\TyVar\co\kappa} \Co {\tau_1 \sim \tau_2}}{\alpha\#\TEnv};
 {\CoKinding \TEnv {\Forall {\TyVar\co\kappa} \Co} {\Forall {\TyVar\co\kappa}\tau_1 \sim \Forall {\TyVar\co\kappa}\tau_2}}
 }
}

\newcommand\KCoComp{
 \ib{\irule[\trule{co-comp}]
 {\CoKinding \TEnv {\Co_1} {\tau_1 \sim \tau_2}}
 {\CoKinding \TEnv {\Co_2} {\sigma_1 \sim \sigma_2}}
 {\TyKinding \TEnv {\tau_i\App \sigma_i} \kappa};
 {\CoKinding \TEnv {\Co_1\App \Co_2} {\tau_1\App \sigma_1 \sim \tau_2\App \sigma_2}}
 }
}

\newcommand\KCoFComp{
 \ib{\irule[\trule{co-F-comp}]
 {\many{\CoKinding \TEnv \Co {\sigma \sim \tau}}^n}
 {\TyKinding \TEnv {F\App \many\sigma^n} \kappa};
 {\CoKinding \TEnv {F \App \many\Co^n} {F\App\many\sigma^n \sim F\App\many\tau^n}}
 }
}

\newcommand\KLeftCo{
 \ib{\irule[\trule{co-left}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Left \Co} {\tau_1 \sim \tau_2}}
 }
}

\newcommand\KRightCo{
 \ib{\irule[\trule{co-right}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Right \Co} {\sigma_1 \sim \sigma_2}}
 }
}

\newcommand\KCastCo{
 \ib{\irule[\trule{co-leftc}]
 {\CoKinding \TEnv \Co {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
 {\CoKinding \TEnv {\Cast {\Co_1} \Co_2} {\tau_1 \sim \tau_2}}
 }
}

\newcommand\KCoAx{
 \ib{\irule[\trule{co-ax}]
 {\CoKinding \TEnv \Co {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
 {\CoKinding \TEnv {\Cast {\Co_1} \Co_2} {\tau_1 \sim \tau_2}}
 }
}

\newcommand{\KTyVar}{
 \ib{\irule[\trule{ty-var}]
 {\TyVar\co\kappa \in \TEnv};
 {\TyKinding \TEnv \TyVar \kappa}
 }
}
\newcommand{\KTyApp}{
 \ib{\irule[\trule{ty-app}]
 {\TyKinding \TEnv \sigma {\kappa' \to \kappa}}
 {\TyKinding \TEnv \tau \kappa'};
 {\TyKinding \TEnv {\sigma\App\tau} \kappa}
 }
}
\newcommand{\KFCon}{
 \ib{\irule[\trule{ty-fcon}]
 {F \co \many \kappa^n \to \kappa' \in \TEnv}
 {\many {\TyKinding \TEnv {\sigma} {\kappa}}^n};
 {\TyKinding \TEnv {F \many\sigma^n} {\kappa'}}
 }
}
\newcommand{\KTyCon}{
 \ib{\irule[\trule{ty-con}]
 {T \co \kappa \in \TEnv};
 {\TyKinding \TEnv {T} {\kappa}}
 }
}
\newcommand{\KTyAll}{
 \ib{\irule[\trule{ty-all}]
 {\TyKinding {\TEnv,\TyVar\co\kappa} {\sigma} \star}
 {\fresh \TyVar \TEnv};
 {\TyKinding \TEnv {\Forall {\TyVar\co\kappa} \sigma} \star}
 }
}
\newcommand{\KCoConst}{
 \ib{\irule[\trule{co-const}]
 {c\co{\tau \sim\sigma}\in\TEnv};
 {\CoKinding \TEnv c {\tau \sim\sigma}}
 }
}

\begin{figure}[ht]
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \Co \kappa$}\\
 \KReflCo \rsp \KSymCo \rsp \KTransCo \\
 \KForallCo \rsp \KInstCo \\
 \KCoComp \rsp \KCoFComp\\
 \KLeftCo \rsp \KRightCo \\
 \KCoConst
 \end{gather*}
 \caption{Coercion Typing: Excerpt of Static Semantics of \SFC}
 \label{fig:sfc-typing-co}
\end{figure}

To formalize our intuitions of how coercions ought to behave, we
provide static semantics in the form of declarative style typing (or kinding)
judgments rules in \pref{fig:sfc-typing-co}. These judgment rules
are templates, instantiations of which become the building blocks of the
proof. In general, the proofs in this system look like trees. The root of the tree
is the subject theorem judgment, while the leaves are the instantiated axioms, and
the internal nodes of proof tree are the instantiated rules of the
system~\cite{wadler_propositions_2015}.

All coercions are types such that their kinds tell us which two types
considered equal. We read the coercion kinding judgment
\fbox{$\CoKinding \Gamma \Co \kappa$} as, ``under the assumptions in
$\Gamma$, $\Co$ provably has the $\kappa$''. The kinding rules
\trule{co-refl}, \trule{co-sym} and \trule{co-trans} makes coercion an
equivalence relation. The rules \trule{co-$\E\forall$} and
\trule{co-$\I\forall$} justifies coercions between polytypes and their
instantiations. If two polytypes are equal, then their instantiations
with equal types are also equal \trule{co-$\E\forall$}, and similarly
if two types, with a free type variable, are equal then type
abstraction on both the types yields equal polytypes
\trule{co-$\I\forall$}. The rule \trule{co-comp} enables lifting
coercions for higher kinded types and enable reasoning of equalities
between them and similarly the rule \trule{co-F-comp} enables coercion
lifting for fully saturated type functions.


\begin{figure}[ht]
\begin{gather*}
 \fbox{$\TyKinding \TEnv \tau \kappa$}\\
 \KTyVar \rsp \KTyApp \\
 \KTyCon \rsp \KFCon \rsp \KTyAll
\end{gather*}
 \caption{Kinding Judgments: Excerpt of Static Semantics of \SFC}
 \label{fig:sfc-typing-ki}
\end{figure}

The kinding rules for types, \fbox{$\TyKinding \TEnv \tau \kappa$},
which are not coercions, are listed in \pref{fig:sfc-typing-ki}.
They are fairly standard in comparison to \SF. The rule $\trule{ty-app}$
enables kinding applications of higher kinded types like !List Int! or
!Tree Float!. Note that the kinding judgment rules $\trule{ty-fcon}$
and $\trule{ty-con}$ are distinct as only fully saturated type
function constructors are valid types in the system. The impredicative
nature of \SFC is evident from the rule $\trule{ty-all}$: type
quantification is a closed operation over types in the universe of kinds.

To illustrate why coercion composition is useful, consider a higher
kinded algebraic type !Tree! and a coercion
$\Co\co\sigma_1\sim\sigma_2$, then using $\tau_1$ and $\tau_2$ to be
equal to !Tree!, we have that $\Refl{\texttt{Tree}} \App\Co :
\texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_2$. On the
other hand, if we have a coercion $\texttt{Tree}\App\sigma_1 \sim
\texttt{Tree}\App\sigma_1$ then we can recover the coercion components
using the \trule{co-left} and \trule{co-right}, for the higher kinded
type and its arguments respectively. In full generality, we can state
the lifting property formally in \SFC using
\pref{thm:sfc-coercion-lifting}. It captures the idea that
if we have two equal types, substituting it with two equal sub-parts
in them maintains the type equality.

\begin{theorem}[Coercion Lifting]\label{thm:sfc-coercion-lifting}
 If $\TyKinding {\TEnv,\TyVar\co\kappa'}\phi\kappa$, where $\TyVar$ is free in $\phi$
 and does not appear free in $\TEnv$,
 $\CoKinding\TEnv\Co{\sigma_1\sim\sigma_2}$, and $\TyKinding\TEnv{\sigma_i}\kappa'$
 then, $\CoKinding\TEnv{\Set{\TyVar\mapsto \Co}\Refl\phi}
 {\Set{\TyVar\mapsto\sigma_1}\phi\sim\Set{\TyVar\mapsto\sigma_2}\phi}$
\end{theorem}
\begin{proof}[Proof Sketch \pref{thm:sfc-coercion-lifting}]
 Proof is by induction on the derivation of the well kinded type
 $\phi$. In each of the four cases, whenever in the original
 derivation the rule \trule{co-refl} was used, it is replaced by the
 derivation of $\CoKinding \TEnv \Co {\sigma_1\sim\sigma_2}$
\end{proof}

\newcommand\TVar{
 \ib{\irule[\trule{var}]
 {x\co\tau \in \TEnv};
 {\Typing \TEnv x \tau}
 }
}

\newcommand\TAbs{
 \ib{\irule[\trule{\I\to}]
 {\Typing {\TEnv,x\co\sigma} {M} {\tau}};
 {\Typing \TEnv {\Lam x M} {\sigma \to \tau}}
 }
}
\newcommand\TApp{
 \ib{\irule[\trule{\E\to}]
 {\Typing \TEnv \Tm {\sigma \to \tau}}
 {\Typing \TEnv N \sigma};
 {\Typing \TEnv {\Tm \App N} {\tau}}
 }
}
\newcommand\TTyApp{
 \ib{\irule[\trule{\E\forall}]
 {\Typing  \TEnv \Tm {\Forall {\alpha\co\kappa} \tau}}
 {\Kinding \TEnv \sigma \kappa};
 {\Typing  \TEnv {M\App\sigma} {\tau}}
 }
}

\newcommand\TTyAbs{
 \ib{\irule[\trule{\I\forall}]
   {\Typing {\TEnv,\alpha\co\kappa} \Tm \tau}
   {\alpha\#\TEnv};
   {\Typing \TEnv {\Forall {\alpha\co\kappa} \Tm} {\tau}}
 }
}

\newcommand\TAlt{
 \ib{\irule[\trule{alt}]
 {H\co{\Forall{\many{\alpha\co\kappa}}{\Forall{\many{\beta\co\iota}}{\many\sigma \to T\many\alpha}}}\in{\TEnv}}
 {\Subst = \Set{\many{\alpha \mapsto \tau'}}}
 {\Typing {\TEnv, \many{\beta\co\Subst\iota}, \many{x\co\Subst\sigma}} {N} {\tau} };
 {\Typing \TEnv {H\App\many{\beta\co\Subst\kappa}\App\many{x\co\Subst\sigma} \to N} {T\many{\tau'} \to \tau}}
 }
}

\newcommand\TCast{
 \ib{\irule[\trule{cast}]
 {\Typing \TEnv {\Tm} {\tau}}
 {\CoKinding \TEnv \Co {\tau \sim \sigma}};
 {\Typing \TEnv {\Cast \Tm \Co} {\sigma}}
 }
}
\newcommand\TCase{
 \ib{\irule[\trule{case}]
 {\Typing \TEnv {\Tm} {\sigma}}
 {\many{\Typing \TEnv {P \to N} {\sigma \to \tau}}};
 {\Typing \TEnv {\Case \Tm {\many{P \to N}}} {\tau}}
 }
}

\begin{figure}[ht]
\begin{gather*}
  \fbox{$\Typing \TEnv M \tau$}\\
  \TVar   \rsp \TAbs \rsp \TApp\\
  \TTyAbs \rsp \TTyApp \\
  \TCast  \rsp \TCase \\
  \TAlt
\end{gather*}

 \caption{Typing Judgments: Excerpt of Static Semantics of \SFC}
 \label{fig:sfc-typing-ty}
\end{figure}

A typing rules for terms, \fbox{$\Typing \TEnv \Tm \tau$}, is
shown in \pref{fig:sfc-typing-ty}. The rules inherited from \SF
are $\trule{var}$, $\trule{\I\to}$, $\trule{\E\to}$,
$\trule{\I\forall}$, and $\trule{\E\forall}$. The novel rule
$\trule{cast}$ transforms a term $M\co\tau$ to a term $M\co\sigma$ with
a witness coercion $\Co\co\tau\sim\sigma$.

The two typing rules worth discussing are the ones for typing case
statements $\trule{ty-case}$ and typing alternatives
$\trule{ty-alt}$. A case statement can only be well typed if the
discriminant has a type $\sigma$ and each of the alternatives $P \to
N$ have the same type $\sigma \to \tau$. For alternatives, if the
pattern is a data constructor, only the existential type variables
$\many\beta$ are brought into scope explicitly in the pattern.
The reason to do this is to ensure that the continuation term, $N$, can use
the existentially bounded variables. However, we should be careful to
not let the existential variables escape their scope. This invariant
needs to be enforced by the elaboration step from surface syntax into
the core language as \SFC is a general calculus.
For example, consider the data constructor !IsEquals! and its type
with explicit kind annotations from the previous section.

\begin{CenteredBox}
\begin{code}
IsEquals :: FORALL (a :: *). FORALL (b :: *). DEq b -> b -> b -> GAlgExp a
\end{code}
\end{CenteredBox}

Inside a case term the alternative has the form

\begin{CenteredBox}
\begin{code}
(IsEquals (b :: *). (d :: Eq b) (x :: b) (y :: b)) -> N
\end{code}
\end{CenteredBox}

where, $N$ is the continuation term if the discriminant matches the pattern
!IsEquals!, and the existential variable !d! the dictionary for the !Eq!
typeclass, instances of which implements equality behavior over types.
This type transformation is sound as the type of !IsEquals! is
isomorphic to the following type:

\begin{CenteredBox}
\begin{code}
FORALL (a :: *). (EXISTS (b :: *). (Eq b, b, b)) -> GAlgExp a
\end{code}
\end{CenteredBox}

\subsection{Operational Semantics}\label{sec:sfc-op-sem}
\newcommand{\Beta}{
 \ib{\irule[\trule{$\beta$}]
 {};
 {$\stepsto {(\Lam {x\co\tau} M) \App N} {\Set{x\mapsto N}M}$}
 }
}
\newcommand{\TBeta}{
 \ib{\irule[\trule{Ty-$\beta$}]
 {};
 {$\stepsto {(\TLam \TyVar M) \App \tau} {\Set{\TyVar\mapsto \tau}M}$}
 }
}
\newcommand{\CaseE}{
 \ib{\irule[\trule{case}]
 {};
 {\stepsto {\Case {(K \many\sigma\many\phi\many\Tm)} {\Set{...; K\App\many\beta\App\many x \to N; ...}}} {\Set{\many {\beta\mapsto\phi}, \many{x\mapsto\Tm}}N}}
 }
}
\newcommand{\CoTransE}{
 \ib{\irule[\trule{Co-Trans}]
 {};
 {$\stepsto {\Cast {(\Cast \Val \Co)} {\nu}} {\Cast \Val {(\Trans{\Co} {\nu})}}$}
 }
}

\newcommand{\TyPush}{
 \ib{\irule[\trule{ty-push}];
    % {\Co : {\Forall {c\co\kappa} \tau} \sim \Forall {c\co\kappa} \tau'};
 {$\stepsto {(\Cast{\TLam {\TyVar\co\kappa} M}\Co)\App \tau} {({\TLam {\TyVar\co\kappa} (\Cast M {\Co\At\TyVar})})\App \tau}$}
 }
}

\newcommand{\CoPush}{
 \ib{\irule[\trule{co-push}]
 {\substack {\mathlarger{\nu\co \sigma_1' \sim \sigma_2'}\\
 \mathlarger{\Co_1 : \sigma_1 \sim \sigma_1' = \Left {(\Left \Co)}}}}
 {\substack {\mathlarger{\Co\co (\sigma_1 \sim \sigma_2 \then \sigma_3) \sim (\sigma_1' \sim \sigma_2' \then \sigma_3')}\\
 \mathlarger{{\Co_2: \sigma_2 \sim \sigma_2' = \Right{(\Left\Co)}\quad{\Co_3:\sigma_3\sim\sigma_3' = \Right\Co}}}}};
 {$\stepsto {(\Cast{\TLam {\TyVar\co(\sigma_1\sim\sigma_2)} M}\Co)\App \nu} {\Cast {(\TLam {\TyVar\co(\sigma_1\sim\sigma_2)} M)\App (\Co_1 \circ \nu \circ \Sym \Co_2)} {\Co_3}} $}
 }
}

\newcommand{\Push}{
 \ib{\irule[\trule{push}]
 {\Co : \tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}
 {\Co_1 = \Right (\Left \Co)}
 {\Co_2 = \Right \Co};
 {$\stepsto {({\Cast {(\Lam x M)} {\Co}}) \App N} {\Cast {(({\Lam x M})\App {(\Cast N {\Sym \Co_1})})} \Co_2}$}
 }
}

\newcommand{\HPush}{
 \ib{\irule[\trule{h-push}]
 {\substack{\mathlarger{\Co : T\App\many\sigma \sim T\App\many\tau}\\
 \mathlarger{H : \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \rho \to T\App\many\TyVar^n}}}}
 {\substack{\mathlarger{{\Subst = \Set{\many{\TyVar_i \mapsto \Co_i}, \many{\beta_i \mapsto \phi_i}}}}\\
 \mathlarger{\Tm'_i = \Cast {\Tm_i} \Subst\rho_i}}}
 {\Co_i = \Right (\Left^{i-1}\Co) }
 {\phi' =
 \begin{cases}
 \Cast {\phi_i} \Subst(v_1 \sim v_2) &\text{if }\beta_i:v_1 \sim v_2\\
 \phi_i\quad &\text{otherwise}
 \end{cases}
 };
 {$\stepsto {\Case {(\Cast {H\App \many\sigma\App\many\phi\App\many\Tm} \Co)} {\many{\Ptrns \to N}} }
 {\Case {(H\App \many\tau\App\many{\phi'}\App\many{\Tm'})} {\many{\Ptrns \to N}} }$}
 }
}

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Value Types} && T\Val &::= T \bnfor \tau \to \tau \bnfor \Forall {\TyVar\co\kappa}\\
 \text{Plain Values} && \Val &::= H \bnfor \Lam {x\co\tau} M \bnfor \TLam {\TyVar\co\kappa} M \\
 \text{CValues} && C\Val &::= \Val \bnfor \Cast \Val \Co\\

 \text{Evaluation Contexts} && \EvalCtxt &::= \EvalCtxtHole{-} \bnfor \EvalCtxt\App M \bnfor \EvalCtxt\App \tau \bnfor \Cast \EvalCtxt \Co \bnfor \Case \EvalCtxt {\many{P}}\\
 \end{syntax}
\end{figure}

Now that we have established the static semantics for \SFC, we
analyze the operational semantics, or the run time behavior, of the
types and terms of the language. The value terms fall into two
categories: normal values,  $\Val$, or coercion values, $C\Val$,
which are values with coercion casts.  Value terms denote the ``good''
terms of the language which do not evaluate. For example, !True!, of
type !Bool! is a value term. Coercion values, or cvalues, are needed
to maintain the type preservation property. We use $\EvalCtxt$
notation to identify which sub-term reduces in the next step. This
presentation uses call by need semantics, but it may as well use call
by value without affecting the soundness properties.
The terms involving coercions can step in one of the four interesting
ways given by the rules \trule{push}, \trule{ty-push},
\trule{co-push}, or \trule{h-push} shown in the \pref{fig:op-sem-sfc}.
\begin{figure}[ht]
 \centering
 \begin{gather*}
 \fbox{$\stepsto M N$}\\
 \Push \rsp \TyPush\\
 \CoPush\\
 {\mathsmaller{\HPush}}\\
 \Beta \rsp \TBeta\\
 \CaseE \rsp \CoTransE
 \end{gather*}
 \caption{Operational Semantics of \SFC}
 \label{fig:op-sem-sfc}
\end{figure}

In the rule \trule{push}, the coercion $\Co$, which is applied to the
lambda term, is decomposed into two coercions; the first coercion,
`$\Co_1$', is used in the coercion applied to the argument, the second
coercion `$\Co_2$', is used to apply the complete term. Applying this
transformation rule exposes a \trule{$\beta$}-redux. This shows that
coercions do not interfere with the function applications. The rule
\trule{ty-push} for type application moves the coercion inside a type
abstraction instantiated at the type variable. The rule
\trule{co-push} is just like \trule{push} but for moving coercions
inside a coercion abstraction. Just like in the rule \trule{push}, we see that
the rule \trule{ty-push} enables evaluation for type functions applied
to a coercion. The rule \trule{co-push} is similar to the rule
\trule{ty-push} but works on coercion abstractions. The type and
coercion calculation is not arbitrary, it ensures that the types are
preserved when we push the coercions inside the body of a lambda bound term.

The most complex rule, \trule{h-push}, pushes the coercion within
the case term scrutinee. This rule is best illustrated with an example: consider
the case scrutinee

\begin{CenteredBox}
\begin{code}
Cons (T Bool) x xs /> $\Co$
\end{code}
\end{CenteredBox}

where, !Cons : FORALL a. a -> [a] -> [a]!,
!$\Co$ : [T Bool] ~ [Int]!, and !T! is a type constant.
The cast transforms the scrutinee into a type ![Int]! by pushing
the coercion into its sub-components.
\[
\stepsto {\Cast {(\texttt{Cons}\App \texttt{(T Bool)}\App \texttt{x}\App
    \texttt{y})} \Co} {\texttt{Cons} \App \texttt{Int}\App
  (\Cast {\texttt{x}} \Right\Co) (\Cast {\texttt{xs}}
  {(\Refl{[]}\Right\Co)})}
\]
After this transformation, the term is no longer is a
coercion term, but a data constructor, and can be used
to match and make progress using the rule \trule{case}.
Coercion lifting plays an important role: it ensures that the term
sub-components $\Cast M_i {\Subst}\rho_i$ are of
the appropriate type. Finally, the rule \trule{co-trans}
flattens a chain of two coercions, $\Co$ and $\nu$, into a casted term
with one coercion, $\Trans\Co\nu$.

The other rules, \trule{$\beta$}, \trule{ty-$\beta$} and \trule{case}
are straightforward and are carried over from
\SF~\cite{pierce_tapl_2002}. As a short hand to chain multiple rules of
the operational semantic, we define a  multi-step reduction. It is a
reflexive transitive closure of the single step operational semantics,
$\stepsto \Tm N$.

\newcommand\MultiStepRefl{
    \ib{\irule[\trule{$\rightsquigarrow^*-r$}];
      {\manystepsto \Tm \Tm}
    }
}
\newcommand\MultiStepLift{
    \ib{\irule[\trule{$\rightsquigarrow^*-r$}]
      {\stepsto \Tm N};
      {\manystepsto \Tm N}
    }
}
\newcommand\MultiStepTrans{
    \ib{\irule[\trule{$\rightsquigarrow^*-t$}]
      {\stepsto \Tm {\Tm'}}
      {\manystepsto {\Tm'} N};
      {\manystepsto \Tm N}
    }
}

\begin{defn}[Multi step reduction relation: $\manystepsto \Tm N$]\label{defn:multi-step-reduction}
  \begin{gather*}
    \MultiStepRefl \rsp \MultiStepLift \rsp \MultiStepTrans
  \end{gather*}
\end{defn}


The operational calculus of coercions is not required during
actual runtime. They are required to prove important
meta-theoretic property of the calculus. The system maintains a strict
phase distinction where the coercions and types can be safely erased
after the type checking phase.

\subsection{Meta-theory}
\subsubsection{Soundness}
The above formalization aids us to state and prove
the important property of the system: do the static semantics
effectively weed out all the programs that may fail at runtime?
A way to show soundness for a system is by first proving the subject
reduction property~\cite{wright_syntactic_1994}.

\begin{claim}[Progress and Subject Reduction]\label{claim:sfc-ty-safety}
 If $\Typing \TEnv \Tm \tau$ then, either $\Tm \in C\Val$ or, $\stepsto \Tm \Tm'$ and
 $\Typing \TEnv {\Tm'} \tau$
\end{claim}

Informally, if we have a well typed term $\Tm$, with type $\tau$, then
there can be two possible cases:
\begin{enumerate}
\item the term is a value, and it cannot evaluate any further, or;
\item it can either evaluate using one of the rules given by the
  operational semantics.
\end{enumerate}
In practice, this shows that a well typed term in the system
cannot raise in an exception, as all those terms would already have
been rejected by the typechecker.

\subsubsection{Consistency}
An important step while proving subject reduction would be to ensure that we
can never derive anything obviously wrong such as !coBAD : Int ~ Bool!
in our system. The coercion, !coBAD!, has the power to cast a term of
a type !Int! into !Bool!. If such unsound equalities were
derivable within the system, any term that has a type !Int! could be
casted into a term !Bool!. This would mean we would be able to escape
the protection provided by the typechecker. In other words, the
coercion, !coBAD!, can be used in a cast, to convert an ill-typed term
to a well typed term and break subject reduction property. The
\pref{claim:sfc-ty-safety} needs to be strengthened using an
appropriate restriction on $\TEnv$. In general, checking consistency
at top level is an undecidable. There exists is a conservative
syntactic check which is sufficient for our purpose.

\begin{definition}[$\Good\TEnv$]
 A type environment, $\TEnv$, is $\Good\TEnv$ when it satisfies the
 following properties:
 \begin{itemize}
 \item If $\CoKinding \TEnv \Co {T \many\sigma \sim \tau}$ and $\tau$
   is a value type, then $\tau$ is of the form $T\App\many\sigma'$
 \item If $\CoKinding \TEnv \Co {(\sigma' \to \sigma) \sim \tau}$ and
   $\tau$ is a value type, then $\tau$ is of the form $\tau' \to \tau''$
 \item If $\CoKinding \TEnv \Co {\Forall {\TyVar\co\kappa} \sigma \sim
     \tau}$ and $\tau$ is a value type, then $\tau$ is of the form
   $\Forall {\TyVar\co\kappa} \sigma'$
 \end{itemize}
\end{definition}

\begin{theorem}[Progress and Subject Reduction]\label{thm:progress-sfc}
 If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ then, either $\Tm \in
 C\Val$ or, $\stepsto \Tm \Tm'$ and $\Typing \TEnv {\Tm'} \tau$
\end{theorem}

It is important to note that the consistency criteria is
dependent on the language feature. We will see in detail
how the consistency criteria falls out naturally in the following
section when we try to encode language different language features in to
\SFC. \pref{thm:progress-sfc} also establishes the phase distinction
property~\cite{harper_higher-order_1989} where types no not
interfere with the execution of the program: even if we erase all the
type and kind annotations from a well typed term, the term does not
crash at runtime, or that the types do not hold any computational
significance. Phase distinction is important as it provides formal
guarantee that the efficient representation of the terms will not
change its meaning. We formalize this in the
\pref{lemma:sfc-syntax-soundness}.


The erasure function, $\Erased{-}$, is defined on the term structure.
It erases all the type and coercion annotations from a \SFC term and
obtains an untyped lambda calculus term. The term, $\Unit$, is a
special term constant. It contains no computational information and is
used to simulate application of type level lambdas.

\begin{defn}[Erasure of a \SFC Terms: $\Erased{\Tm}$]\label{defn:term-erasure}\hfill{}

  \begin{CenteredBox}
    \begin{minipage}[ht]{0.5\linewidth}
      \begin{flalign*}
        \Erased{x\co\tau}            &= x\\
        \Erased{\Lam {x\co\tau} \Tm} &=\Lam x\Erased{\Tm}\\
        \Erased{\TLam \alpha \Tm}    &= \Lam \alpha {\Erased \Tm}\\
        \Erased{H\App\many{\beta\co\kappa}\App\many{x\co\tau}} &= H\App \many\beta\App\many x
      \end{flalign*}
    \end{minipage}%
    \begin{minipage}[ht]{0.5\linewidth}
      \begin{flalign*}
        \Erased{\Cast \Tm \Co} &= \Erased \Tm\\
        \Erased{\Tm\App N} &= \Erased{\Tm}\App\Erased{N}\\
        \Erased{\Tm \App \tau} &= \Erased{\Tm}\App\Unit\\
        \Erased{H} &= H
      \end{flalign*}
    \end{minipage}
  \end{CenteredBox}

  \begin{CenteredBox}
    \begin{minipage}{1.0\linewidth}
      \begin{flalign*}
        \Erased{\Case N {\many{\Ptrns \to \Tm}}} &= \Case {\Erased{N}}
                                               {\many{\Erased{\Ptrns} \to \Erased{\Tm}}}
      \end{flalign*}
    \end{minipage}
  \end{CenteredBox}
\end{defn}

We can now state the important property of the system in spirit of
Milner's ``Well typed programs don't go
wrong''~\cite{milner_theory_1978}.
\begin{corollary}[Syntactic Soundness]\label{lemma:sfc-syntax-soundness}
  If $\Good\TEnv$ and $\Typing \TEnv \Tm \tau$,
  then $\manystepsto \Tm \Val$ iff $\manystepsto {\Erased\Tm} \Val$ where $\Val$ is
  a value of a ground type, or $\Tm$ diverges.
\end{corollary}

% reason for divergence is that we have datatypes!

\section{Encoding Language Features in \SFC}\label{sec:sfc-encoding-features}%%%%%%%%%
\subsection{GADTs}\label{sec:fc-encodes-gadts}
\begin{figure}[ht]
\centering
\begin{syntax}
\text{Constraints} && C \bnfeq& \empt \bnfor C, c\co\tau\sim\tau'\\
\text{Monotypes} && v,\tau \bnfeq& \TyVar \bnfor \tau\to\tau \bnfor T\App\many\tau\\
\text{Constrained Types} && \eta \bnfeq& \tau \bnfor C \then \eta\\
\text{Polytypes} && \pi \bnfeq& \eta \bnfor \Forall\TyVar\pi
\end{syntax}
\caption{GADT Surface level type syntax}
\label{fig:gadt-type-syntax}
\end{figure}

We now formalize how GADTs can be encoded into \SFC
using a type directed elaboration. Although \SFC does not distinguish
between monotypes and polytypes, the surface level syntax does. This
is due to pragmatic reasons; if higher ranked type schemes were allowed
it makes the type inference non-trivial~\cite{jones_practical_2007}.
In the \pref{fig:gadt-type-syntax} constraints, $C$ are type equality
predicates. The monotypes, $\tau$, can be constrained using these type
equality constraints. The polytypes, $\pi$, are quantified constrained types.


\newcommand\GADTVar{
 \ib{\irule[\trule{g-var}]
 {x\co\pi \in \TEnv};
 {\GTranslate C \TEnv x {\pi} x}
 }
}
\newcommand\GADTEq{
 \ib{\irule[\trule{g-eq}]
 {\GTranslate C \TEnv \Tm \tau \Tm'}
 {\CoKinding C \Co {\tau \sim \tau'}};
 {\GTranslate C \TEnv \Tm {\tau'} {\Cast {\Tm'} \Co}}
 }
}
\newcommand\GADTForallI{
 \ib{\irule[\trule{g-$\I\forall$}]
 {\GTranslate C \TEnv \Tm \pi \Tm'}
 {\fresh \TyVar {C, \TEnv}};
 {\GTranslate C \TEnv \Tm {\Forall {\TyVar\co\star} \pi} {\TLam {\TyVar\co\star} \Tm'}}
 }
}
\newcommand\GADTForallE{
 \ib{\irule[\trule{g-$\E\forall$}]
 {\GTranslate C \TEnv \Tm {\Forall {\TyVar\co\star} \pi} \Tm'};
 {\GTranslate C \TEnv \Tm {\Set{\TyVar\mapsto\tau}\pi} {\Tm'\App \tau}}
 }
}
\newcommand\GADTCI{
 \ib{\irule[\trule{g-$\I C$}]
 {\GTranslate {C,c:\tau\sim\tau'} \TEnv \Tm {\eta} \Tm'};
 {\GTranslate C \TEnv \Tm {\tau\sim\tau'\then\eta} {\TLam {(c\co\tau\sim\tau')} \Tm'}}
 }
}
\newcommand\GADTCE{
 \ib{\irule[\trule{g-$\E C$}]
 {\GTranslate {C} \TEnv \Tm {\tau\sim\tau'\then\eta} \Tm'}
 {\CoKinding C \Co \tau\sim\tau'};
 {\GTranslate C \TEnv \Tm {\eta} {\Tm'\App\Co}}
 }
}
\newcommand\GADTAlt{
 \ib{\irule[\trule{g-alt}]
 {\substack{
 \mathlarger{H\co \Forall {\many\TyVar} {\Forall {\many\beta} {\many{\tau'\sim\tau''} \then \many\tau \to T\many\TyVar}}}\quad
 \mathlarger{\many\TyVar \cap \many\beta = \varnothing}\quad
 \mathlarger{\fvs{\many\tau, \many{\tau'}, \many{\tau''}} = \fvs{\many\TyVar, \many\beta}}\quad
 \mathlarger{\Subst = \Set{\many{\TyVar\mapsto v}}}\quad
 \mathlarger{\fresh {\many{c}} {C, \TEnv}}\\
 \mathlarger{\GTranslate {C,\many{c\co\Subst{\tau'}\sim\Subst\tau''}\,} {\,\TEnv,\many{x\co\Subst\tau}\,} \Tm {\tau'} \Tm'} }};
 {\GTranslate C \TEnv {H\App\many x \to \Tm} {T\App\many v \to \tau'}
 {H\App(\many{\beta\co\star})\App(\many{c\co\Subst\tau'\sim\Subst\tau''})\App(\many{x\co\Subst\tau}) \to \Tm' }}
 }
}
\begin{figure}[h]
 \centering
 \begin{gather*}
 \fbox{$\GTranslate C \TEnv {\Tm} {\pi} {\Tm'}$}\\
 \GADTVar \rsp \GADTEq\\
 \GADTForallI \rsp \GADTForallE\\
 \GADTCI \rsp \GADTCE
 \end{gather*}
 \begin{gather*}
 \fbox{$\GTranslate C \TEnv {p \to \Tm} {\pi \to \pi} {p' \to e'}$}\\
 \GADTAlt
 \end{gather*}
 \caption[Encoding GADTs]{Type-directed Translation of GADTs in \SFC}
 \label{fig:encoding-gadts}
\end{figure}

The typing-cum-elaboration judgment
\fbox{$\GTranslate C \TEnv \Tm \pi \Tm'$} denotes that: given a well
typed surface level term $\Tm$ with type $\pi$, it is elaborated to a
term $\Tm'$ in \SFC under the constraints C and typing environment
$\TEnv$. The key idea of the elaboration is that the type equality
constraints, $\tau\sim\tau'$, are elaborated to coercions in \SFC. The
constraint $C$ is a collection of named type equalities. The rules
\trule{g-var}, $\trule{g-\I\forall}$ and \trule{g-$\E\forall$} are
standard rules used for elaborating Hindley-Milner style
system~\cite{wadler_polymorphism_1989} to \SF while \trule{g-$\I C$}
and \trule{g-$\E C$} reminiscent of elaborating typeclass
constraints. In the implementation, the solver would produce the
coercion $\Co$ using the list of assumptions $\TEnv$ in
$\trule{g-\E\forall}$. The complex looking \trule{g-alt} elaborates
case statements into \SFC. Each surface level data constructor is
elaborated to the \SFC data constructor with explicit existential
coercions as pattern variables. The rule $\trule{g-eq}$ is the same as
\trule{cast} rule. The important detail here is that the coercion,
$\Co$, is inferred from the constraint context $C$. Constructing the
appropriate $\Co$ algorithmically is possible by using a simple
unification algorithm based on ~\cite{lassez_unification_1988}.

We can now demystify the type magic that the !eval! function
performed by elaborating the definition of the eval function into \SFC.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
data GAlgExp :: * -> * where
  Value  :: FORALL a. a -> GAlgExp a
  Plus   :: FORALL a. (a ~ Int) => GAlgExp a
                              -> GAlgExp a
                              -> GAlgExp a
  IsZero :: FORALL a. (a ~ Bool) => Int -> GAlgExp a
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
eval :: FORALL a. GAlgExp a -> a
eval (Value x) = x
eval (Plus (co :: a ~ Int) (x :: Int) (y :: Int))
   = (((eval (x /> sym co)) /> co)
     + ((eval (y /> sym co)) /> co)) /> sym co
eval (IsZero (co :: a ~ Bool) (x :: Int))
   = (isZero x) /> sym co
\end{code}
\end{minipage}

In case alternative for !Value!, the argument to the constructor is a generic
variable of type !a!, and it agrees with the return type of the !eval!
function. In the case alternative of !IsZero!, an existential
coercion, !co :: a ~ Bool!, is brought into scope. Recall that the type of the
function !isZero! is !Int -> Bool!. The coercion, !co!, can
be used to refine the result type of !isZero x! to
the generic variable !a! from the actual type !Bool!. The complicated
looking !Plus!, can be read systematically: Each argument
!x! and !y! is first, coerced to a generic type !a! using the
existential coercion !co : a ~ Int! and then coerced back into !Int!
to perform the addition operation (!+!). Before returning, it is
casted back into the generic type variable !a!.


The following lemmas are extensions of the meta-theoretic properties
discussed in the previous section. We want to formalize the argument
that ensures we have not lost the subject reduction property proved in
the \pref{thm:progress-sfc}.

\begin{lemma}[Type Preservation]
 If $\GTranslate C \empt \Tm \tau {\Tm'}$ then $\Typing C {\Tm'} \tau$
\end{lemma}
Type preservation is an important sanity check for the
elaborations. It says that the the elaborated terms have
the same type that was inferred at surface level.
\begin{theorem}[GADT Consistency]\label{thm:gadt-consistency}
 If $\dom\TEnv$ does not contain type variables and coercion
 constants, and $\CoKinding \TEnv \Co {\tau\sim\tau'}$
 then, $\tau$ and $\tau'$ are syntactically identical.
\end{theorem}
\pref{thm:gadt-consistency} says that if two base types, or type
function free types, are provably equal, then they must be
syntactically identical. Finally, all GADT programs are sound in \SFC:
\begin{theorem}[GADT Soundness]
 If $\GTranslate \empt \empt \Tm \tau \Tm'$, then $\manystepsto {\Tm'}
 \Val$ iff $\manystepsto {\Erased\Tm} \Val$ where $\Val$ is a
 value of a ground type, or $\Tm$ diverges.
\end{theorem}

\subsection{Generative Abstract Types}\label{sec:fc-encodes-newtypes}
Generative abstract types have a trivial translation into
\SFC. Each surface level type declaration gives rise to an coercion
axiom. For example, consider the !newtype HTML! example from
\pref{fig:html-generative-type}.

\begin{CenteredBox}
\begin{code}
newtype Html = MkHtml String
coHtml :: Html ~ String
\end{code}
\end{CenteredBox}

The axiom !coHtml! can be used freely to cast any term with a type
!HTML! to be used in the context that expects a !String!. The
attractiveness of this encoding is that it enables type safety at zero
runtime cost.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      mkHtml :: String -> Html
      mkHtml x = Html (escapeString x)

      unHtml :: Html -> String
      unHtml x = case x of HTML y -> y
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      mkHtml :: String -> Html
      mkHtml x = (escapeString x) /> (sym coHTML)

      unHtml :: Html -> String
      unHtml x = x /> coHtml
    \end{code}
  \end{minipage}
  \caption[\lstinline{HTML}]{\lstinline{newtype HTML} functions (left) and its elaboration in \SFC (right)}
  \label{fig:newtype-html-example}
\end{figure}

For example, in the programmer written function !mkHTML! as shown in \pref{fig:newtype-html-example}, the elaborated \SFC code will have a type cast instead of explicit boxing with a data constructor. Similarly, in the elaborated !unHTML! function, code would contain a type cast instead of explicit unboxing using a !case! statement as shown above. We now formalize the above elaboration procedure using a type directed translation. We require a new category of datatypes which allows only one data constructor.

\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Newtype Type Constructors} & T_N\\
    \text{Newtype Data Constructors} & D_N
  \end{syntax}
  \begin{syntax}
    \text{Types}               && \tau,\sigma \bnfeq& \alpha \bnfor \cdots \bnfor T_N\\
    \text{Newtype Declaration} && D_{cl} \bnfeq& \texttt{\textbf{newtype}}~ T_{N}~\many\alpha = D_N\App\tau\\
  \end{syntax}
  \caption{\lstinline{newtype} support in \SFC}
  \label{fig:newtypes-sfc}
\end{figure}

The type directed translation is shown in
\pref{fig:nt-elaboration}. The rule \trule{nt-ax} generates an axiom
from the newtype declaration. There are two elaboration rules for the
use of newtype data constructors, the rule \trule{nt-pack} is used
when the data constructor is used to create a term of the type
newtype. The rule \trule{nt-unpack} is used eliminate the match
statement and replace it with the term casted to the representation
type using the appropriate instantiations of the coercion axiom, as in
the translation of !unHTML!. Both the rules, \trule{nt-pack} and
\trule{nt-unpack} can be seen in action in the definition of the term
!mkHtml! and !unHtml! respectively in the \pref{fig:newtype-html-example} above.

\newcommand\NTAx{
 \ib{\irule[\trule{nt-ax}];
 {\NTranslate \TEnv {\texttt{newtype}~ T_N~ \many{\alpha} = D_N~ \Set
     {x : \tau}}
   {\substack {\mathlarger{\TEnv, D_N : \tau \to T_N\App\many\alpha, x : T_N\many\alpha \to \tau,}\\
     \mathlarger{\texttt{axiom}~coT_N : \Forall {\many{\alpha}}{T_N~\many\alpha \sim \tau}}
   }}}
 }
}

\newcommand\NTElab{
 \ib{\irule[\trule{nt-pack}]
 {\NTTranslate \TEnv M \tau {M'}};
 {\NTTranslate \TEnv {D_N\App M} {T_N\many\tau} {\Cast {M'} {\texttt{sym}~(coT_N \At \many\tau)}}}
 }
}

\newcommand\NTPatElab{
 \ib{\irule[\trule{nt-unpack}]
   {\NTTranslate \TEnv \Tm \tau {\Tm'}}
 {\NTTranslate {\TEnv, x\co\tau} N \sigma {N'}};
 {\NTTranslate \TEnv {\Case {D_N\App \Tm} {\Set {D_N\App x \to N}}}
   {\sigma} {{\Set{x \mapsto ({\Cast {\Tm'} {coT_N \At \many\tau}})} N'}}}
 }
}

\begin{figure}[ht]
\centering
\begin{gather*}
  \NTAx \\
  \NTElab \\
  \NTPatElab
\end{gather*}
\caption{\texttt{Newtype} type directed translation}
\label{fig:nt-elaboration}
\end{figure}

It is now easy to see that while on the surface level the functions !mkHtml!
and !unHTML! seem to pack and unpack the !String! value stored in the
!Html! type, the compiler generates efficient code. All the packing and
unpacking of the values are removed by using explicit type casts. This
optimization step cannot be performed on algebraic data types declared
using !data! keyword. The runtime cost characteristics of types declared using
!newtype!s and !data! are different. Due to the newtype definition
we know that !Html! \emph{is a} !String!. We should not pay
any runtime cost for coercing !Html! to !String! and vice versa as
!String! is the concrete runtime representation of !Html!.
The coercion axiom introduced during the elaboration step of the
newtype declaration makes this zero runtime cost cast possible.
Similar to GADT syntactic soundness, we also have the syntactic
soundness for newtypes:

\begin{theorem}[New types Syntactic Soundness]\label{lem:nt-syntax-soundness}
 If $\Good\TEnv$ and $\NTTranslate {\TEnv} \Tm \tau {\Tm'}$ then, $\manystepsto {\Tm'}
 \Val$ iff $\manystepsto {\Erased\Tm} \Val$ where $\Val$ is a value of
 a ground type, or $\Tm$ diverges.
\end{theorem}

\subsection{Type Computation}
\subsubsection{Associated Types}\label{sec:fc-encodes-assoctypes}
Elaborating associated types to \SFC is very similar to translating
GADTs. The constraint context now contains class predicates along with
equality predicates. This extension is also needed if GADT data
constructors need to constrain certain type parameters. The equality
constraints can now appear not only in the context of GADT data
constructors but in any term type.

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Class Declarations} &&C_{ls} \bnfeq& \textbf{\texttt{class }} D\App\many\TyVar \textbf{\texttt{ where }} \many{dsigs}; \many{sigs}\\
 \text{Instance Declarations} &&I_{nts} \bnfeq& \textbf{\texttt{instance }} D\App\many\tau \textbf{\texttt{ where }} \many{adata}; \many{val}\\
 \text{Associated types} &&dsigs \bnfeq& \textbf{\texttt{type }} \tau\\
 \text{Method signatures} &&vsigs \bnfeq& x\co\tau\\
 \text{Associated type instance} &&asigs \bnfeq& \tau = \sigma\\
 \text{Method bindings} &&asigs \bnfeq& x = \Tm
 \end{syntax}
 \caption[Class Syntax]{Class and Associated Types Surface Syntax}
 \label{fig:assoc-types-syntax}
\end{figure}

The judgment $\DTranslate C {D\App\tau} d$ elaborates the predicate
$D\App\tau$ to a dictionary $d$ in \SFC. The rule \trule{subst} allows
replacing type class parameters with equal types. This is necessary to
account for associated types that appear in the type class
signature. $D\App\tau$ may contain an associated type, and depending
on the instantiations of the free type variables in $\tau$ an
appropriate coercion can be used to justify casting the dictionary $d$
to the corresponding actual type.


$$
\ib{\irule[\trule{subst}]
 {\DTranslate C {D\App\tau} d}
 {\CoKinding C \Co {D\App\tau \sim D\App\tau'}};
 {\DTranslate C {D\App\tau'} {\Cast d \Co}}
}
$$

As an example, reconsider the !Con c! typeclass and a function
!sumCon :: (Con c, Num (Elem c)) => c -> Elem c!, which sums up all
the elements of the collection !c!. The predicate !Num (Elem c)!
asserts that the elements of the collection can be summed up. A
reasonable use of !sumCon! would be at type ![Int]!, as list of
integers can folded into an integer by using a repeated addition
operation. This would result in instantiating the type parameter !c!
with ![Int]!. The rule \trule{subst} in this case justifies the use of
!Num Int! dictionary as if it was a !Num (Elem [Int])! dictionary.

Each class declaration introduces a new class predicate name in the
type environment along with its method names. With associated types, a
new ``type function'' name---!Elem c! in the case of !Con c!---is
added to the type environment. Each instance introduces a new axiom
into the typing environment. For the !Con c! typeclass the instance
!Con [Int]! introduces the coercion !CoCon : Elem [Int] ~ Int!.
In general, each instance generates an axiom of the form
\begin{CenteredBox}
\begin{code}
co : FORALL $\many{\TyVar\co\star}$. F $\sigma$ $\sim$ $\sigma'$
\end{code}
\end{CenteredBox}

where $\fvs\sigma = \many\TyVar$ and $\fvs{\sigma'} \subseteq \many\TyVar$.
These axioms can also be viewed as type rewrite inducing axioms as it
rewrites the type $F\App\many\sigma$ to the right hand side of the
equation, $\sigma$. However, these type rewrite axioms are indeed type
equivalences, as the typechecker can replace an occurance of $\sigma'$
to $F\App\many\sigma$

\begin{theorem}[Associated Type Consistency]
If $\TEnv$ contains type rewrite axioms that are confluent and
terminating, then $\TEnv$ is consistent.
\end{theorem}
For non-overlapping typeclass instances, the rewrite axioms
are indeed confluent and terminating. We omit the syntactic soundness
theorem as it is exactly the same as the previous formalization.


\subsubsection{Open Type Functions}\label{sec:fc-encodes-opentypefun}
An unusual feature of the system, which also makes the type system
expressive is the ability to write open type functions.
An interesting observation is that the associated types can be
disassociated from their typeclasses and have an independent
existence. Such types are called type functions. They enable
expressing complex type level computations. Further, they are also
open, meaning, just like class instances, they can be extended. An
example of how type computations are expressed by type functions that
define addition of naturals at type level, shown in \pref{fig:open-type-fun-add}.
\begin{figure}[ht]
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
data Z
data S n
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
type family Plus m n
type instance Plus Z n = n
type instance Plus (S m) n = S (Plus m n)
\end{code}
\end{minipage}
\caption{Type level Arithmetic}
\label{fig:open-type-fun-add}
\end{figure}

Each type family instance directly translates to coercion axioms.
Thus for the type function !Plus m n!, the two associated instances
would introduce the following axioms:

\begin{CenteredBox}
\begin{code}
coPlusZn :: FORALL n. Plus Z n ~ n
coPlusSmn :: FORALL m n. Plus (S m) n ~ S (Plus m n)
\end{code}
\end{CenteredBox}

There are certain caveats on how these coercion axioms introduced by
type functions can be used by the typechecker to avoid inconsistency.
For example, consider an open type function !F! and two
of its instances that give rise to axioms !coFIB! and !coFBB! respectively.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
            type family F a
            type instance F Int = Bool
            type instance F Bool = Bool
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
      axiom coFIB: F Int ~ Bool
      axiom coFBB: F Bool ~ Bool
\end{code}
\end{minipage}

We can derive a coercion !Int ~ Bool! if we are careless.
Both axioms have !Bool! as their result type, and hence
we can derive !F Int ~ F Bool! by transitivity, and then
we can use the !right! construct to derive !Int ~ Bool!.
The following term

\begin{CenteredBox}
\begin{code}
 right (coFIB  . sym coFBB)
\end{code}
\end{CenteredBox}

has the type !Int ~ Bool!, a blunder$\bang$ It is crucial to use the !left! and !right!
rules only on types that are not applications of type functions as
they can be non-injective. This also influences the surface language
design: the type functions are special and need to be fully saturated,
and they cannot be curried. For this reason, the GHC convention is
to call them !type family! to disambiguate them from
generic type functions.

\subsection{Observations}
In the current formalization, all the types and coercions are explicit
in the system. While this makes a machine implementation of the system easy
to design and work with, it is difficult for humans to read
and understand the \SFC terms. As an optimization step,
it may be worth while to see if it is possible to elide some of the
explicit coercions and then reconstruct them using a type synthesis
algorithm. We first define a system with implicit coercions: \SFCi.
The key difference between \SFC and \SFCi is that where \SFC has a
coercion type $\Co$ of kind $\tau\sim\tau'$, \SFCi only gives the
equality kind in curly braces $\tau\sim\tau'$.
Hence, for terms
\begin{itemize}
\item Type casts, $\Cast \Tm \Co$, turns into $\Cast \Tm \Set{\tau \sim \tau'}$ and,
\item Coercion applications, $\Tm\App\Co$ turns into $\Tm\App\Set{\tau \sim \tau'}$
\end{itemize}
\begin{theorem}[Undecidability of coercion reconstruction of \SFCi]
 If $\Tm_i$ is an expression in \SFCi and $\TEnv$ is a typing
 environment then, reconstructing a \SFC term $\Tm$
 such that $\Typing \TEnv {\stepsto {\Tm_i} \Tm} \sigma$, where
 $\Typing \TEnv \Tm \sigma$ holds is undecidable.
\end{theorem}
The proof of undecidability amounts to reducing the problem of
coercion reconstruction to A-ground theories in a
semi-Thue system, which known to be
undecidable~\cite{post_recursive_1947}.
If there exists an alternative formulation of \SFCi with fewer
explicit type equalities and is sufficient to encode all the language
features while enjoying decidable type checking is an open question.
This result is not surprising as type synthesis problem for an
arbitrary \SF term is known to be an undecidable
problem~\cite{wells_typability_1999}. It is important to remember that
the intention of \SFC is to be used as internal language for the
compiler. The users of the language would never be required to read
the verbose code in the same way we do not expect programmers to
understand assembly code.

The second observation is that the coercions are types, kinds of which
give type equalities. This characterization is novel to \SFC.
The status quo is to express coercions as
terms~\cite{sheard_meta-programming_2008,weirich_type-safe_2000,baars_typing_2002,
neis_non-parametric_2011}.
The goal of the core language is to be practical;
due to all coercions being encoded at type level, they can be erased
at runtime to generate efficient code. Another reason is
that for implementation purposes, the error term, $\bot$ which
trivially just halts the program, by throwing an exception, can be
typed at all types. If equality had been encoded at the level of types
by making coercions a term level entity, we would have an ``error
coercion'' term, $\bot :: \tau \sim \sigma$. This would require
guaranteeing that evaluating the type equality evidences does not lead
to non-termination before we evaluate the term.
Without this evaluation check we cannot use a coercion for casts while
also guaranteeing type soundness. To avoid such complications and maintain
a phase distinction \SFC follows the slogan:
\emph{Kinds are propositions for type equality, proofs are (coercion) types}


\part{III: Extensions of \SFC}\label{part:III}
\section{\SFR}\label{sec:sfr} % R for roles

\begin{figure}[ht]
\centering
\begin{minipage}{0.5\linewidth}
\begin{code}
        newtype Html           = String
        type family F a
        type instance F Html   = Bool
        type instance F String = Char
\end{code}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{flalign*}
coHtml &: \texttt{Html} \sim \texttt{String}\\
coFHS  &: \texttt{F}~\texttt{Html} \sim \texttt{Bool}\\
coFSC  &: \texttt{F}~\texttt{String} \sim \texttt{Char}\\
\end{flalign*}
\end{minipage}
\caption{Type Functions and New Types}
\label{fig:nt-tf-example}
\end{figure}
Consider the previously defined newtype data definition:
!newtype HTML!, and a hypothetical open type
function, !F! as shown in~\pref{fig:nt-tf-example}.
The type family instance declarations add
two new type equalities: !F Html ~ Bool! and !F String ~ Char!,
along with the existing type equality !Html ~ String!
Now, using these three equalities, and the formulation of \SFC as
above, we can derive a type unsound coercion !Char ~ Bool!.

While puzzling at first, a careful re-examination reveals the
how we can derive the unsound coercion !Char ~ Bool!.
By virtue of the type function axiom we have !Char ~ F String!,
then we use coercion lifting to derive !F String ~ F Html!
(as !String ~ Html!), and finally, obtain !Char ~ Bool!
(by using !F Html ~ Bool!, and !F String ~ Char!):
\[
\Trans {\Trans {(\Sym coFHS)} {(F\App coHtml)}} {coFSC} : Char \sim Bool
\]
Although we had a characterization of $\Good\TEnv$ in
\pref{thm:progress-sfc} to make these ``bad'' coercions
impossible to derive, we have in fact managed to derive them.
Type functions interacting with newtypes is just one of the cases where
seemingly innocuous features interacting cause type unsoundness.
\citet{weirich_generative_2011} gives similar type unsoundness behavior
by using GADTs and newtypes. The crux of the problem is
\emph{unconstrained coercion lifting}: the use non-parametric
features (type functions, GADTs) of the language in the context that assumes
parametricity (newtypes).

In the previous \pref{sec:sfc-encoding-features}, we studied \SFC
encodings of each language feature and their respective soundness
criteria was straightforward when viewed independently.
However, the above ``bug'' is an evidence that we need to be more
careful to prove type soundness while working with multiple features.
To this effect, we need to reformulate our system to allow safe coercion lifting.


 \begin{figure}[ht]
 \begin{syntax}
 \text{Type Vars} &\TyVar,\beta,\Co &\qquad\text{Type constants} &T \\
 \text{Term Vars} &x,y &\qquad\text{Newtypes} &\shl{\NType} \\
 \text{Coercion Vars} &c &\qquad\text{Type Functions} &F\\
                      &  &\qquad\text{Indices} &i,n \in \mathbb{N}\\
 \end{syntax}
 \begin{syntax}
   \text{Roles} &&\rho \bnfeq& \shl{\texttt{N} \bnfor \texttt{R} \bnfor \texttt{P}}\\
   \text{Kinds} &&\kappa \bnfeq& \star \bnfor \kappa \to \kappa \bnfor \shl{\sigma \sim^\kappa_\rho \tau}\\
   \text{Types} &&\tau,\sigma \bnfeq& \TyVar \bnfor \mathcal{T} \bnfor
   \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor
   F\many\tau \bnfor \Co \\
    \text{Type Constants} &&\TypeConst \bnfeq& T \bnfor (\to) \bnfor \shl{\NType}\\
 \text{Coercions} &&\nu,\Co \bnfeq& c \bnfor \Refl\tau \bnfor \Sym\Co \bnfor \Trans\nu\Co % equiv relation
 \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction, instantiations
 \bnfor \nu\App\Co \\
                  &&              &\bnfor \Left \Co \bnfor \Right \Co \bnfor \Nth i \Co \bnfor \TypeConst\App\many\Co \bnfor F\many\Co \bnfor \shl{\SubCo \Co} \\  % compose/decompose
 \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
 \text{Patterns} &&P \bnfeq& H\App \many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co
 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Role Context} &&\mathcal{R} \bnfeq& \empt \bnfor \mathcal{R},\alpha\co\rho\\
                     &&\roles{\TypeConst} \bnfeq& {\Set{\rho \mid \alpha\co\rho \in Params(\TypeConst)}}\\
                     &&Params(\TypeConst) \bnfeq& \many{\alpha\co\rho}\quad  \text{s.t. } (\TypeConst\App\many\alpha) \co \star% \\
                     %&&\roles{F} \bnfeq& \many{\alpha\co N}\qquad\text{ where }F\many\alpha\co\star
 \end{syntax}
 \caption{Excerpt of Syntax of \SFR; extension of \SFC}
 \label{fig:sfr-syntax}
 \end{figure}

The key extension to \SFC which disallows unconstrained coercion lifting
is by making the equality predicate finer grained.
This enriches the type equality by encoding
\emph{how} the two types are equal. In the above case, we see that
we have two flavors of type equality coercions:
(i) nominal equality, which is established
via type function instances (!F String ~ Char!), and
(ii) representational equality which is established
via newtype definitions (!Html ~ String!).
With these notions of type equality made explicit we
can restrict coercion lifting.

\subsection{Syntax}\label{sec:sfr-syntax}
We formalize the syntax of \SFR in \pref{fig:sfr-syntax}.
The highlighted portions are the additional pieces
\pref{fig:sfc-syntax}. Although the original work
by \citet{breitner_safe_2014} and \citet{weirich_generative_2011}
formalizes the system slightly differently, the presentation in
this report is easier to understand as an extension of \SFC.
It is also representative to what the GHC compiler implements.
To represent finer grained type equality, we use the concept
of \emph{roles}, $\rho$. All type parameters to a type constructor are
decorated with a role. We define the function
$\roles{\TypeConst}$ that returns the roles of
all the type parameters of a type constant $\TypeConst$.
While comparing to datatypes for equality, the role annotations
tell us how the two type parameters are equal.

\subsection{Static Semantics}\label{sec:sfr-static-sem}
The important judgment for coercions is
$\CoKinding \TEnv \Co {\tau\sim^\kappa_\rho\sigma}$.
It is read as ``in the type environment $\TEnv$
the coercion $\Co$, witness the equality between types $\tau$
and $\sigma$ that have the same kind $\kappa$ at role $\rho$''.
The ``equality at role $\rho$'' is the novel feature of \SFR.
We will omit the kind $\kappa$ when it is clear from the context.
\SFR has three different roles for three different flavors of equalities:
\begin{itemize}
\item\textbf{Nominal:} The strictest kind of equality denoted by
  $\teqN$ which holds exactly when the two types are the \emph{same}.
  For example, an type function axiom !F Int = Bool!
  makes !F int $\teqN$ Bool!. A type synonym is also defines nominally
  equal types, for example, !type UserName = String!.
\item\textbf{Representational:} This equality holds when the two types
  have the same runtime representation.
  For example, a new type declarations !newtype Age = Int! makes
  !Age!$\teqR$!Int!
\item\textbf{Phantom:} The weakest kind of equality holds for all
  types: !a $\teqP$ b! is valid at all types !a! and !b!
\end{itemize}

\newcommand\CastR{
  \ib{\irule[\trule{cast-r}]
    {\Typing \TEnv \Tm \tau}
    {\CoKinding \TEnv \Co {\tau \teqR {\tau'}}};
    {\Typing \TEnv {\Cast \Tm \Co} {\tau'}}
  }
}
\newcommand\CastN{
  \ib{\irule[\trule{cast-n}]
    {\Typing \TEnv \Tm \tau}
    {\CoKinding \TEnv \Co {\tau \teqN \tau'}};
    {\Typing \TEnv {\Cast \Tm \Co} \tau'}
  }
}

\newcommand\KSubCo{
 \ib{\irule[\trule{co-sub}]
 {\CoKinding \TEnv {\Co} {\tau \teq\rho \tau'}}
 {\rho < \rho'};
 {\CoKinding \TEnv {\SubCo \Co} {\tau \teq{\rho'} \tau'}}
 }
}

\newcommand\KNthCo{
 \ib{\irule[\trule{co-nth}]
   {\CoKinding \TEnv {\Co} {T \App \many\sigma \teqR T\App\many{\tau'}}}
   {\many\rho = \text{roles}(T)};
   {\CoKinding \TEnv {\Nth i \Co} {\tau_1 \teq{\rho_i} \tau_2}}
 }
}

\newcommand\KLeftCoR{
 \ib{\irule[\trule{co-left}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \teqN \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Left \Co} {\tau_1 \teqN \tau_2}}
 }
}

\newcommand\KRightCoR{
 \ib{\irule[\trule{co-right}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \teqN \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Right \Co} {\sigma_1 \teqN \sigma_2}}
 }
}

\newcommand\KTyAppCo{
  \ib{\irule[\trule{co-ty-app}]
    {\substack {\mathlarger {\Kinding \TEnv {\tau_1\App\sigma_1} {\kappa}}\\
               {\mathlarger {\Kinding \TEnv {\tau_2\App\sigma_2} {\kappa}}}}}
    {\substack {{\mathlarger{\CoKinding \TEnv {\Co_1} {\tau_1 \teq\rho \tau_2}}}\\
               {\mathlarger {\CoKinding \TEnv {\Co_2} {\sigma_1 \teqN \sigma_2}}}}};
    {\CoKinding \TEnv {\Co_1\App\Co_2} {\tau_1 \sigma_1 \teq\rho \tau_1 \sigma_1}}
  }
}

\newcommand\KFAppCo{
  \ib{\irule[\trule{co-F-app}]
    {F \in \TEnv}
    {\Kinding \TEnv {F\App\many{\tau}} {\kappa}}
    {\Kinding \TEnv {F\App\many{\sigma}} {\kappa}}
    {\many{\CoKinding \TEnv {\Co} {\tau \teqN \sigma}}};
    {\CoKinding \TEnv {F \many\Co} {F \many{\tau} \teqN F \many{\sigma}}}
  }
}

\newcommand\KTyConAppCo{
  \ib{\irule[\trule{co-tycon-app}]
    {\many\rho = \roles{\TypeConst}}
    {\substack {\mathlarger {\Kinding \TEnv {\TypeConst\App\many{\tau}} {\kappa}}\\
               {\mathlarger {\Kinding \TEnv {\TypeConst\App\many{\sigma}} {\kappa}}}}}
    {\many{\CoKinding \TEnv {\Co} {\tau \teq\rho \sigma}}};
    {\CoKinding \TEnv {\TypeConst \many\Co} {\TypeConst \many{\tau} \teq\rho \TypeConst\many{\sigma}}}
  }
}


\begin{figure}[ht]
 \centering
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \tau \kappa$}\\
 \KSubCo\\
 \KNthCo \rsp \KLeftCoR \rsp \KRightCoR \\
 \KTyAppCo \rsp \KFAppCo  \\
 \KTyConAppCo
\end{gather*}
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}\\
    \CastR \rsp \CastN
  \end{gather*}

 \caption{Excerpt of Static Semantics of \SFR: Coercion Calculus}
 \label{fig:sfr-typing}
\end{figure}
Type safe casts can now be expressed using the kinding rules
\trule{cast-r} and \trule{cast-n} as shown in \pref{fig:sfr-typing}.
The rule \trule{cast-r} captures the notion of equality which arises
due to newtype definitions as the rule enables coercions between
expressions that have the same representation type. The rule
\trule{cast-n} captures the notion of type equalities introduced by
instantiating axioms arising due to type functions.
The phantom equality on types seems unnecessary, and outright wrong:
why would any two types be equal if they are obviously not equal?
Phantom equality is useful in keeping the type system practical.
In case of multiparameter datatypes, some type parameters are used
only as type tags, their instantiation does not affect the
representation of the datatype in anyway. Phantom equality is useful
to assert equality on such data types.

Backwards compatibility is also one of the goals of this extension.
We must be able to type check all the previous programs that passed type
checking and rightly did so. We leverage the fact that
the equality roles form a total relation: $N < R < P$. If two types
are nominally equal, they are representationally equal as
well. Similarly, if the two types are representationally equal, they
are also equal at phantom role. This is captured in the \trule{co-sub}
shown in \pref{fig:sfr-typing}. It says that if we \emph{want} to have
equality at representational role, and we are \emph{given} that the
two types are nominally equal ($\Co$), then we can generate a coercion that
proves the wanted equality from the given equality ($\SubCo \Co$).
Nominal equality is the strictest from of equality while phantom
equality is the weakest.


\newcommand\RVar{
  \ib{\irule[\trule{r-var}]
    {\tau\co\rho' \in \REnv}
    {\rho' \leq \rho};
    {\Typing \REnv \tau \rho}
  }
}

\newcommand\RTyApp{
  \ib{\irule[\trule{r-ty-app}]
    {\Typing \REnv \tau \rho}
    {\Typing \REnv \sigma N};
    {\Typing \REnv {\tau\App\sigma} \rho}
  }
}

\newcommand\RTyConApp{
  \ib{\irule[\trule{r-tycon-app}]
    {\many{\Typing \REnv {\tau} {\rho}}}
    {\many{\rho} \text{ is a prefix of } \roles{\TypeConst}};
    {\Typing \REnv {\TypeConst\App\many\tau} R}
  }
}

\newcommand\RFApp{
  \ib{\irule[\trule{r-f-app}]
    {\many{\Typing \REnv \tau N}};
    {\Typing \REnv {F\App\many\tau} N}
  }
}


\newcommand\RTAssign{
  \ib{\irule[\trule{r-T}]
    {H\co
      \Forall{\many{\alpha\co\kappa}}{\Forall{\many{\beta\co\kappa'}}{\many\sigma
          \to T\many\alpha}} \in
      C_{trs}(T)}
    {\tau \in \many\sigma}
    {\Typing {\many{\alpha\co\rho},\many{\beta\co N}} \tau R};
    {\RoleAssign {\many\rho} T}
  }
}

\newcommand\RNTAssign{
  \ib{\irule[\trule{r-nty}]
    {co\NType : \Forall \alpha {\NType\many\alpha \teqR \sigma}}
    {\Typing {\many{\alpha\co\rho}} \sigma R};
    {\RoleAssign {\many\rho} \NType}
  }
}

\newcommand\RToAssign{
  \ib{\irule[\trule{r-$\to$}]
    {};
    {\RoleAssign {\alpha\co R, \beta\co R} {\alpha \to \beta}}
  }
}

\newcommand\REqAssign{
  \ib{\irule[\trule{r-$\sim$}]
    {};
    {\RoleAssign {\alpha\co\rho, \beta\co\rho} ({\alpha} \teq\rho {\beta})}
  }
}

\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\many\rho \forces \TypeConst$}\\
    \RNTAssign \rsp \RTAssign\\
    \RToAssign \rsp \REqAssign
  \end{gather*}
  \begin{gather*}
    \fbox{$\Typing \REnv \tau \rho$}\\
    \RVar \rsp \RTyApp\\
    \RTyConApp \rsp \RFApp
  \end{gather*}
  \begin{gather*}
    \fbox{$\rho \leq \rho'$}\\
    {\ib{\irule[]{};{N \leq \rho}}} \rsp {\ib{\irule[]{};{\rho \leq \rho}}} \rsp {\ib{\irule[]{};{\rho \leq P}}}
  \end{gather*}
  \caption{\SFC Assignment and Validity Rules for Roles}
  \label{fig:sfr-validity}
\end{figure}


Role assignments to type parameters are performed during type checking
the user defined type constant. They are formalized using the role
validity as shown in \pref{fig:sfr-validity}. The judgment
$\RoleAssign {\many\rho} \TypeConst$ is to be read as ``$\many\rho$
are appropriate for the type constant \TypeConst'', while the judgment
$\Typing \REnv \tau \rho$ is read as ``under the role context
assumptions $\REnv$, the type $\tau$ has the role $\rho$''. We do not
want a global type context to claim that a type parameter's role which
is in conflict with the role given to it by the definition the type
constant. The role assignment algorithm plays an important role (no
pun intended) in inferring the best role for the type parameters for a
type constant by walking over its definition. We want to ensure that
the role assignments are no too restrictive nor to permissive. In the
former case, we can assign each type parameter a nominal role but we
would fail to type check a huge majority of the programs that we want
to be able to type check. In the later case we can assign each type
parameter a phantom role but that would be type unsound.

\begin{minipage}{0.5\linewidth}
\begin{code}
                 data Any a = Any
                 newtype App f a = MkApp (f a)
\end{code}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{code}
                 $\roles{\texttt{Any}} = \Set{P}$
                 App f a ~ (f a)
                 $\roles{\texttt{App}} = \Set{R, N}$
\end{code}
\end{minipage}

Consider the following definitions shown above for !Any a! and !App f a!.
With the rules given in \pref{fig:sfr-validity}, for the
generative type !App!, the roles inferred will be
$\roles{\texttt{App}} = \Set{R, N}$. Now, due to the rules of the coercion
calculus we can derive the following representational type equality:
\begin{align}\label{eqn:app-any-newtype}
  \texttt{App } \App \texttt{Any}\App \texttt{Int} \teqR \texttt{App}\App\texttt{Any} \App{Bool}\tag{app-co}
\end{align}
This is possible because of the following chain of reasoning:
\begin{align}
  \texttt{App} \App \texttt{Any}\App \texttt{Int}%
  \teqR \texttt{Any}\App \texttt{Int}%
  \teqR \texttt{Any}\App \texttt{Bool}%
  \teqR \texttt{App}\App\texttt{Any} \App{Bool}\nonumber
\end{align}
The first step and the last step of the reasoning chain is valid due to
the instantiations of the coercion axiom,
$\texttt{coApp} : \Forall{f,a}. \texttt{App}\App f\App a \teqR f\App a$.
We do not want to allow the rule
\trule{co-nth} to \pref{eqn:app-any-newtype}. Because if we did, then
we would be able to produce a coercion $\texttt{Int} \teqR
\texttt{Bool}$, a blunder. We thus allow the \trule{co-nth}
only on vanilla algebraic type constants ($T$); not on newtypes
($T_N$) and neither on type function constants $F$.

\subsection{Meta-theory}\label{sec:sfr-metatheory}
The proof of progress does not change with respect to \SFC as the only
additional construct is roles, which does not play any part in the
operational semantics of the language. To recall, the proof of
progress is necessary to show that the well typed terms are either
values or they can take a step.
\begin{theorem}[Progress for \SFR]\label{lem:sfr-progress}
 If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ then, either $\Tm \in C\Val$ or, $\stepsto \Tm N$
\end{theorem}
The proof of preservation uses the formulation of coercion calculus
extended with roles and thus requires some attention. The reason for
type unsoundness was that we allowed too many coercions with
unrestricted coercion lifting in \SFC.
\begin{lemma}[Soundness of Role Narrowing]\label{lem:role-narrowing}
If $\RoleAssign{\many\rho}{\TypeConst}$, and $\many\rho' = \Set{\rho'
  \mid \rho \in \many\rho, \rho' \leq \rho}$ then $\RoleAssign{\many\rho'}{\TypeConst}$
\end{lemma}
\pref{lem:role-narrowing} claims that it is sound to make the role
assignments stricter for any type constant. Narrowed role assignments
will reject more programs so the programmer will find it difficult to
work with even if it guarantees type safety. We now need a role
assignment algorithm which ensures that the typechecker can assign the
roles automatically. This characterization of role inference runs
parallel to type inference: we want to develop an algorithm, possibly
efficient, such that when the typechecker assigns the roles, it is
most general, in the sense we do not want to make the system unusable,
but also we want to have roles to be correctly assigned.
\begin{figure}[ht]
  \begin{algorithmic}[1]
    \Procedure{\RoleInfer}{[$\TypeConst$]}\Comment{Infer the roles of type parameters of all $\TypeConst$s}

    \State For all $\TypeConst$, populate $roles(\TypeConst)$ using programmer supplied annotations. \\
    \qquad Or default ADT and newtype type parameters to $P$.
    \State For every $H\co\tau \in C_{trs}(\TypeConst)$ and every $\sigma$ that appears in $\tau$: call $\walk\TypeConst\sigma$\label{here}
    \State For every newtype $T_N$, and its representation type $\sigma$: call $\walk\TypeConst\sigma$
    \State If any role parameter changed in previous steps then go to Step 2.
    \State For all $\TypeConst$, Check $\RoleAssign {\roles\TypeConst} \TypeConst$. Error on inconsistency.
    \EndProcedure
  \end{algorithmic}
  \begin{algorithmic}[1]
    \Procedure{\texttt{walk}}{$\TypeConst, \sigma$}\Comment{Role marking for $\TypeConst$}
    \State $\walk \TypeConst {\alpha} \bnfeq$ Mark $\alpha$ to $R$, if not already marked.
    \State $\walk \TypeConst {H\App \many\tau} \bnfeq$ let $\many\rho = \roles{H}$
    \If {Any $\rho_i = N$ such that $\rho_i \in \many\rho$}
        \State Mark all unmarked parameters in $\tau_i \in \many\tau$ as $N$
    \ElsIf{Any $\rho_i = R$}
        \State call $\walk{\TypeConst}{\tau_i}$
    \EndIf
    \State $\walk \TypeConst {\tau\App\tau'} \bnfeq$ $\walk{\TypeConst}{\tau}$ and then mark all unmarked parameters in $\tau'$ as $N$
    \State $\walk \TypeConst {F\App\many\tau} \bnfeq$ Mark all type parameters in $\many\tau$ to $N$
    \State $\walk \TypeConst {\Forall{\beta\co\kappa}\tau} \bnfeq \walk \TypeConst \tau$
    \EndProcedure
  \end{algorithmic}
\caption{\RoleInfer algorithm and $\walk\TypeConst\sigma$}\label{alg:role-infer}
\end{figure}

The role inference algorithm, \RoleInfer, as shown in the \pref{alg:role-infer}
has three choices to make for the type parameters of each type constant.
The role defaulting policy for type parameters is to assign phantom
roles to algebraic datatypes ($H$), nominal role for function type
($\to$) and type function constants. This is a way for the programmer
to override the defaulting role assignment, using a new optional
syntax. This syntax is not backwards compatible but it is lightweight
and can be easily incorporated by using compiler pre-processor pragmas
which GHC already supports CPP style macros\footnote{\url{https://downloads.haskell.org/ghc/latest/docs/users\_guide/phases.html\#standard-cpp-macros}}
which can decide by checking the compiler version if these type role
annotations are to be kept in the source code to be passed to the
typechecker or not.

\begin{CenteredBox}
\begin{code}
type role App representational nominal
newtype App f a = MkApp (f a)
\end{code}
\end{CenteredBox}
In the above code, for the !App! user defined type, the type parameter
!f! is assigned representational while !a! is assigned nominal, or
that $\roles{App} = \Set{R, N}$

The $\walk{\TypeConst}{\sigma}$ is the workhorse for \RoleInfer.
It assigns the type parameters of the $\TypeConst$ with appropriate
roles. The algorithm \RoleInfer runs $\walk{\TypeConst}{\sigma}$ for
every new $\TypeConst$
defined by the programmer. In the first case of
$\walk\TypeConst\sigma$, if $\sigma$ is a type variable, $\alpha$,
which is not a type parameter, we can just ignore it. The other cases
are easy to follow from the algorithm snippet.

We have the following properties for \RoleInfer algorithm:
\begin{enumerate}
\item terminating;
\item sound role choice for all type parameters and;
\item optimal role choice for all type parameters and
\end{enumerate}

The algorithm is terminating as at every call to
$\walk{\TypeConst}\sigma$, we either mark the type parameter role
stricter that the previous one, or we never touch it again. Due to the
lower bound on the role ordering, and there being a finite number of
type parameters, the algorithm is guaranteed to terminate. The role
assignment soundness property follows directly from
\pref{lem:role-narrowing}, as each walk essentially narrows the
role. The algorithm is greedy which proves that the role
assignment is optimal, as if there was a valid role assignment that
was wider than the one assigned by the algorithm, then the algorithm
would have chosen that before narrowing it.
\begin{lemma}[Type Preservation for \SFR]\label{lem:sfr-preservation}
If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ and $\stepsto \Tm \Tm'$, then $\Typing \TEnv {\Tm'} \tau$
\end{lemma}
The role directed coercion calculus can be proved to be confluent only
if the right hand sides of the coercion axioms have linear
patterns. Confluence of type rewriting guarantees consistency for the
complete system. In case of overlapping non-linear patterns, the proof
of confluence is an open problem~\cite{mizuhito_rta_1995}.
Finally, type preservation, \pref{lem:sfr-preservation}, along
with progress, \pref{lem:sfr-progress} is the formal guarantee that
ensures type soundness.

The goal of \SFR is to find a good power to pain ratio: making
minimal changes to the system to the benefit the programmer, and
re-establishing type soundness in presence of newtypes, type functions
and GADTs by disallowing unrestricted coercion lifting. The key
challenge was to identify and solve the problem of generating fewer
type coercions but enough to be backwards compatible.

\section{\SFP}\label{sec:sfp} % P for promotion
Consider a GADT implementation of a vector datatype, !Vec!.
This encoding stores the length of the vector at the level of types
along with the type of the element.

\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
        data Z
        data S n
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
    data Vec : * -> * -> * where
       Nil : Vec Z elem
       Cons : elem -> Vec n elem -> Vec (S n) elem
\end{code}
\end{minipage}

The types !Z! and !S n! encode the size of the vector. The data
constructor !Nil! is a zero length vector which is asserted by the
index type !Z!, while !Cons! appends an element of type !elem! to a
vector of size !n! to return a vector of size !S n!. With a primitive
kind system, there is no enforcement that the type argument to !Vec!
has to be either !Z! or !S n!. Any type of kind, $\STAR$ is a valid
argument. A desirable alternative would be to restrict the kind
of the index type that describes the size of the vector.

\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
         data Nat = Z | S Nat
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
     data Vec : Nat -> * -> * where
        Nil : Vec Z elem
        Cons : elem -> Vec n elem -> Vec (S n) elem
\end{code}
\end{minipage}

In this setting, the typechecker has enough information to reject
semantically absurd types like !Vec Char a!. As the kind of !Char! is
$\STAR$ and not !Nat!. Similar problems arise while writing addition function on type level
naturals. The situation is worse as writing type functions does not
include any kind information; the argument and return types are
defaulted to kind $\STAR$, making it seem ill-kinded. The intention of
the programmer is not that any type of kind $\STAR$ is a valid
argument to the function !Plus!. A better alternative would be to
allow specifying the concrete kind specification to the function---in
this case specifying that the arguments and return kind of !Plus! is a
!Nat!.


\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
     type family Plus m n
     type instance Plus Z m = m
     type instance Plus (S n) m = S (Plus n m)
\end{code}
\end{minipage}
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
     type family Plus (m:Nat) (n:Nat) :: Nat
     type instance Plus Z m = m
     type instance Plus (S n) m = S (Plus n m)
\end{code}
\end{minipage}

Allowing user defined kinds, albeit more expressive, are awkward to use without kind polymorphism. Consider a higher kinded datatype !TApp! which is useful for generic programming that encodes type application:

\begin{CenteredBox}
\begin{code}
data TApp f a = MkTApp (f a)
\end{code}
\end{CenteredBox}

A naive algorithm may compute the kind of the type argument !f! to be $\STAR \to \STAR$, making the kind of !TApp! to be $(\STAR \to \STAR) \to \STAR \to \STAR$. This however may be too restrictive as it allows encoding only those types that have a kind $\STAR$. A more appropriate kind for !TApp! would be $\Forall k {(k\to\STAR) \to k \to \STAR}$, allowing a wider range of types to benefit from generic programming techniques. For example, we can then use feature to write generic programs over higher kinded types like !List!s and !Trees!~\cite{magalhaes_generic_2010}.

We have identified two requirements: allowing user defined kinds, and
more expressive kind polymorphism. An unimaginative solution for the former would involve adding a new syntax to the language that accepts kind definitions similar to the type definition. For example !kind Nat = Z | S Z!. An improvement would be by reusing the datatype declaration syntax. For any algebraic datatype that the programmer defines, each of its data constructors will be promoted along with the type constant. To encode type level naturals, the programmer declares the datatype, as they normally would, and the compiler will automatically generates new type and kind level bindings. For example, in case of datatype !Nat!, the data constructor !S : Nat -> Nat! gets promoted to the type level while !Nat! get promoted to the kind level. Kind polymorphism is straight forward by allowing kind quantification only in prenex form. For example for a datatype !T : FORALL$\many{a}$.$\many{k}$ -> *!, all the kind variables $\many{a}$ are quantified before the type arguments of kind $\many{k}$. This simplifies the semantics of the type constants.

\subsection{Syntax}\label{sec:sfp-syntax}
\begin{figure}[ht]
 \centering
 \begin{syntax}
 \shl{\text{Kind Vars}} &\shl{\chi, \mathcal{Y}} &  & \\
 \text{Type Vars} &\TyVar,\beta,\Co &\qquad\text{Type constants} &T,\shl{H} \\
 \text{Term Vars} &x,y &\qquad\text{Indices} &i,n \in \mathbb{N} \\
 \text{Coercion Vars} &c & &
 \end{syntax}
 \begin{syntax}
 \text{Sort} &&\square & \\
 \text{Kinds} &&\kappa,\eta \bnfeq& \STAR \bnfor
                                  % \shl{\texttt{CONSTRAINT}} \bnforc
                                  \kappa \to \kappa \bnfor \sigma\sim\tau \bnfor \shl{\chi} \bnfor \shl{T\App\many\kappa}\\
 \text{Types} &&\tau,\sigma \bnfeq& \TyVar \bnfor T
                                  \bnfor \tau \to \tau \bnfor
                                  \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau  \bnfor F_n
                                  \bnfor \shl{\Forall \chi \tau} \bnfor \shl{\tau\App\kappa} \bnfor \shl{H}\\
 \text{Type Constants} && T \bnfeq& T \bnfor T_N\\
 \text{Coercions} &&\nu,\Co \bnfeq& c \bnfor \Refl\tau \bnfor \Sym\Co \bnfor \Trans\nu\Co % equiv relation
 \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
 \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co  % compose/decompose
 \bnfor \shl{\Forall \chi \Co} \bnfor \shl{\Co\App\chi} %
 \bnfor \shl{\Co\At\chi}\\
 \text{Patterns} &&P \bnfeq& H \App\shl{\many\chi}\App\many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co %
 \bnfor \shl{\TLam \chi \Tm} \bnfor \shl{\Tm\App\kappa} \\

 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Substitutions} &&\Subst \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
 \end{syntax}
 \caption{The Syntax of \SFP as an extension for \SFC}
 \label{fig:sfp-syntax}
\end{figure}

\SFP~\cite{yorgey_giving_2012}, achieves both the requirements mentioned above with a relatively small extension to \SFC. The syntax for the core language is shown in \pref{fig:sfp-syntax}. The key changes in the language of types---highlighted to compare with \SFC---are allowing kind variables ($\chi$), kind abstraction ($\Forall \chi \kappa$) and application ($\tau\App \kappa$). These changes are reflected in the coercion language which needs additional constructs: kind poly-type congruence ($\Forall \chi \Co$), kind application ($\Co\App\chi$) and kind instantiations ($\Co\At\kappa$).

Other noteworthy addition to the kind and type language are promoted constructors $T\App\many\kappa$ and $H\App\many\tau$, respectively. The data constructors are now explicit in their kind polymorphism by expecting all the kind variables $\many\chi$ to be instantiated before any type variables are instantiated, evident in their type signatures, which now expects the kinds before the type as formal parameters.
% \footnote{As a  convenience notation, $\Forall {\many\TyVar} \tau$ is just a shorthand for $\Forall {\TyVar_1} {\Forall {\TyVar_2} {... \Forall {\TyVar_i} \tau}}$, while $\many\tau \to \tau'$ is a short hand for $\tau_1 \to \tau_2 \to ... \to \tau_i \to \tau'$}

$$
H : \Forall {\shl{\many\chi}} {\Forall {\many{\TyVar\co\kappa}}
  {\Forall {\many{\beta\co\eta}} {\many\tau \to T
      \App\many{\chi}\App\many{\TyVar}}}}
$$

Promotion of type and data constructors may result in name-space issues. It, however, can be disambiguate from the context. Another mechanism is to add explicit quotation marks as prefix to aid disambiguation.
For example ``!T!'' is a type constant but ``!'T!'' is a kind constructor.
Finally, to stop the buck on the syntax classification hierarchy \emph{all} kinds are classified by a unique sort, $\square$. To keep the kind inference tractable, there are no kind level lambdas, only promotion of higher kinded types which would stand in for kind functions.

\subsection{Static Semantics}
It is not clear which constructors can be promoted without threatening
either the consistency, or usability of the language. Backwards
compatibility is an important aspect while adding a new feature, as
the code that compiled without promotion should also work after
promotion. We use the following criteria:

\begin{itemize}
\item For a type constant, $T\App\many\kappa$ is a valid kind only if $T$ is fully saturated and has a kind $\many\STAR \to \STAR$
\item For a data constructors, $H\App\many\tau$ is a valid type if all the type arguments $\many\tau$ to the constructor can be promoted and the type of the data constructor can be promoted.
\end{itemize}

The restriction on the promotion of type constant is because promoting higher kinded types, such as $(\STAR \to \STAR) \to \STAR$ would not be possible without having a richer kind classification system. Further, promotion of type constant that themselves accept promoted types such as !Vec : * -> 'Nat -> *! are not promoted as that would involve double promotion of the type !Nat! or would require type and kind levels to be fully dependent, making the system complex. Kind polymorphic type constant are also not promoted as that would require polymorphic sorts, which is absent from the system.

\newcommand\KCoTAppK{
 \ib{\irule[\trule{co-$\tau\kappa$-app}]
 {\Kinding \TEnv {\kappa} {\square}}
 {\CoKinding \TEnv {\Co} {\tau \sim \tau'}};
 {\CoKinding \TEnv {\Co\App\kappa} {\tau\App\kappa \sim \tau'\App\kappa}}
 }
}

\newcommand\KCoKAbs{
 \ib{\irule[\trule{co-$\kappa$-abs}]
 {\Kinding \TEnv {\kappa} {\square}}
 {\CoKinding \TEnv {\Co} {\tau \sim \tau'}};
 {\CoKinding \TEnv {\Forall \kappa \Co} {\Forall \kappa \tau \sim {\Forall \kappa \tau'}}}
 }
}

\newcommand\KCoVarAx{
 \ib{\irule[\trule{co-$\Co$-ax}]
   {\substack {\mathlarger{c\co \Forall{\many\chi}{\Forall{\many{\alpha\co\eta}}{\tau'\sim\tau''}} \in \TEnv}\\
              {\mathlarger\Subst_\kappa = \Set{\many{\chi \mapsto \kappa}},
                \Subst_\tau=\Set{\many{\alpha\mapsto\tau}},
                \Subst_\sigma=\Set{\many{\alpha\mapsto\sigma}}}}}
   {\substack{\mathlarger{\many{\CoKinding \TEnv \Co {\tau\sim\sigma}}}\\
             {\mathlarger{\many{\CoKinding \TEnv {\tau, \sigma} {\Subst_\kappa\eta}}}}}};
 {\CoKinding \TEnv {c\App\many\kappa\App\many\Co} {\Subst_\kappa\Subst_\tau\tau' \sim \Subst_\kappa\Subst_\sigma\tau''}}
 }
}

\newcommand\KCoKappaInst{
 \ib{\irule[\trule{co-$\kappa$-inst}]
 {\Subst = \Set{\chi \mapsto \kappa}}{\Kinding \TEnv \kappa \square}
 {\CoKinding \TEnv {\Co} {\Forall\chi\tau \sim \Forall\chi{\tau'}}};
 {\CoKinding \TEnv {\Co[\kappa]} {\Subst\tau \sim \Subst\tau'}}
 }
}


\newcommand\KLift{
  \ib{\irule[\trule{H-lift}]
     {\vdash \TEnv}
     {H\co\tau \in \TEnv}
     {\empty \vdash \tau \hookrightarrow \kappa};
     {\Kinding \TEnv H \kappa}
  }
}


\newcommand\KKTyAbs{
  \ib{\irule[\trule{ty-$\I{\forall\chi}$}]
     {\Kinding {\TEnv, \chi\co\square} {\tau} {\star}};
     {\Kinding \TEnv {\Forall{\chi}{\tau}} \star}
  }
}

\newcommand\KKTyApp{
  \ib{\irule[\trule{ty-$\E{\forall\chi}$}]
     {\Kinding \TEnv \tau {\Forall \chi \kappa}}
     {\Typing \TEnv \eta \square}
     {\Subst = \Set{\chi \mapsto \eta}};
     {\Kinding \TEnv {\tau\App\eta} {\Subst\kappa}}
  }
}

\newcommand\TyKAbs{
  \ib{\irule[\trule{$\I\kappa$}]
     {\Typing {\TEnv,\chi\co\square} \Tm \tau};
     {\Typing \TEnv {\TLam \chi \Tm} {\Forall \chi \tau}}
  }
}

\newcommand\TyKApp{
  \ib{\irule[\trule{$\E\kappa$}]
     {\Kinding \TEnv \Tm {\Forall \chi \tau}}
     {\Kinding \TEnv \kappa \square}
     {\Subst = \Set{\chi \mapsto \kappa}};
     {\Kinding \TEnv {\Tm\App\kappa} {\Subst\tau}}
  }
}

\newcommand\TyCase{
  \ib{\irule[\trule{case}]
    {\Typing \TEnv \Tm {T\App\many\kappa\App\many\sigma}}      {\many{H\co\Forall{\many\chi}{\Forall{\many{\alpha\co\kappa}}{\Forall{\many{\mathcal{Y}}}{\Forall{\many{\beta\co\eta}}{\many\tau \to T\App\many\chi\App\many\alpha}}}} \in \TEnv}}
     {\substack{\mathlarger{\Subst_\kappa = \Set{\many{\chi \mapsto \kappa}}}\\
               {\mathlarger{\Subst_\sigma  = \Set{\many{\alpha \mapsto \sigma}}}}}}
     {\Typing {\TEnv, \many{\chi\co\square},\many{\beta\co\Subst_\kappa\eta}, \many{x\co\Subst_\kappa\Subst_\sigma\tau}} u \tau};
     {\Kinding \TEnv {\Case \Tm {\many{H\App\many{\mathcal{Y}}\App\many{\beta\co\eta}\App\many{x\co\tau} \to u}}} {\tau}}
  }
}


\begin{figure}[ht]
  \centering
\begin{gather*}
  \fbox{$\Kinding \TEnv \tau \kappa$}\\
  \KLift \rsp \KKTyAbs \rsp \KKTyApp\\
\end{gather*}
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \tau \kappa$}\\
 \KCoVarAx \rsp \KCoTAppK \\
 \KCoKAbs \rsp \KCoKappaInst
\end{gather*}
\begin{gather*}
  \fbox{$\Typing \TEnv \Tm \tau$}\\
  \TyKAbs \rsp \TyKApp\\
  \TyCase
\end{gather*}
 \caption{Excerpt of Static Semantics of \SFP: Kinding and Typing Judgments}
 \label{fig:sfp-static-sem}
\end{figure}


The programmer is expected to provide minimal (if any) kind annotations to the programs. This necessitates changing the type inference algorithm. While instantiating a kind polymorphic term, fresh kind unification variables are generated, and during type unification, the kinds of the types are also unified. This is necessary due to the design decision: there are no kind equalities. The unification of kinds does not produce any evidences, unlike for type unification. This helps in solving kind unification on the fly in contrast to solving type unification where they are collected as equality constraints and then solved separately, generating coercion as evidences.


The datatype declaration is type checked in a manner very similar to mutually dependent terms:
\begin{itemize}
\item The type declarations are sorted into strongly connected components;
\item The kind inference assigns a new unification kind variable to each of the type constant and then walks over the definitions solving for kind constraints that may arise;
\item Finally, all the constructors are kind generalized at the end.
\end{itemize}

\subsection{Operational Semantics}\label{sec:sfp-opsem}
\TODO{Does adding kind abstraction and kind application business to terms meddle with the operational semantics? we need to make sure we can erase them and preserve the phase distinction property.}

\subsection{Meta-theory}\label{sec:sfp-metatheory}
\TODO{What is the consistency criteria? how is progress and preservation guaranteed?}

In \SFP, there are no kind coercions nor there are kind equality
constraints. This keeps the design and implementation simple but at
the expense of leaving out certain programs that can be well
typed. The introduction of the kind variables in a controlled environment allows to
side step a lot of issues that would make the system complex and difficult to work with.
A logical continuation of the work would be to answer the
question of how can we incorporate kind equalities in the type system.


\section{\SFK}\label{sec:sfk} % K for kind eq
% We have type equalities, why not kind equalities?
% But we would then have two kinds of equalities: type and kind.
% So why not just squish types and kinds together, making it truly impredicative
GADTs are expressed using explicit type equality predicates in the type system. For example, consider the following GADT that encodes type representations:

\begin{minipage}[ht]{0.5\linewidth}
\begin{lstlisting}
data TyRep : * -> * where
  TyInt : TyRep Int
  TyBool : TyRep Bool
\end{lstlisting}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{lstlisting}
data TyRep : * -> * where
  TyInt : a ~ Int => TyRep a
  TyBool : a ~ Bool => TyRep a
\end{lstlisting}
\end{minipage}

The right hand side is just an elaborated version of the left hand side with explicit type equalities equalities.
Now, we can use local type equalities to pattern match on the different values of \lstinline{TyRep} to produce appropriate values. The function \lstinline{zero} computes a default value for each type representation.

\begin{minipage}{0.5\linewidth}
 \begin{codef}
 zero : forall a. TyRep a -> a
 zero TyInt = 0
 zero TyBool = False
 \end{codef}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
 \begin{codef}
 zero : forall a. TyRep a -> a
 zero (TyInt (co : a ~ Int)) = 0 /> sym co
 zero (TyBool (co : a ~ Bool)) = False /> sym co
 \end{codef}
\end{minipage}

The operator \lstinline|sym| reverses the direction of the type equality \lstinline|co : a ~ Int| to \lstinline|Int ~ a| and then the cast operator, \lstinline{/>}, uses the flipped coercion to justify the return type of the function.

It may be tempting to extend \lstinline{TyRep} datatype to represent more complex types such as lists. However, \SFC is not expressive enough to achieve this without auxiliary definitions. The type system is not expressive enough allow higher kinded types. Consider a new definition of \lstinline{TyRep} extended with two new data constructors \lstinline{TyList}, which represents a list type, and \lstinline{TyApp} which represents type application.

\begin{minipage}[ht]{0.4\linewidth}
\begin{lstlisting}
data TyRep :: forall k. k -> * where
  TyInt :: TyRep Int
  TyBool :: TyRep Bool
  TyList :: TyRep []
  TyApp :: TyRep a -> TyRep b -> TyRep (a b)
 \end{lstlisting}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{lstlisting}
data TyRep :: forall k. k -> * where
  TyInt :: ((k ~ *),(a : k) ~ Int) => TyRep k Int
  TyBool :: ((k ~ *),(a : k) ~ Bool) => TyRep k Bool
  TyList :: ((k ~ * -> *), (a : k) ~ []) => TyRep k []
  TyApp :: (k1 ~ * -> *) => (k2 ~ *)
        => TyRep k1 a -> TyRep k2 b
        -> TyRep k2 (a b)
\end{lstlisting}
\end{minipage}

The new \lstinline{TyRep} accepts two parameters, the first is the kind and the second is the type that has the kind of the first parameter. Pattern matching on the data constructor now exposes kind equalities along with type equalities. The new \lstinline{zero} function that now also returns the zero case for list would look as follows:
% \begin{minipage}[ht]{0.5\linewidth}
\begin{codef}
 zero : forall (a:*). TyRep a -> a
 zero TyInt = 0
 zero TyBool = False
 zero (TyApp TyList _) = []
\end{codef}

With explicit kind equalities allowed in the system, while the case for \lstinline{TyInt} and \lstinline{TyBool} in \lstinline{zero} is the same as in \SFC, the case for \lstinline{TyApp} can now be allowed and type checked. The first argument to \lstinline{TyApp} is inferred to have kind \lstinline{* -> *} and the second argument is inferred to have kind \lstinline{*}, making the result to be of kind \lstinline{*}. As a side note, because we have constrained the kind of the type parameter to \lstinline{zero} to be a \lstinline{*}, we cannot write \lstinline{zero (TyApp ty b) = zero ty} as \lstinline{ty} would be inferred to have kind \lstinline{k -> *}.

Adding kind equalities to the system is non-trivial owing to its interaction with type equalities. A few challenges are:
\begin{itemize}
 \item \SFK squashes the types and kinds to be the same by adding the axiom \lstinline{* : *}, or ``type is of kind type''. This necessitates a new formalism that proves that the meta-theoretic properties of \SFK are carried over from \SFC. In dependently type languages \lstinline{* : *} axiom amounts to adding inconsistency, however this is not a problem in Haskell as all kinds are already inhabited;
 \item in \SFC, equalities could only exist between types of the same kind, but now, due to explicit kind equalities, it is possible to construct equalities between heterogeneous types. The matters get more complicated with polymorphic types;
 \item coercions should not interfere with the operational semantics of the language;
 \item Kind indexed GADTs need to be able to abstract over coercions and use them in the types, a feature missing in \SFC.
\end{itemize}

\subsection{Syntax}\label{sec:sfk-syntax}
In \SFP, we had disallowed promoting types that were indexed or dependent on a promoted type, we can now get rid of this restriction because we make no distinction between kinds and types.

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Type Vars} &\TyVar,\beta,\Co &  & \\
 \text{Term Vars} &\TmVar,y &\qquad\text{Indices} &i,n \in \mathbb{N} \\
 \text{Coercion Vars} &c & &
 \end{syntax}
 \begin{syntax}
 \text{Type Constants} &&T \bnfeq& (\to) \bnfor \star \bnfor H\\
 \text{Type level names} &&w \bnfeq& \TyVar \bnfor F_n \bnfor T\\
 \text{Propositions} &&\Prop \bnfeq& \tau\sim\sigma\\
 \text{Types and Kinds} &&\tau,\sigma,\kappa \bnfeq& w \bnfor \tau\App\tau %
 \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor %
 \Forall {c\co\Prop}\tau \bnfor \Cast\tau\Co \bnfor \tau\App\Co\\
 \text{Coercions} &&\MCo,\Co \bnfeq& c \bnfor \Refl\tau \bnfor \Sym\Co \bnfor \Trans\MCo\Co \\ % equiv relation
 && \bnfor& \shl{\ForallC {\MCo} {(\TyVar_1, \TyVar_2, c)} \Co} \bnfor \shl{ \MCo\App(\Co, \Co')} %
 \bnfor \shl{\ForallC {(\MCo_1, \MCo_2)} {(c_1, c_2)} \Co} %
 \bnfor \Co\At\MCo \bnfor \shl{\Co\At(\MCo, \MCo')}\\ % abstraction instanst
 && \bnfor& \MCo\App\Co \bnfor \Left \Co \bnfor \Right \Co %
 \bnfor \Nth i \Co \bnfor \shl{\Kind \Co} \bnfor T\App\many\phi \\  % compose/decompose
 \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
 \text{Patterns} &&P \bnfeq& H\App \shl{\many{\Telescope}}\App{\many{x\co\tau}} \\
 \text{Telescopes} &&\Telescope \bnfeq& \empt \bnfor \Telescope, \TyVar\co\kappa \bnfor \Telescope, c\co\Prop\\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N %
 \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \\
 && \bnfor& \Lam {c\co\Prop} {\Tm} \bnfor \Tm\App\Co \bnfor \shl{\Contra \Co \tau}%
 \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co\\

 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Substitutions} &&\Subst \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
 \end{syntax}
\caption{The Syntax of \SFK}\label{fig:sfk-syntax}
\end{figure}




\newcommand\TContra{
 \ib{\irule[\trule{t-contra}]
 {\CoKinding \TEnv {\Co} {T\App\many\phi \sim T'\App\many{\phi'}}}
 {T \neq T'}
 {\Kinding \TEnv {\tau} {\star}};
 {\Typing \TEnv {\Contra \Co\tau} {\tau}}
 }
}

\newcommand\KCAppCo{
 \ib{\irule[\trule{co-capp}]
 {\CoKinding \TEnv {\Co} {\tau\sim\tau'}}
 {\Typing \TEnv {\tau\App\MCo} {\kappa}}
 {\Typing \TEnv {\tau'\App\MCo'} {\kappa'}};
 {\CoKinding \TEnv {\Co\App(\MCo, \MCo')} {\tau\App\MCo \sim \tau'\App\MCo'}}
 }
}

\newcommand\KCAllT{
 \ib{\irule[\trule{co-$\I{\forall\tau}$}]
 {\substack{ \mathlarger{\CoKinding {\TEnv,\TyVar\co\kappa,\TyVar'\co\kappa',c\co\TyVar\sim\TyVar'} {\Co} {\tau\sim\tau'}}\\
 \mathlarger{\Kinding \TEnv {\Forall {\TyVar\co\kappa} {\tau}} {\star}}}}
 {\substack{ \mathlarger{\CoKinding \TEnv \MCo {\kappa\sim\kappa'}}\\
 \mathlarger{\Kinding \TEnv {\Forall {\TyVar'\co\kappa'} {\tau'}} {\star}}}};
 {\CoKinding \TEnv {\ForallC\MCo{(\TyVar,\TyVar',c)}{\Co}} {\Forall {\TyVar\co\kappa}{\tau} \sim \Forall {\TyVar'\co\kappa'}{\tau'}}}
 }
}

\newcommand\KCAllC{
 \ib{\irule[\trule{co-$\I{\forall\Co}$}]
 {\substack{\mathlarger{\CoKinding {\TEnv,c\co\Prop,c'\co\Prop'} {\Co} {\tau\sim\tau'}} \\
 \mathlarger{\Kinding \TEnv {\Forall {c\co\Prop} {\tau}} \star}%
 \quad\mathlarger{\Kinding \TEnv {\Forall {c'\co\Prop'} {\tau'}} \star}
 }}
 {\mathlarger{\fresh {\Set{c, c'}}{\Erased\Co}}}
 {\substack{\mathlarger{\CoKinding \TEnv {\MCo_1} {\sigma_1 \sim \sigma_1'}}\\
 \mathlarger{\CoKinding \TEnv {\MCo_2} {\sigma_2 \sim \sigma_2'} }}}
 {\substack{\mathlarger{p = \sigma_1\sim\sigma_2}\\
 \mathlarger{p' = \sigma_1'\sim\sigma_2'}}};
 {\CoKinding \TEnv {\ForallC{(\MCo_1,\MCo_2)}{(c, c')} {\Co}} {\Forall {c\co\Prop}{\tau} \sim \Forall {c'\co\Prop'}{\tau'}}}
 }
 }

\newcommand\KCInstCo{
 \ib{\irule[\trule{co-$\E\forall\Co$}]
 {\CoKinding \TEnv {\Co} {\Forall {c\co\Prop} {\tau} \sim \Forall {c'\co\Prop'} {\tau'}}}
 {\CoKinding \TEnv \MCo \Prop}
 {\CoKinding \TEnv {\MCo'} \Prop'};
 {\CoKinding \TEnv {\Co\At(\MCo, \MCo')} {\Set{c\mapsto\MCo}\tau \sim \Set{c'\mapsto\MCo'}\tau'}}
 }
}

\newcommand\KExtCo{
 \ib{\irule[\trule{co-ext}]
 {\CoKinding \TEnv {\Co} {\tau \sim \tau'}}
 {\CoKinding \TEnv \tau \kappa}
 {\CoKinding \TEnv {\tau'} \kappa'};
 {\CoKinding \TEnv {\Kind \Co} {\kappa \sim \kappa'}}
 }
}

\subsection{Static Semantics}\label{sfk-static-sem}
\begin{figure}[ht]
 \centering
 \begin{gather*}
 \fbox{$\Typing \TEnv M \tau$}\\
 \TContra
 \end{gather*}
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \Co {\tau \sim \tau}$}\\
 \KCAllT\\
 \KCAllC\\
 \KCAppCo \rsp \KCInstCo\\
 \KExtCo
 \end{gather*}
 \caption{Static Semantics of \SFK (Excerpt)}
 \label{fig:sfk-typing}
\end{figure}

\subsection{Operational Semantics}\label{sfk-op-sem}
\TODO{How does adding kind equality change the operational semantics
  of the language? }

\TODO{Push rule gets uglier?}

\subsection{Meta-theory}\label{sfk-meta-theory}
We have the following properties due by the way of static semantics.
All coercion proofs have no computational value, they are irrelevant
to the proof of equality. The ensures that even for heterogenous type
equalities, the proof of coercions between kinds, plays no value
during runtime. But is this limiting the expressivity in any way? We
do not want to allow only ``trivial'' equality proofs.


\TODO{\SFK is a representative formalization of the current GHC compiler. It supports kind heterogeneous type equality or in other words, the type language is un-kinded due to impredicativity.
}

\TODO{how to characterize type soundness and preservation?}


\part{IV: The Path Ahead and The Detours}\label{part:IV}
\section{Future Work}\label{sec:future-work}
Consider the !Con e c! from \pref{fig:tc-collection-fd}.
\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\begin{code}
class Con e c | c ~> e where
  empty :: FORALL e c. Con e c => c
  extend :: FORALL e c. Con e c => e -> c -> c

instance Con Int [Int] where
  empty = ...
  extend = ...
\end{code}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{code}
type family FDCon c : *
empty : FORALL c. Con (FDCon c) c => c
extend : FORALL c. Con (FDCon c) c => e -> c -> c

axiom coFDCon[Int]Int : FDCon [Int] ~ Int
empty_[Int]  : Con [Int] (FDCon [Int]) => Int
extend_[Int] : Con [Int] (FDCon [Int]) => Int -> [Int] -> [Int]

\end{code}
\end{minipage}
\caption[FunDeps in \SFC]{Elaborating Functional Dependencies into \SFC}
\label{fig:elab-fundeps-sfc}
\end{figure}
Functional dependencies introduce the
uniqueness criteria for a subset of the multiparameter type classes.
We may expect obtaining a natural encoding of functional dependencies
via the mechanism of open type functions. In this encoding scheme,
each functional dependency induces a special
type function that relates the determiner of the functional dependency
to the determinant. Each typeclass instance introduces an axiom that
encodes this mapping. Further, every type that mentions the
determinant of the functional dependency gets replaced by the type
function. For example, as shown in \pref{fig:elab-fundeps-sfc},
!Con e c! gets converted to !Con (FCCon c) c!. While this encoding is
sound, and works for simple cases, it does not cover all the use cases.
Consider a specialized case of containers, !UCon con!, for unsorted containers,
where the container elements may not have a ordering function defined
on them.

\begin{CenteredBox}
\begin{code}
class Con elem con => UCon con
\end{code}
\end{CenteredBox}

Representing the typeclass constraints in a first order predicate logic, we get the following
(seemingly broken) formula:
\[
\Forall{con}{\texttt{UCon}\App con \then \texttt{Con}\App elem\App con}
\]
It is broken, as the type variable !elem! is out of scope.
The intended meaning of the class declaration is:
whenever we have an evidence that says the typeclass constraint
!UCon con! is satisfied, we can also assume, or produce, an
evidence for typeclass constraint !Con elem con!.
Although it may seem that the type parameter
!elem! is free in this definition, it is uniquely
determined due to the functional dependency, !con ~> elem!.
The type parameter !con! is existentially
bound; now made explicit in the formula below:
\[
\Forall{con}{\texttt{UCon}\App con \then \Exists{\bang elem}\texttt{Con}\App con\App elem}
\]
To tackle this problem \citet{karachalias_elaboration_2017} develop an
elaboration scheme for typeclass and instance declarations which
elaborates functional dependencies. However,
their work cannot provide an explanation of the general theory of
the simplifying and improvement of types\cite{jones_simplifying_1995}
as functional dependencies may introduce partial type
refinements. Functional dependencies remain a type inference level
artifact and have no evidence at the level of \SFC. This gap is evident
from the comments from the GHC developers, ``If someone can explain how to
elaborate functional dependencies into well-typed evidence in System
FC, that would be good. My inability to do so was one of the reasons
we developed type families and System FC in the first
place.''\footnote{Comment on GHC Issue \#9627:
  \url{https://gitlab.haskell.org/ghc/ghc/-/issues/9627\#note\_88529}}
The consequence of the incomplete encoding of functional
dependencies is reflected in GHC implementation:
obscure type errors, and worse the type checker is unnecessarily more
restrictive than it should be. For example, issues with functional
dependencies interacting with associated typeclasses\footnote{GHC Issue \#11534:
  \url{https://gitlab.haskell.org/ghc/ghc/-/issues/11534}},
GADTs\footnote{GHC Issue \#345:
  \url{https://gitlab.haskell.org/ghc/ghc/-/issues/345}},
or more generally, causing non-confluence issues in the GHC constraint
solver \footnote{GHC issue \#18851:
  \url{https://gitlab.haskell.org/ghc/ghc/-/issues/18851}}

The functional dependency style has advantages over the
functional style\cite{jones_language_2008}. Consider the !extendWithOne!
function which uses the !extend! method from !Con con elem!
typeclass as shown in \pref{fig:fundeps-vs-tfs}.
\begin{figure}[ht]
\centering
\begin{minipage}{0.5\linewidth}
\begin{code}
       extendWithOne :: Con con Int
                     => con -> con
       extendWithOne 1 c = extend 1 c
\end{code}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{code}
      extendWithOne :: (Elem con ~ Int, Con con)
                    => con -> con
      extendWithOne 1 c = extend 1 c
\end{code}
\end{minipage}
\caption[FundDeps or TFs]{Functional Dependencies vs Type Functions}
\label{fig:fundeps-vs-tfs}
\end{figure}

In the associated type class setting as the associated type is
independent of the typeclass itself, the !extendWithOne! function will
need to have two constraints. One that fixes the element of the
container, the other that constrains the container type.
In general, writing a polymorphic function over the
determiner of the typeclass will require such redundant
constraints. This means that the user has to provide more hints to the
constraint solver to convince it that the program is type correct.
Further, the compiler writer needs to write a non-trivial solver that
now accepts and verifies explicit equality constraints.

\begin{figure}[ht]
  \centering
\begin{tikzpicture}[->,>=stealth,auto,
roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=8mm},
squarednode/.style={rectangle, draw=red!60, fill=red!5, very thick, minimum size=8mm},
node distance=1.5cm
]

%Nodes
\node[squarednode]      (centercirc)  {$\substack{Typed\\Intermediate\\ Language}$};
\node[roundnode]        (leftcirc)    [left=of centercirc] {$\substack{Functional\\Dependencies}$};
\node[roundnode]        (rightcirc)   [right=of centercirc] {$\substack{Type\\Functions}$};

%Lines
\draw[dashed, -{Stealth[length=3mm, width=2mm]}] (leftcirc.east)  -- (centercirc.west);
\draw[dashed, -{Stealth[length=3mm, width=2mm]}] (rightcirc.west) -- (centercirc.east);

\draw[-{Stealth[length=3mm, width=2mm]}] (leftcirc.north)  to
[out=30,in=150] (rightcirc.north) node[midway, above=1.6cm] {\cite{karachalias_elaboration_2017}};
\draw[-{Stealth[length=3mm, width=2mm]}] (rightcirc.south) to
[out=210,in=340] (leftcirc.south) node[midway, below=1.6cm] {\cite{jones_language_2008}};
\end{tikzpicture}
  \caption[Open Problem]{Compiling Functional Dependencies and Type
    Functions to a unified Typed Intermediate Language}
  \label{fig:future-work}
\end{figure}


The hope is to find an acceptable solution to find a good representation
of the behavior of functional dependencies in a core typed
intermediate language. This would liberate the language designer from
the choosing one language feature over another, and in turn give the
user the flexibility to choose the style of their liking.

% and on the way liberate the compiler writer to make a choice between
% having fundeps and type functions?

% Closed type classes should fall out directly from


% There is no satisfactory account of functional
% dependencies in any of these system although they inherently exhibit
% type equality. Functional dependencies have the following nice
% properties~\cite{jones_simplifying_1995}:
% \begin{itemize}
% \item They aid type inference by improving and simplifying types
% \item Each type equality is embedded in a typeclass constraint
% \end{itemize}

% The first point is good from a programmers perspective. The types that
% are inferred by the typechecker may be ambiguous, but with the presence of
% functional dependencies, the inferred type can be matched exactly with
% the semantics.

% \begin{code}
%   t :: C Int b => Int -> Int
%   class C a b | a ~> b
% \end{code}

% there can only be one unique value of b, and hence this type is not
% ambiguous. The compiler has only one option to choose from.
% The improving and simplifying types goes beyond just the above
% ``simple'' example.

% For the second point, the current typechecker implementation is
% plagued with cases that handle class constraints and those that handle
% type equality constraints. With functional dependencies, we would have
% type equalities only due to class constraints. The typechecker can
% then be simplified by having only once case that handles typeclasses
% and it embeds type equalities uniformly. \TODO{Is this safer that
%   having bare type equalities? maybe we can relax $\Good\TEnv$
%   condition?}

% Both functional dependencies and associated types provide the same set
% of features: improve type inference by resolving ambiguous types and
% enable type level computations. It would be logical to pose
% questions such as: is one style more expressive than the other or is
% it just a matter of stylistic preference? In other works, are
% functional dependencies equivalent to associate types? Jones' original
% work does not delve into formalization aspects of functional
% dependencies feature. ~\citet{jones_language_2008} claim (without
% proof) that they  are equivalent in expressiveness but favor
% functional dependencies as it introduces minimal overhead in terms of
% type system implementation. ~\citet{karachalias_elaboration_2017} use
% an elaboration technique go the other way around. They however fail to
% compare how their system translates improvement in type inference
% which is offered by a functional dependencies and pose it as an open
% question.


% It is worthwhile to consider the alternative design of \SFC: what if
% the type equality proofs were at the level of types instead of at the kind
% level. The consequence of which would be that the formalization would
% have to take into consideration the computational complexity that
% might arise due to operational semantics of the coercions within the
% typechecker. An interesting research question still remains: Is it
% better to have equality proofs as types or equality proofs as kinds?
% There are systems designed with both, type equalities at the level of
% kinds and type equalities at the level of types. There is no study of
% whether both of them are equivalent or one may want to prefer the
% other over which use cases.

\section{Related Work}\label{sec:related-work}
%%%%%%%%%
\subsection{Ad-hoc polymorphism}\label{sec:rw-adhoc-poly}
\subsubsection{Typeclass alternatives}
\citet{kaes_parametric_1988} had very similar ideas related to
implementation aspects of adhoc polymorphism before
~\citet{wadler_polymorphism_1989}. The work on functional dependencies
for multi-parameter typeclasses is directly influenced by parametric
type classes~\cite{chen_parametric_1992}. Implicit
objects~\cite{oliveira_typeclasses_2010} provide a mechanism for the
programmer to choose the intended behavior instance if
there are multiple available options. This pushes the burden of choosing the
right instance on programmer whenever the typechecker cannot uniquely
resolve an option or when the programmer wants to force the
typechecker to resolve it to a specific instance. Constraint handling
rules(CHR)~\cite{fruhwirth_theory_1998,stuckey_theory_2005} is a recent
way to account for operator overloading. The limitation of using CHRs
is that the programmer needs to ensure confluence external to the
system to be able to get coherence guarantees.

\subsubsection{Traits and Interfaces}
Object oriented languages provide an explicit mechanism of abstraction
over behavior of classes in the form of traits (cf. Rust, Scala) or
interfaces (cf. Java, C\#). In dynamically typed languages
(cf. python), traits are emulated using implicit programmer
conventions. Typeclasses subsume traits and interfaces. Typeclasses
can be higher kinded, which often is lacking in object oriented
languages, with an exception of Scala where traits can be declared on
higher kinded types by means of generics. There is no equivalent of
multi-parameter typeclasses and funtional dependencies in interfaces
and the expressivity of type level computation is non-existent. Traits
in Scala 3 can have parameters, but there is no way to express a
functional relation between the parameters. Superclasses (not to be
confused with superclasses for classes in object oriented languages)
mechanisms exists in both typeclasses and traits and interfaces that
establish a containment relationship. The abstract behaviour described
by superclasses is contained in the sub-classes.


% \subsubsection{Automatic Typeclass Derivation}
% For simple cases we should be able to automatically derive typeclass
% instances such as !Show a!. This is a principled way of ``scraping your
% boilerplate''. The key point is that a simple pattern is identifiable
% to generate these implementation functions. For ADTs deriving
% machinery is straightforward. Recall that an ADT can be realized as
% named sums and products. Deriving an typeclass instance amounts to
% identifying the simple common substructure. For eg. consider a data
% structure

% \begin{CenteredBox}
% \begin{code}
% data T = X Int | Y (String, Bool)
% \end{code}
% \end{CenteredBox}

% In this artificial example, a cannonical !Show! instance will be
% printing the Name of the data constructor and then printing each of
% the fields, each of which is just a recursive call to the show
% function. The generated them is then typechecked and desugared so that
% the recursive calls to show are called at the appropriate type.
% For example, for the !X Int! case the !show (Y s b)! will look like

% \begin{CenteredBox}
% \begin{code}
% show (Y pair) = "X" ++ show pair
% \end{code}
% \end{CenteredBox}

% the show in the left hand side of equation is called at type
% !(Int, Bool)!. As we know how to print Int and a Bool and also a pair of an
% !Int! and a !Bool!, we can also print the !Y pair! case.

% GHC provides an automatic efficient automatic deriving machinery
% for a wide variety of typeclasses. This feature is a definitive time
% saver by automatically generating boilerplate code.

% \begin{CenteredBox}
% \begin{code}
% data T = X Int | Y (String, Bool)
%                 deriving (Show)
% \end{code}
% \end{CenteredBox}

\subsubsection{Closed Typeclasses}
Closed typeclasses, as
opposed to the open typeclasses, cannot be extended with
instances. GHC/Haskell does not support closed typeclasses out of the
box. In Scala, sealed traits accomplish this behavior by using modules
as implicit boundaries to prohibit extensions. This can be useful in
cases where we want the typechecker to be able to deduce that certain
typeclass constraints can never be satisfied. In current Haskell,
unsatisfiable typeclass constraints for a function type, amounts dead
code as such functions cannot be used.
With a support to closed typeclasses, the typechecker would be
able to identify such cases and warn the programmer about the dead code.
Only partial behavior of closed typeclasses can be
encoded in current Haskell. This is achieved by limiting the export
names from modules. While this establishes the closed-ness property of
the typeclass. The typechecker cannot use this information that the
unexported typeclass cannot be extended and use it to reason about
whether the constraint can be solvable or not.

\subsubsection{Instance Chains}
Instance chains~\cite{morris_instance_2010} enable finer grained
control of which instance should be used to satisfy
a typeclass constraint. The instances for !Show! typeclass defines how
a value can be converted to a string:

\begin{CenteredBox}
\begin{code}
type String = [Char]
class Show a where show :: a -> String
class Show a => Show [a] where show = ...
class Show String where show = ...
\end{code}
\end{CenteredBox}
If we know how to write a !show! function for any value of a particular
type, we can write a generic instance of show on a list of such types.
However, we may want to override this behavior for some specific
instances. For example, the type !String! in GHC is declared as a
type synonym for a list of !Char!---(!type String = [Char]!).
The typechecker now has two ways of resolving the typeclass
constraint !Show String!: it can either use the generic instance via
!Show [Char]!, or it can use the specific instance !Show String!. The
typechecker has no way cannot decide which instance is more appropriate.
Such instances are called \emph{overlapping instances}.
Haskell disallows overlapping instances as this gives rise to
incoherence issues\cite{jones_coherence_1993}.
Instance chains solves this problem by allowing programmers
to specify in what order the the instances can be resolved. It also
subsumes closed typeclasses by providing a capability to express
a default failure clause in an instance chain declaration.

\begin{CenteredBox}
\begin{code}
instance Show String where show s = ...
else instance Show a => [a] where show s = ...
\end{code}
\end{CenteredBox}

\subsection{Type Functions}\label{sec:rw-type-fun}
\subsubsection{Closed type families}
The programmer may want to write type functions which has a restricted
extension. For example, the previously defined !Add! type
function is defined only on !Z! and !S n! types. Closed type
families~\cite{eisenberg_typefamilies_2014} help in
expressing such use cases. For example, consider the code snippet shown below:

\begin{CenteredBox}
\begin{code}
type family Add m n where
  Add (S m) n = S (Add m n)
  Add _ n = n
\end{code}
\end{CenteredBox}

An additional advantage is that the programmer can also define a
default case handler (!Add _ n = n!) for such type function
definitions. This declaration is inexpressible using open type
functions as the axioms which are introduced by type function
instances are order independent: the order in which the instances
are declared is independent of the order in which they are searched by
the solver. Writing closed type functions is very similar to writing case
statements at term level. The difference is that instead of writing
equations for terms, the user write equations for types. While this
makes the system expressive and helps in making error messages related
to type functions more user friendly, closed type families with
non-linear type patterns---such as, !Add x x!---are reducible to
term rewriting theories\cite{mizuhito_rta_1995}, which are conjectured
to guarantee confluence. The meta-theory needs to appeal to infinitary
unification~\cite{jaffar_efficient_1984} of types to claim
consistency. This is unsatisfactory, as Haskell does not have
infinite types.

\subsubsection{Injective type famililes}
Type functions are non-injective by default. It can be a useful programming idiom to declare a type function to be injective so that the typechecker can use this information to rule out programs which violate this property and further use the injective property to refine the type information. For example, consider an injective type function !F! declared to be injective below:

\begin{CenteredBox}
\begin{code}
type family F a = r | r ~> a
type instance F Int = Char
type instance F Bool = Int
\end{code}
\end{CenteredBox}
This syntax is direct inspiration from functional dependencies for typeclasses. The declaration !r ~> a! says that the result of the type function !r!, can uniquely determine the argument of the type family !a!. Now, during type checking the instances of type functions, the typechecker would reject any axiom which would make !F! non-injective. For example, in the above code, the declaration !type instance F Char = Char! would be rejected by the typechecker. Further while trying to solve a wanted type equality constraint such as, !F a ~ Char!, the typechecker can leverage the fact that !F! is injective, and hence, there can be only one solution for the equation: !a ~ Int!, thus aiding the solver with extra information to solve the original wanted constraint. Injective type families~\cite{stolarek_injective_2015} can be extended for closed type families as well.

\subsubsection{Constrained type families}
Constrained type families~\cite{morris_typefamilies_2017} allows type computation to proceed only when the type family application can be provably reduced to a ground type. This simplifies the meta-theory of the language significantly. Constrained type families can prove confluence of type rewriting hence gives stronger type soundness guarantees. It avoids the pitfall of having to prove consistency by resorting to the use of infinitary unification and a conjecture to prove consistency. Closed type classes, which otherwise would need special infrastructure, fall out naturally in this system. The formalized system however, does not have a corresponding  working implementation, and there is no clear way of retrofitting representational type equality without resorting to the roles infrastructure as in \SFR.

\subsubsection{Partially Applied Type families}
\SFC had a restriction on the usage of type functions; they have to
be fully applied. An unsaturated application of a type function can
creep in type unsoundness due to unrestricted coercion lifting issues
similar to the discussion made in \SFP. The restriction can be
relaxed by stratifying the function
hierarchy~\cite{kiss_higher-order_2019}: the functions that can be
matched on, and the other that cannot be matched on. This allows the
typechecker to reject unsafe programs which could violate the
safety property.

\subsubsection{Partial Type Constructors}
Consider the associated type appearing as type argument within a data
constructor:

\begin{CenteredBox}
\begin{code}
data E a = MkE (Elem a)
\end{code}
\end{CenteredBox}

GHC designers decided to ignore the question of whether typechecking
an instance of an element of !E a! should check for satisfaction of
the associated !Con a! typeclass. This makes the type system restrictive as to
ensure safety, certain typeclass abstraction machinery cannot be used
leverage coding patterns such as the constrained monad
problem~\cite{sculthorpe_constrained-monad_2013} or its generalized
version, the constrained typeclass problem. Partial type
constructors~\cite{jones_partial_2019,ingle_partial_2022} explores the language design
space which takes into consideration such type partiality. The
advantage of which allows a simple and elegant solution to the
constrained typeclass problem.

\subsection{Modules}\label{sec:rw-modules}
% Modules and typeclasses
% are they same
% no, different.
% Their co-existence incoherent
Modules help decompose large programs into smaller programs by allowing the programmers to establish separation of concerns. All general purpose programming languages provide some capabilities to make programs modular. ~\citet{macqueen_modules_1984} pioneered the formalizing semantics of modules in Standard ML.
Various attempts~\cite{dreyer_modular_2007, wehr_ml_2008, white_modular_2014} have been made to enable a happy co-existence of parametric and implicit adhoc polymorphism together in a language by encoding typeclasses as modules and vice versa. However, every attempt has failed to avoid incoherence issues without crippling the language expressivity. ML and its variants~\cite{milner_definition_1997,leroy_ocaml_2023} disallow allow operator overloading to enable modular compilation, while Haskell like languages compromise on the correct module behavior to allow operator overloading via typeclass mechanisms.


\subsection{Type Inference}\label{sec:rw-type-inf}
The monograph did not delve into the surface syntax typechecking aspects of the
compiler. We assumed that there either exists an algorithm which can
efficiently compute the types of a programmer provided term, which
does not contain any types via a complete type
construction~\cite{milner_theory_1978}, or the  programmer supplies
sufficient typing annotations so that the type construction problem
becomes tractable via partial type
construction~\cite{pierce_local_2000, dunfield_bidirectional_2021}.
\HMX and its variants is the state of the art type modular inference
framework that is based on by using constraint handling rules. The
stratified type inference algorithm~\cite{pottier_stratified_2006} works in two
phases: the first phase collects the typing constraints and then the
second phase solves them. This is different than the greedy approaches
used by original typing algorithms~\cite{lee_proofs_1998} $\mathcal{W}$
and $\mathcal{M}$. The stratified type inference systems are better
suited for languages that support GADTs and language features that
introduce local constraints~\cite{vytiniotis_outsideinx_2011}.


\section{Conclusion}\label{sec:conclusion}
\begin{figure}[ht]
 \centering
 \begin{tabular}[ht]{c | c}
 Parametric Features                    & Non-parametric Features \\
 \hline\hline
   \multirow{2}*{Modules~\cite{macqueen_modules_1984}}    & {Typeclasses~\cite{wadler_polymorphism_1989}}\\
                                        & \emph{Functional Dependencies}~\cite{jones_tcfd_2000}\\
   \hline
   ADTs~\cite{burstall_hope_1980}         & \emph{GADTs}~\cite{cheney_first-class_2003}\\
   \hline
   \multirow{3}*{\emph{Generative abstract types}~\cite{breitner_safe_2014}}
                                        & \emph{Open Type Functions}~\cite{schrijvers_type_2008}\\
                                        & Closed Type Functions~\cite{eisenberg_typefamilies_2014},\\
                                        & \emph{Associated types}~\cite{chakravarty_associated_2005}
 \end{tabular}
 \caption{Features of Haskell}
 \label{fig:haskell-lang-features}
\end{figure}
We surveyed some important language features--the ones which functional
programmers prefer---and seen in depth, a theoretical
foundation of a modern day declarative functional programming
language. The language features are tabulated in the
\pref{fig:haskell-lang-features}. The features that we have
covered in some detail in this monograph are emphasized.
There are two ways of supporting a new programming language
feature, either via encoding them with the help of libraries or, via
supporting them as a core language feature. In the latter scenario, we
realized that composing different language features is non-trivial and
their subtle interactions can threaten semantic consistency. We
first identified that the three seemingly different language features:
generative abstract types, generalized algebraic datatypes and type
functions can be encoded using a simple single construct: type
equality. We then showed how non-parametric features
interacting with parametric features of the language that contain
explicit type equalities can cause type unsoundness bugs, unless they are
treated carefully. Taming ad-hoc language features is non-trivial and requires
sophisticated proof techniques to formalize and argue about their
correctness. As new programming languages are designed, built and
used, it is not only important to identify and carry over the good design principles
established by theory using the formalization techniques, but also
strengthen the foundational theory of programming languages
by understanding the real requirements of practice.

%\newpage
%%%% Bibliography
\bibliography{comp}%%%%%%%%%


\part{V: Appendix for Proofs, Definitions, and Thorny Bushes}\label{part:appendix}

\begin{defn}[Coercion Substitution in Types]
The special case is $\Refl\tau$. Let $\Subst = \Sub{\alpha}{\Co}$\\
\begin{minipage}{0.5\linewidth}
\begin{flalign*}
\Subst c &:= c\\
\Subst T &:= T\\
\Subst\Refl{\beta} &:= \texttt{if}~ \alpha = \beta~ \texttt{then}~ \Co~ \texttt{else}~\Refl\beta\\
\Subst\Sym\nu     &:= \Sym{\Subst\nu}\\
\Subst\Trans\nu{\nu'} &= \Trans{\Sub\alpha\Co\nu}{\Sub\alpha\Co\nu'}\\
\Subst\Right\nu     &:= \Right\Subst\nu
\end{flalign*}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{flalign*}
\Subst{\beta} &:= \texttt{if}~ \alpha = \beta~ \texttt{then}~ \Co~ \texttt{else}\beta\\
\Subst(\tau\to\sigma)  &:= (\Subst\tau)\to(\Subst\sigma)\\
\Subst(\tau\App\sigma) &:= (\Subst\tau)\App(\Subst\sigma)\\
\Subst(\Forall {\beta\co\kappa}\tau) &:= \Forall {\beta\co\kappa}{(\Subst\tau)}\\
\Subst(\nu\At\tau)     &:= (\Subst\nu)\At(\Subst\tau)\\
\Subst\Left\nu     &:= \Left\Subst\nu\\
\Subst(F\App\many\tau) &= F\App\Subst\many\tau
\end{flalign*}
\end{minipage}
\end{defn}

% \begin{theorem}[Coercion Lifting]\label{thm:sfc-coercion-lifting-appendix}
%  If $\TyKinding {\TEnv,\TyVar\co\kappa'}\phi\kappa$, where $\TyVar$ is free in $\phi$
%  and does not appear free in $\TEnv$, and
%  $\CoKinding\TEnv\Co{\sigma_1\sim\sigma_2}$, and $\TyKinding\TEnv{\sigma_i}\kappa'$
%  then, $\CoKinding\TEnv {\Sub{\TyVar}{\Co}\Refl{\phi}}
%  {\Set{\TyVar\mapsto\sigma_1}\phi\sim\Set{\TyVar\mapsto\sigma_2}\phi}$
% \end{theorem}
\begin{proof}[\pref{thm:sfc-coercion-lifting}]
By induction on the structure of the derivation of the well kinded
types we have the following cases:

\begin{itemize}
\item[\trule{ty-var}] $\alpha$: It is a well formed type (assumption), the coercion
  substitution is well formed (assumption), so the claim holds,
  $\TyKinding \TEnv {\Sub{\alpha}{\Co}\Refl{\alpha}}
  {\Sub{\alpha}{\sigma_1}\alpha \sim \Sub{\alpha}{\sigma_2}\alpha}$.
  If the type variable is not $\alpha$, the substitution has no effect
\item[\trule{ty-con}] $T$ is a constant, so the substitution does not
  have any effect.
\item[\trule{ty-fcon}] $F\App{\many\sigma}$: By
  induction principle application on the well kinded type arguments
  $\many{\TyKinding \TEnv \sigma \kappa}$
\item[\trule{ty-all}] $\Forall{\alpha\co\kappa}{\tau}$:
  after necessary renaming gymnastics to avoid variable capture,
  induction hypothesis with weakening applies to $\TyKinding {\TEnv,\alpha\co\kappa} \tau \kappa$
\item[\trule{ty-app}] $\tau\App\sigma$: substitution distributes
  over the type application and induction on the derivations of
  $\TyKinding \TEnv \tau {\kappa \to \kappa'}$
  and $\TyKinding \TEnv \sigma {\kappa}$
\end{itemize}
\end{proof}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% eval: (visual-line-mode 1)
%%% eval: (auto-fill-mode 1)
%%% eval: (tex-source-correlate-mode 1)
%%% eval: (flyspell-mode 1)
%%% TeX-command-extra-options: "--synctex=1"
%%% TeX-master: t
%%% End: