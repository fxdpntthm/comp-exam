\newif\ifcomments\commentstrue

\RequirePackage[svgnames,dvipsnames,prologue,x11names]{xcolor}

\documentclass[screen,nonacm]{acmart}

\usepackage{comp}

\title{Practical Functional Programming with \SFC and its Extensions}
% \subtitle{Extensions to System F}

\author{Apoorv Ingle}
%   \orcid{0000-0002-7399-9762}
\affiliation{%
  \institution{University of Iowa} \department{Department of Computer Science} \streetaddress{McLean Hall} \city{Iowa City} \state{Iowa} \country{USA}}
% \keywords{typeclass, type family}

\begin{document}

\begin{abstract}
This monograph is a juxtaposition of practical (typechecker) and
meta-theoretical (formalization) aspects of a modern statically typed
functional programming language. We first lay
some philosophical underpinnings and the general scope of the research
area in the \pref{sec:introduction}. We then survey some quintessential
and advanced language features of a statically typed
functional programming language in the \pref{sec:language-features}.
Some of these features are specific to the Haskell programming language.
This section serves as a \emph{programmers view} of the language.
The \pref{sec:sfc} we formalize \SFC, which
forms the basis of the core intermediate language
of GHC, the de facto Haskell language compiler.
In the \pref{sec:sfc-encoding-features}, we sketch how \SFC
can efficiently encode a diverse set of language features.
These features were previously either impossible or required non-trivial extensions
to \SF making the overall system convoluted and complex to maintain.
This section servers as the \emph{compiler writers} view of the
language. The vanilla \SFC is insufficient to encode all the programming
idioms which programmers would want to express. In the
\pref{sec:fc-extensions}, to this effect, we study different
extensions of \SFC: \SFR makes type equality finer grained while
providing stronger type soundness guarantees, \SFP makes the kind system more
expressive, and \SFK squashes the distinction between types and kinds
making the type (and kind) level computation even more
expressive. Each of these extensions are formalized and backed by a formal
argument for its correctness. Finally, in the \pref{sec:related-work} we point out some important related
work not covered in this monograph, give some directions to future
work in the \pref{sec:future-work}, and make some concluding remarks
in the \pref{sec:conclusion}.
\end{abstract}

\maketitle
\section{Introduction}\label{sec:introduction}
Constructive mathematics and computer programming are acts of doing
the same thing. They both involve identifying and then exploiting the right
abstractions to solve a problem. Mathematicians tackle it from top to
bottom; by building the abstractions first and then enriching them
to make them represent real world problems. Programmers on the other
hand, turn this process on to its head; they first write a software
program which solves a very specific task, and then by means of
iterative code refactoring, build the right level of
abstraction. The refactoring process also serves as a means to fix the incorrect program behaviors.
This is a well established, named observation in the field of computer science: Curry-Howard or the
Brouwer–Heyting–Kolmogorov correspondence\cite{wadler_propositions_2015,han_deep_2023}
and it forms the basis for the field of type theory\cite{barendregt_lambda_2013,hottbook_2013,nordstrom_programming_1990}.

A compiler is a program that transforms a higher level language, suitable for
humans, to a lower level language, suitable for computers. It may
perform this transformation via a multi-step compilation
process\cite{essentials_siek_2023}. Every useful software program
evolves over time by means of extensions, and a compiler is no
exception to this rule. A new language feature is reflected as
a compiler extension. One way of realizing this extension is by
transforming the new language feature by encoding them
into the existing high level language features. However, some exotic
language features may not be able to leverage this transformation
scheme. This necessitates enrichment to the intermediate language to
support the new language feature. Extensions to the intermediate
language, however, does not scale. Multiple extensions to the
intermediate language causes a feature clutter, making it difficult to
maintain. We then need to find the right abstraction to refactor the
intermediate language so that the different high level language
features can be encoded into a simplified intermediate language that
possibly needs fewer extensions.

In compilers for a statically typed programming languages, the type checking component, or
simply, the typechecker, is the gate keeper. Its purpose is to ensure
that it does not let \emph{bad} programs pass, and flag the
appropriate offending parts of programs to the programmer.
This gate keeping is desirable; it aids the programmer,
by guaranteeing that a class of program errors---such as runtime null
pointer exceptions due to incorrect function arguments---can never
occur. They can then concentrate on the other, more important aspects
of the program. In the programmers mind ``If my program was type
checked by the compiler, it will definitely run without
surprises''. On the other hand, the typechecker needs to allow
the \emph{good} programs---the ones that the programmer definitely
expects to work in all cases---to pass without an intense struggle.
The safety net casted by the typechecker should not become the reason
of its undesirability. For this reason, the structure of the types need to be
sufficiently enriched to allow complex programmer ideas be expressible,
while rejecting definitive runtime unsafe programs.
As an example, consider writing an abstract syntax tree (AST) for representing the
language for a compiler. The compiler writer may want to capture certain semantic
invariants on the data structure representing the AST. This avoids the
need for having predicate functions to explicitly check the invariants
as the data structure is correct by construction. For example, in the
AST representing the control structures, such as !if! or !while!
loop, the sub-term representing the conditional needs to have a
!Boolean! type: it better evaluate to either !True! or
!False! and nothing else. Or even more complex invariants, where in a
multi-stage compiler pipeline, the AST may be gradually enriched with
more information at every pass. The type structure ensures that if the
information is enriched in the third pass, trying to
access that information in the first pass will result in a
type error. All this while also ensuring maximum data structure
sharing for memory efficiency\cite{peyton_jones_trees_2017}. Once we
identify the right abstractions to express these invariants in
the structure of types, we will use type theory as a mathematical tool
for a principled reasoning of its correctness.

\section{Features of Typed Functional Programming Languages}\label{sec:language-features}%%%%%%%%%
\subsection{Functions and Polymorphism}
\subsubsection{Functions}
There can be no functional programming language without a support for first class
functions. They capture the essence of the program execution by encoding the logic via
transforming data. An example of a function declaration in Haskell is that of an identity
function, !idI! over integers and !idC! over characters shown below.

\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
idI :: Int -> Int
idI i = i
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
idC :: Char -> Char
idC i = i
\end{code}
\end{CenteredBox}
\end{minipage}

The type signature !idI :: Int -> Int! specifies the input-output behavior of the
function: !idI! accepts an argument of type !Int! and returns a value of type !Int!. The
function definition !idI i = i! specifies how the function satisfies the typing
specification: it takes an argument !i! and returns it unmodified. Functions declared in a
functional programming language resemble mathematical functions in two ways: first, they
are referentially transparent, i.e. a call to the same function twice with the same
arguments will always return the same result. The expression, !idI 3!, will always
evaluate to !3!. Second, they are declarative, in a sense they abstract away the
implementation and execution details of how the function executes on the actual underlying
hardware; there is no mention of allocating and freeing program memory or even an explicit
return. This declarative style, may discomfort skeptics on the execution efficiency, but
liberates them from the burden of understanding the exact execution semantics of the
hardware.

\subsubsection{Parametric Polymorphism}
Functions like !idI!, while enough to build large programs, are too cumbersome in
practice to program with. It is sometimes necessary to define the same functionality over
different types. For example, an identity function over !Char!, !idC! has the exact same
definition as !idI!. The functions that work on base types, such as !Int!, are called as
monomorphic functions. To enhance re-usability of programs, it is necessary to be able to
abstract over types.


\begin{minipage}{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
id :: t -> t
id a = a
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
hotm :: t -> t
hotm = id id
\end{code}
\end{CenteredBox}
\end{minipage}

The function !id!, shown above, abstracts over all types by an implicit universal
quantification over the type variable !t!. Such functions are called parametrically
polymorphic\cite{strachey_fundamental_2000} functions. The compiler
can deduce which instance of the polymorphic function is required when
an argument of a concrete type is passed as an argument to a
polymorphic function. Parametric polymorphism also aids the language feature for defining and using
higher order functions as functions can now be passed as arguments to other
functions. For example, the typechecker will infer the type of the
left hand side !id! in the definition of !hotm! as !(t -> t) -> (t -> t)!. ML\cite{milner_logic_1975,milner_theory_1978}
was the pioneer among the functional programming languages in
introducing a declarative style implicit parametric polymorphism.
Imperative languages also allow user defined
polymorphic functions but require, explicit pointer casts as in the
language C, or the use of template programming, as inc the language C++.

\subsubsection{Adhoc Polymorphism}
Consider the following two terms, !addI! and !addF!:

\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
addI :: Int = 1 + 2
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
addF :: Float = 1.1 + 2.3
\end{code}
\end{CenteredBox}
\end{minipage}

In the definition of !addI!, the operator (!+!) is applied to two
integers, but in the definition of !addF!, the operator (!+!) is
applied to two floating point numbers (!Float!). Although the
programmer uses the same symbol, the meanings of the two terms are
distinct: !addI! adds two !Int!s and returns an !Int! while the !addF!
adds two !Float!s and returns a !Float!. The low level compiler generated code
for each of them would also differ as they would make a call to two
different built-in subroutines. This scheme of name punning, which depends on the
type of arguments, or the context of where the function appears, is called as adhoc
polymorphism\cite{strachey_fundamental_2000}. Implicit operator overloading is a popular
mechanism to implement adhoc polymorphism. The compiler resolves the overloaded
operator (!+!) to the actual operator (!int_plus! or !float_plus!)
during one of the phases of typechecking.

\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.3\linewidth}
\begin{code}
class Num a where
  (<=) :: a -> a -> Bool
  (==) :: a -> a -> Bool
  (+) :: a -> a -> a
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{code}
instance Num Int where
  (<=) = int_le
  (==) = int_eq
  (+) = int_plus
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{code}
instance Num Float where
  (<=) = float_le
  (==) = float_eq
  (+) = float_plus
\end{code}
\end{minipage}
\caption{\lstinline{Num} typeclass and instances}
\label{fig:tc-num}
\end{figure}

To make implicit overloading practical in functional programming languages,
\citet{wadler_polymorphism_1989} proposed a dictionary passing style
mechanism. Typeclasses are the incarnation of the the dictionary passing mechanism for
Haskell. In contrast to parametrically polymorphic functions like
!id!, functions like !(+)! act only on some specific class of constrained or qualified
types\cite{jones_qualified_1994}: the term !False + True! is meaningless, as it does not have a well
defined semantics precisely because Booleans cannot be added
together. To this effect, the addition function (!+!) has the type
!Num t => t -> t -> t!. This means that the function can only be used
on those types !t! which satisfy the !Num t! constraint. This !Num t!
constraint is declared by the programmer as a typeclass language
feature. The instances of the typeclass specify when the typeclass
constraint holds, and what is the intended behavior at that type. For
example, the !Num! typeclass and its instances are shown in the
\pref{fig:tc-num}. In this typeclass setting, the use of the function (!+!), say for
example in the definition of !addF!, the typechecker needs to justify
its type correctness. This amounts to satisfying the constraint !Num Float!.
which in turn can only be satisfied, if the typechecker can find a declaration of the
the instance declaration !Num Float!. To keep the treatment of qualified types
tractable, the special arrow (!=>!) separates the actual type---written on
the right hand side of the arrow---from the constraints---written on the left hand side of the arrow.

Typeclasses can also be viewed as relations over types\cite{morris_simple_2014}
where, every instance declaration extends that relation. The typeclass !Num!
represents a unary relation for those types whose values behave like
numbers, they can be compared (!<=!, !==!) and added together (!+!).
The declarations in the \pref{fig:tc-num} can be interpreted as an
evidence of fact that !Float! and !Int! belong to the unary relation
!Num!, $\{$ !Int! , !Float! $\} \subseteq$ !Num!, and also !Bool $\not\subseteq$ Num!.

\subsection{Type Computation}\label{sec:type-computation}
A generalization of single parameter typeclasses is multi-parameter
typeclasses. It naturally extends the idea of having n-ary relations
over types. For example, following\cite{jones_tcfd_2000}, the
typeclass !Con e c! can be used to give a unified treatment of
different containers and the elements that they contain as shown in
the \pref{fig:tc-collection}. The type variable !e! stands in for the
element type of the container while !c! stands in for the container
type. The behavior of the containers is captured by the methods of the
typeclass !empty!, which returns the empty container and !insert!,
that inserts the element of type !e! in to the container of type !c!.
We can imagine having instances for such a typeclass as
!Con Int [Int]! that says that the container list of integers has
integers as its elements, !Con Word (Tree Chars)! that says that a
container tree of !Char! contains !Word!s as its
elements as they are of fixed lengths. Speaking in terms of
relations, we have $\{$ !(Int, [Int])!, !(Int, Tree Int)! $\}$ $\in$ !Con!.

\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.3\linewidth}
\begin{code}
class Con e c where
  empty :: c
  insert :: e -> c -> c
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{code}
instance Con Int [Int] where
  empty = ...
  insert = ...
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.3\linewidth}
\begin{code}
instance Con Word (Tree Char) where
  empty = ...
  insert = ...
\end{code}
\end{minipage}
\caption[\lstinline{Con} typeclass]{\lstinline{Con} typeclass and its instances}
\label{fig:tc-collection}
\end{figure}

The use of !empty! in a polymorphic setting, however, leads to a type ambiguity during
compilation. For example, in a term !single3 :: [Int] = insert 3 empty!, the typechecker infers
the type of !empty! as !Con e [Int] => [Int]!. The type inferred by
the typechecker is the right one however, we have a free type variable
!e! in the constraint. We do not have enough information to determine what the
concrete type of !e! should. There is no way to rule out an
existence of an instance such as !Con Float [Int]!.
This results in an ambiguity in compilation step; the compiler cannot
resolve the typeclass constraint to a unique instance and hence cannot
choose the right implementation to use. Such types that
give rise to ambiguity are called ambiguous types. Formally, if a
type variable only appears in the constraints of the type it is called
an \emph{ambiguous type}. The type of !empty!,
!FORALL e c. Con e c => c!, is ambiguous due to the occurrence of !e! only in the
constraint. Terms with ambiguous types need to be rejected by the compiler
precisely because they do not have a well defined semantics.
A possible solution to this problem, is to resort to assigning local
type annotations, for example,

\begin{CenteredBox}
\begin{code}
single3 :: [Int] = insert (3 :: Int) (empty :: [Int])
\end{code}
\end{CenteredBox}

However, this solution is unsatisfactory on various fronts: it relies
on the fact that the programmer knows where to add the type
annotations and has deep understanding of how the typechecker uses the
type annotation information and worse, it is fragile, unless each
sub-term is annotated, any changes to the typechecker
algorithm may result in unpredictable type inference causing
previously typecheck-able programs to now fail.
% The problem is that the typeclasses are relations making them too
% general for this use case.

\subsubsection{Functional Dependencies}\
The problem with multi-parameter type classes is that there is a
mismatch between the programmers intention and the expressivity of the
typeclass machinery in the type system. Continuing with the above example, the
programmers intention is to have a functional relation between !c! and
!e! of the typeclass !Con e c!, or that the choice of the type
parameter !c! fixes the choice of the type parameter !e!. In general,
there can be arbitrary functional relation between the type parameters
of the typeclass. In the current setting, all multi-parameter
typeclasses are treated as general n-ary relations. Inspired by the database's
relational algebra, \citet{jones_tcfd_2000} introduced functional
dependencies, a new conservative extension to the typeclass language
feature. It is able to efficiently capture the functional
nature between the typeclass parameters.

\begin{figure}[ht]
\begin{CenteredBox}
\begin{code}
class Con e c | c ~> e where
  empty :: c
  extend :: e -> c -> c
\end{code}
\end{CenteredBox}
\caption[\lstinline{Con} typeclass]{The \lstinline{Con} Typeclass with Functional Dependency}
\label{fig:tc-collection-fd}
\end{figure}

The extra annotation !c ~> e!, as seen in the \pref{fig:tc-collection-fd},
on the typeclass definition captures the functional dependency of the typeclass
!Con e c! and it reads as ``for a given !c!, !e! can be uniquely
determined.'' The typeclass parameter !c! is called the determiner
and the parameter !e! is called the determinant of the
functional dependency. This extension to typeclasses has a good power
to weight ratio; with a minimal change to the typeclass syntax and
complete backwards compatibility with no change to typeclass instance declarations.
Typechecker needs to now verify that the new instance declarations do
not violate the functional dependency: We cannot allow both the typeclass instances,
!Con Int [Int]! and !Con Float [Int]! to exist together.

\subsubsection{Associated Types}
Another way of introducing a functional relation on types, or simply
type functions, would be via associating the determinant of the
functional dependency within the class
definition\cite{chakravarty_associated_2005} in a more direct fashion.
Every multi-parameter typeclass can be normalized into having only a single
parameter and thus completely circumvent the problem of
ambiguous types. Using our previous !Con c e! example, if !c ~> e! then,
another style to capture the programmers intention is to use a special
type function, say !Elem c!, where ever the type parameter
!e! appears. The !Elem! is a special type function that takes in a type
and returns a type and is associated with the typeclass.
This effectively gives an alternative syntax to express functional
dependencies.

\begin{figure}[ht]
\begin{center}
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
class Con c where
  type Elem c
  empty :: c
  insert :: Elem c -> c -> c
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
instance Con [Int] where
  type Elem [Int] = Int
  empty = ...
  insert = ...
\end{code}
\end{minipage}
\end{center}
\caption[Con typeclass]{\lstinline{Con} Typeclass with Type Function}
\label{fig:assoc-types}
\end{figure}

In the \pref{fig:assoc-types}, the typeclass !Con! now has only one
parameter, !c!, for the container type, with an additional field for
the type function !Elem! that depends on the type parameter !c! of the
typeclass. The type signature of !insert!, also now only depends on
the type parameter !c!, and the !Elem c! stands in for the element
type of container. In the instance declaration, the !Elem [Int]! is mapped to
!Int!, meaning, the element of the list of integers is of type !Int!.
This gives an syntactic alternative to write the functional
dependencies in a type function form.


\subsection{User defined Datatypes}
\subsubsection{Algebraic Datatypes}
Aiding organization of related data together is an important feature of any programming language. The structures store information by encoding the domain elements. The transformations, or functions, on these structures then, models the program logic. Algebraic datatypes (ADT) are a primary way to define such new structures in a functional programming language. They are called algebraic because they can be viewed as composite datatypes of named sums of products. As an example, consider modeling a simple calculator application which performs addition operation on integers. The domain, in this case, are algebraic expressions can be encoded using an ADT in Haskell as follows:

\begin{CenteredBox}
\begin{code}
data AlgExp where
   Value :: Int -> AlgExp
   Plus :: (AlgExp, AlgExp) -> AlgExp
\end{code}
\end{CenteredBox}
The !data! keyword defines a new user defined datatype with name !AlgExp!.
The data constructor !Value! stores evaluated !Int! values, while !Plus! encodes the operation of adding the two expressions. Now, the function of the calculator---to compute expressions---is simulated by evaluating the encoded expression. This is performed by an !eval! function defined recursively as shown below:

\begin{CenteredBox}
\begin{code}
eval :: AlgExp -> Int
eval (Value i) = i
eval (Plus (x, y)) = eval x + eval y
\end{code}
\end{CenteredBox}

The declarative style of programming allows us to write the !eval! function on a case by case basis via pattern matching. There are only two ways in which we could have obtained a value of type !AlgExp!. The first case says that if we have a !Value i!, we can deduce that !i! is an !Int! and return it. In the second case, !Plus!, we first evaluate the expressions !x! and !y! recursively, and then return the addition of the two results. Another advantage of having a declarative style is that extending the calculator application, say to include operations such as multiplication and division operation, is straightforward. The code changes required would be to add the associated data constructors, !Mult! and !Div!, in the datatype declaration followed by extending the !eval! function with cases for !Mult!, which multiplies, and !Div! that divides.

In an untyped programming language, a predicate is necessary to check that certain structural invariants hold: the constructor !Plus! should always have two sub expressions. In a typed setting the typechecker can reject programs with ill-structured data: the expression !Plus (Value 3)! is ill-typed. The declarative style of defining algebraic datatypes and pattern matching was introduced in Hope\cite{burstall_proving_1969, burstall_hope_1980} and has become an essential feature of all modern functional programming languages.

\subsubsection{Generalized Algebraic Datatypes}
Consider extending the previously defined !AlgExp! to now be polymorphic over the base types, and also to include a predicate !IsZero!, which checks if the expression is equal to zero.

\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.6\linewidth}
\begin{code}
data AlgExp a where
  Value  :: a                    -> AlgExp a
  Plus   :: AlgExp a -> AlgExp a -> AlgExp a
  IsZero :: AlgExp Int           -> AlgExp a
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
eval :: AlgExp Int -> Int
eval (Value i) = i
eval (Plus x y) = (eval x) + (eval y)
eval (IsZero x) = (eval x) == 0 -- Type error!
\end{code}
\end{minipage}
\caption{\texttt{AlgExp} datatype and \texttt{eval} function}
\label{fig:algexp-eval}
\end{figure}
How should the !eval! function handle the !IsZero! case? The definition, !(eval x) == 0!, fails to type check as the !eval! function requires the return type of the expression to be an !Int! but it is of type !Bool!. Another possible solution is to change the return type of the the !eval! function to instead be !Either Int Bool!. which means that the evaluator either returns an !Int! or a !Bool!. While this would work as expected, it requires a complete rewrite of the !eval! function. The situation of maintaining this !eval! function would become even more cumbersome if later we wanted to add a facility to store and use user defined functions. Extending !AlgExp! with new constructs would mean nesting of !Either! datatype and then checking at each recursive call which value is returned.

All we really wanted was a function that evaluates an expression compositionally. In full generality, the evaluation of the expression may not result in the same type. In our case, the !IsZero! evaluates an expression to a Boolean value, while the !Plus! operator evaluates to an Integer. Thus, the type signature of the !eval! function should abstract over the expression result type: !eval :: FORALL a. AlgExp a -> a!. The current data constructors of !AlgExp! all have a generic return type of !AlgExp a!. The problem would disappear if we were to constrain the return type of each of the data constructors. This constrained return type can be used as a tag to convince the typechecker that !eval! function is indeed type safe.
\TODO{Do you need a type tag here?}
\begin{figure}[ht]
\centering
\begin{minipage}[ht]{0.6\linewidth}
\begin{code}
data GAlgExp a where
  Value  :: a                          -> GAlgExp a
  Plus   :: GAlgExp Int -> GAlgExp Int -> GAlgExp Int
  IsZero :: GAlgExp Int                -> GAlgExp Bool
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
eval :: FORALL a. GAlgExp a -> a
eval (Value i) = i
eval (Plus x y) = (eval x) + (eval y)
eval (IsZero x) = (eval x) == 0 -- Okay!
\end{code}
\end{minipage}%
\caption{\texttt{GAlgExp} datatype and \texttt{eval} function}
\label{fig:galgexp-eval}
\end{figure}

Now in each of the case branches, the type of the right hand side of a pattern match agrees with the type constraint introduced by the pattern match. !Plus x y! is of type !GAlgExp Int! and so is the right hand side of the expression in the case that evaluates the addition. Similarly, the in case branch of !IsZero x!, the type of which is !AlgExp Bool! has a type !Bool! which agrees with the type of its right hand side.

The type constraints which arise due to pattern matching GADTs can further help in identifying dead code\cite{xi_dead_1998,graf_lower_2020}. For example, consider the function !isZero! shown below:

\begin{minipage}[ht]{0.5\linewidth}
\begin{CenteredBox}
\begin{code}
isZero :: GAlgExp Int -> Bool
isZero (Value i) = i == 0
isZero (Plus x y) = isZero x && isZero y
\end{code}
\end{CenteredBox}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
\begin{code}
isZero :: AlgExp Int -> Bool
isZero (Value i) = i == 0
isZero (Plus x y) = isZero x && isZero y
isZero (IsZero x) = error "impossible"
\end{code}
\end{minipage}

The typechecker has enough information to identify that the case !IsZero x! is impossible as it has a type !GAlgExp Bool! while the function only expects an argument of type !GAlgExp Int!. If we were to define the same function for !AlgExp Int!, the function would have to include a bogus case for !IsZero x!.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
data GAlgExp a where
  ...
  Equals :: GAlgExp a -> GAlgExp a -> GAlgExp Bool
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
eval :: GAlgExp a -> a
...
eval (Equals x y) = (eval x) == (eval y) -- Type error!
\end{code}
\end{minipage}

Consider a generalization of !IsZero x! namely, !Equals x y! as shown in the above code block. The intention of !Equals x y! to encode comparing the arguments !x! and !y! and decide if they are equal. The !eval! function case that evaluates !Equals x y! fails to type check. The problem is not that the return type does not match, it is indeed !Bool!, but there is no reason to believe that !eval x! and !eval y! can be compared using the !(==)! operator. The only information we can infer from the pattern match is that the type of !eval x! and !eval y! are of some generic type variable !a!. Such types are called existential as they not appear in the return type. We need to somehow constrain the types of the arguments to !Equal! to only those that can be compared. Fortunately, Haskell already supports such a mechanism via typeclasses. By constraining the type variable to only which satisfy the !Eq! typeclass, we are able to convinced the typechecker that the term !(eval x) == (eval y)! is well typed.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
data GAlgExp a where
...
   Equals :: Eq a => GAlgExp a -> GAlgExp a
                  -> GAlgExp Bool
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
eval :: GAlgExp a -> ac
...
eval (Equals x y) = (eval x) == (eval y) -- Okay!
\end{code}
\end{minipage}

GADTs generalize the notion of algebraic datatypes in two distinct ways:
\begin{enumerate}
\item Phantom or tagged types, where the return type of the data constructor is no longer a generic type variable and it is refined to be a fixed type;
\item existential types, where it is possible for the type variables that appear in the types of the arguments to the data constructors to not appear in the return type.
\end{enumerate}

While there is a rich literature of GADTs (inductive type families) for dependently typed languages\cite{dybjer_inductive_1991, dybjer_inductive_1994}, the idea of GADTs for non-dependently typed languages appeared under different names such as indexed types\cite{zenger_indexed_1997}, first class phantom types\cite{cheney_first-class_2003}, guarded recursive datatypes\cite{xi_guarded_2003}, equality qualified types\cite{sheard_meta-programming_2008}. GHC/Haskell was the only language compiler that supported GADTs\cite{peyton_jones_wobbly_2004} until recently OCaml 4.0\cite{garrigue_gadt_2011}, Scala 3\cite{TODO}, and other languages are playing catch up.

\subsubsection{Generative Abstract Types}\label{subsubsec:gen-abs-types}
Generative types provides a mechanism for the programmer to confine the visibility of the representation details of a type exclusively within a module. External to the module the client program cannot know how the type is represented or in other words, representation of the generative types is opaque to the client module. In Haskell generative types are defined by hiding the type declarations in the signatures. Consider an HTML module defined in Haskell as shown in \pref{fig:html-generative-type}.
\begin{figure}[ht]
 \centering
 \begin{minipage}[ht]{0.4\linewidth}
 \begin{code}
 module HTML (Html, mkHtml, unMkHtml)
 where

 newtype Html = Html String

 mkHtml :: String -> HTML
 mkHtml = Html . escapeString

 unMkHtml :: HTML -> String
 unMkHtml (Html s) = s
 \end{code}
 \end{minipage}%
 \begin{minipage}[ht]{0.4\linewidth}
 \begin{code}
 module Client
 where

 import HTML

 page :: HTML
 page = mkHTML "<html><body>Text</body></html>"

 \end{code}
 \end{minipage}
 \caption{HTML as a generative type}
 \label{fig:html-generative-type}
\end{figure}

Within the scope of !Html! module, the types !String! and !HTML! are synonymous, however the type !Html! will be opaque for any external client using the type !HTML! will not be able to directly manipulate a value of type !Html!---unless of course by using the specific functions exposed in the signature. Inhibiting manipulations and views of the data representation can be useful for avoiding leaking information in secure computation contexts. This (naive) abstraction however comes with a runtime cost. !Html! and !String!, need to be explicitly converted from one form into another using pattern matching. This is explicit conversion is redundant as they have the same representation in memory, but the compiler cannot use this information to perform any optimization on the generated code.

The idea of generative abstract types first appeared in work of \citet{leroy_applicative_1995} and \citet{milner_definition_1997} in context to ML modules systems. Generative abstract types in the context of Haskell are similar to \cite{montagu_modeling_2009}. While they have a hint of existential types, they have different semantics.

\section{\SFC}\label{sec:sfc} %%%%%%%%%
\subsection{History and Motivation}
The Glasgow Haskell Compiler (GHC)\cite{ghc_2020}, is a widely used Haskell\cite{haskell_2010} compiler. The compiler works in three major passes. First, the \emph{parsing phase} that parses the programmer writing text code, also known as the surface syntax, in an appropriate abstract syntax tree representation, the second pass is the \emph{type checking phase} that checks for type correctness the surface syntax, and finally the \emph{compilation phase} that compiles the program into a core language. Haskell is a statically typed functional programming language which has practical and theoretical implications; in practice, only those terms whose types can be verified by the typechecker can be compiled to the core language, and in theory, terms without types have no meanings. This section of the report concentrates on the design and meta-theory of the core language of the GHC compiler.

Implicit parametric\cite{reynolds_user-defined_1978}, and adhoc polymorphism\cite{hall_type_1994} have a rich literature for expressing them in \SF. In this scheme the function types in the elaborated language have explicit type arguments. This means that at the function use site explicit types need to be applied before their value arguments can be passed. For example, the function !id! in \SF will have a type !id :: FORALL a. a -> a! while for the term, !id 3!, where !id! is used, it will be elaborated to !id Int 3!. The typeclasses are converted into a record type called dictionaries. The records contain the methods associated with the type class. The instances of typeclasses are values of this type and contains a reference to the instance of the methods. Elaborating algebraic datatypes in \SF have an even richer literature; they can be expressed by using various encodings schemes such as Church\cite{jansen_efficient_2005,jansen_programming_2013}, Mogensen-Scott\cite{mogensen_efficient_1992}, or Mendler-Parigot encodings\cite{parigot_representation_1990, stump_efficiency_2016}. The data constructors introduced by user defined datatypes are simply constants with the appropriate argument and type specified by the declaration.
% \AI{How should one encode GADTs? Give a simple example by the way of elaboration in System F}

Consider a datatype declaration of AlgExp a from before. The data constructor !Value! has the type !FORALL a. a -> AlgExp a! in the core language, and programmer written term !Value 3! will be elaborated to !Value Int 3!. Notice how the types need to be explicitly applied in the core language. The elaboration procedure needs to find unique values to fill into the placeholder arguments for core level functions. Type checking expressions that contain value constructors of user defined algebraic datatypes is as straightforward as type checking simple term applications as the type of each value constructor can be inferred from the typing context. Now, consider a GADT data constructor !Equals! with the type !Eq a => GAlgExp a -> GAlgExp a -> GAlgExp Bool!. The quantified type variable !a! is existential and needs a special treatment in the typechecker. For example, A function declaration such as !f (Equals x y) = x! needs to be disallowed to avoid the existential type variable to escape its scope. Previous work relied on cunning type inference machinery to establish that such invariants. Further, consider the match statement !eval (IsZero x) = x == x!. To convince the typechecker that this is a type correct expression, a type refinement machinery is necessary. The type of !IsZero x! is !GAlgExp Bool!, and hence it is okay to allow the right hand side of the expression to be of type !Bool!. All previous works\cite{cheney_first-class_2003, xi_guarded_2003, peyton_jones_simple_2006} used ambient equality constraints as extensions to \SF to support GADTs.

Further, consider an associated datatype type !Elem [Int]! from the previous \pref{fig:assoc-types}. To implement such types, there is a need to have a special class of associated types. These are to be used as stand-ins for concrete types. However, due to polymorphism, it is not always possible to erase these types while elaborating it into the core language. Things become more complicated when the associated types appear as arguments to datatypes and the compiler needs to perform extensive rewrite during elaboration into the core language.\TODO{Make the argument about why associated types are a pain more verbose.}

Finally, Consider a generative abstract type declaration given below:

\begin{CenteredBox}
\begin{code}
newtype Fun = MkFun (Fun -> Fun)
\end{code}
\end{CenteredBox}
The intention of this declaration is to make the type !Fun! and !Fun -> Fun! be isomorphic. \SF is parametric which means there is no way to encode this in pure \SF: we cannot inspect the type and check for their equality. It soon became clear that with the addition of GADTs and generative types, and associated types using pure \SF would be too cumbersome, if not impossible to express. Further, the interaction of each of these seamy different features together would be painful for the compiler writer.

\SFC\cite{sulzmann_system_2007} was designed as an alternative to having separate extensions for each language feature. The key insight is that each language feature is in a sense trying to encode some notion of type equality ($\tau\sim\sigma$). GADTs introduce type refinements, as seen in !eval! example, are type equality constraints. Instances of associated types introduce a type equality between the instantiated associated type and the concrete type (!Elem [Int] $\sim$ Int!). The generative abstract type introduces an axiom, instantiations of which are type equalities between the defined type and its representation, for example for !Fun!, (!Fun $\sim$ (Fun -> Fun)!). Thus, by introducing a single construct for type equality within the system, all the features described in \pref{sec:language-features} can be encoded in a straightforward fashion.

In the following sections we describe the  meta-theory of the core language followed by details of how each of the language features GADTs(\pref{sec:fc-encodes-gadts}), generative abstract types(\pref{sec:fc-encodes-newtypes}), and associated types(\pref{sec:fc-encodes-assoctypes}), and also a unique feature of the system, open type functions(\pref{sec:fc-encodes-opentypefun}) is encoded in \SFC with the help of \emph{coercions} which embody the notion of type equality.
% In one statment: \SFC is an intensional intrinsically typed programming language.
% It is intensional, meaning each term encodes its typing derivation, and it is intrinsically typed, meaning all the terms are index by types.

% Some key points to cover:

% Store the equality between types explicitly in the AST during type checking.

% New feature: coercion is a type and its kind tells us what types does the coercion equate.

% Features that can be directly expressed in \SFC: New types or generative types, associated types, functional dependencies, generalized algebraic datatypes (GADTs).

% Brand new feature user defined open type functions.

% Makes type rewriting complicated. But makes the type system more expressive by making it extensible.
% Proving type soundness is a bit more involved now.

% why don't we have a rule that says: if $\sigma_1 \sim \sigma_2$ then

\subsection{Syntax}
\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Type Variables} &\TyVar,\beta,\Co &\qquad\text{Type Constants} &T \\
 \text{Term Variables} &x,y &\qquad\text{Indices} &i,n \in \mathbb{N} \\
 \text{Coercion Vars} &c & &
 \end{syntax}
 \begin{syntax}
 \text{Kinds} &&\kappa \bnfeq& \star \bnfor \kappa \to \kappa \bnfor \shl{\sigma \sim \tau}\\
 \text{Types} &&\tau,\sigma \bnfeq& \TyVar \bnfor T \bnfor \tau \to \tau \bnfor \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor \shl{F\App\many\alpha} \bnfor \shl{\Co}\\
 \text{Coercions} &&\nu,\Co \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\nu\Co % equiv relation
 \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
 \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co\\  % compose/decompose
 \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
 \text{Patterns} &&P \bnfeq& H\App \many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \shl{\Cast \Tm \Co}\\

 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv,F\co\tau \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Substitutions} &&\Subst \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
 \end{syntax}

 \begin{syntax}
 \text{Program} &&P_{gm} \bnfeq& \many{D_{cl}} \mathrel{;} \many{x = \Tm}\\
 \text{Data Declarations} &&D_{cl} \bnfeq& \textbf{\texttt{data }}\App T\co\many{\kappa} \to \star\App \textbf{\texttt{ where }}\App \many{C_{trs}(T)} \\
 && \bnfor& \textbf{\texttt{type }}\App F : \many\kappa \to \kappa\\
 && \bnfor& \textbf{\texttt{axiom }}\App C\App \many{\TyVar\co\kappa} : \sigma_1 \sim \sigma_2\\
 \text{Data Constructors} &&C_{trs}(T) \bnfeq& H : \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \many\sigma \to T\many\TyVar}\\
 \end{syntax}

 \caption{The Syntax of \SFC}
 \label{fig:sfc-syntax}
\end{figure}

The complete syntax of \SFC is shown in \pref{fig:sfc-syntax}. The system is classified into three sub-languages: kinds, types, and terms, which are pairwise interdependent. The kind language sits at the highest abstraction level. It consists of base kind ($\STAR$), higher kinds ($\kappa \to \kappa$), and the novel construct for expressing type equality ($\shl{\tau \sim \sigma}$). Kinds classify types. The type language contains type variables ($\alpha$), type constructors or constants $T$, which ranges over built-in types such as !Int!, !(->)! and other user defined types, and polytypes ($\Forall \alpha \tau)$ for supporting polymorphic functions. Function types ($\tau \to \sigma$), can very well be considered as a type constructor $T$, but we give it a special status for the sake of presentation. The absence of type level lambdas make the type level calculus less expressive by disallowing certain types such as $\Lam a {T\App a\App Int}$. This is a cautious decision that ensures type checking remains tractable and efficient. To make up for the lack of type level lambdas, the system allows higher kinded types. This allows programmers to define inductive algebraic datatypes such as !List!s and !Tree!s. \SFC is impredicative; there is no stratification between polytypes and monotypes. Which means that fully saturated monotypes and polytypes have kind $\STAR$.

The type equality coercion, $\shl{\Co}$, is the novel construct at type level which is classified by kind level type equality predicates, $\tau\sim\sigma$. Coercions are first class at the level of types: they can be constructed, applied to, and passed as arguments, and also abstracted over by using the special coercion infrastructure at the level of types. \SFC supports a full fledged coercion calculus where each syntactic construct corresponds to a logical equation between two types. The simplest coercion can be constructed by using the reflexivity construct $\refl\tau$. It says that the type $\tau$ witness the (obvious) fact that it is equal to itself. The two other simple constructs on coercions are symmetry '$\Sym\Co$', which flips the direction of equality, transitivity '$\trans {\Co_1}{\Co_2}$', which composes two coercions. Reflexivity, symmetry and transitivity together makes type equality an equivalence class in the system. Coercions can also be composed and decomposed using the '$\Co_1\Co_2$', and '$\Left\Co$' and '$\Right\Co$' constructs respectively. Finally, coercion abstraction '$\Forall {\TyVar\co\kappa}\Co$' and coercion application '$\Co\At\tau$' aids equality reasoning on polytypes. It is important point to remember is that \pref{fig:sfc-syntax}, places coercions and types in two different rows for a clear presentation, in principal, \emph{coercions are types} and we will use $\phi$ to mean either. Coercions can also be introduced as equality axioms. This can be thought as a templatized equation. For, example and axiom $\Forall \alpha {F\App a \sim a}$ says that for all types, !F a! can be treated exactly as !a!. We distinguish user defined datatypes $T$ from type function constructors $F$. Only the saturated application of $F$ can appear on the left hand side of a coercion axiom head.

Finally, the types classify the terms of the system. It has the usual constructs: variables ($x$), term abstractions ($\Lam x \Tm$) and term applications ($M \App N$), type level abstraction ($\TLam \alpha \Tm$) and type applications ($\Tm\App\sigma$). Declaration of algebraic datatypes introduces data constructors $H$, the types of which are of the form:
$$
H \co \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa}} \many{\sigma} \to T \many\TyVar}
$$
Here the type variables, $\many\TyVar$, appear in the same order as in the algebraic datatype declarations, and type variables $\many\beta$ are the existential; they do not appear in the return type. This characterization of splitting the type variables into $\many\alpha$ and $\many\beta$ plays a crucial role in encoding GADTs, which we will describe in more detail in the next \pref{sec:fc-encodes-gadts}. The $\Case M {\overline{P \to N}}$ discriminates on the shape of the term $M$ and chooses one of the alternatives $N$ after a successful match on the one of the patterns $P$. The novel term construct of the system is $\shl{\Cast {M} \Co}$. It says that if a term $M$ has a type $\tau$ and we have a type coercion, $\Co$, which says that $\tau\sim\sigma$, then the term, $\Cast M \Co$, justifies treating $M$ as if it has type $\sigma$. For type theory enthusiasts, type coercions should be thought of as extensional type equality. An alternative intuitive characterization of type equality within the realm of \SFC can be thought as if $\Co$ says $\tau \sim \sigma$ then, within a context that expects a term of type $\sigma$, we can use the term, $\Cast M \Co$ without worrying that the term will get stuck or crash when we run the program.  This soundness argument of the system, however, the does not rely on any specific semantic notion of type equality.% The soundness property of the type system guarantees that after the types (including casts) have been erased, the program will not crash or get stuck. Unlike other regular types, however, coercions do not classify values, i.e. there are no term level constructs that have a type $\tau\sim\sigma$.

\subsubsection{Notations}\label{sec:notations}
Before we deep dive into the system formalization any further, we need some helper notations for the sake of presentation. The type environment $\TEnv$ contains type mapping for free term variables ($x\co\tau$), kind mapping for free type variables ($\alpha\co\kappa$), along with the term constants ($H\co\tau$), type function constants ($F\co \tau$), and coercion axioms ($coAx\co(\sigma\sim\tau)$). We will use a convention of naming coercion axioms with a $co$ prefix to distinguish them from non-coercion types. The freshness condition is denoted by $\fresh \alpha \TEnv$: $\alpha$, or that $\alpha$ does not appear in the domain of $\TEnv$, or $(\TyVar\co\kappa) \not\in \TEnv$. Substitutions, $\Subst$, are mappings from term variables to types. By abuse of notation, we will also use it to denote type variables to their kind mappings. Due to naming conventions, we need not worry about (mistakenly) substituting a term into a type. We will use the type constant $(\to)$ and kind constant $(\sim)$ in its infix form rather than in a prefix form: $\tau \to \sigma$ and not $(\to)\App\tau\App\sigma$.

A program, $P_{gm}$, is a list of programmer written type declarations, $D_{cl}$, followed by term bindings, $x = M$. Each $D_{cl}$ introduces a new user defined datatype, a type function, or an axiom. We assume the convention that the declarations are correctly ordered, i.e. defined or at least declared before their use. We use !$\Tm$::$\tau$! (stress on double colons) to assign (or check) type $\tau$ for actual program expression $\Tm$, and $\Tm \co \tau$ (stress on single colon) to mean $\Tm$ has type $\tau$ in the formalization.

\subsection{Static Semantics}\label{sec:sfc-static-sem}

\newcommand\KReflCo{
 \ib{\irule[\trule{co-refl}]
 {\TyKinding \TEnv \tau \kappa};
 {\CoKinding \TEnv {\refl \tau} {\tau \sim \tau}}
 }
}

\newcommand\KSymCo{
 \ib{\irule[\trule{co-sym}]
 {\CoKinding \TEnv \Co {\tau \sim \sigma}};
 {\CoKinding \TEnv {\Sym \Co} {\sigma \sim \tau}}
 }
}

\newcommand\KTransCo{
 \ib{\irule[\trule{co-trans}]
 {\CoKinding\TEnv {\Co_1} {\tau \sim \tau_2}}
 {\CoKinding\TEnv {\Co_2} {\tau_2 \sim \sigma}};
 {\CoKinding\TEnv {\trans {\Co_1} \Co_2} {\tau \sim \sigma}}
 }
}

\newcommand\KInstCo{
 \ib{\irule[\trule{co-$\E\forall$}]
 {\CoKinding\TEnv \Co {\Forall\TyVar\tau_1 \sim \Forall\beta\tau_2}}
 {\Subst_1 = \Sub\TyVar\sigma}{\Subst_2 = \Sub\beta\sigma};
 {\CoKinding\TEnv {\Co\At\sigma} {\Subst_1\tau_1 \sim \Subst_2\tau_2}}
 }
}

\newcommand\KForallCo{
 \ib{\irule[\trule{co-$\I\forall$}]
 {\CoKinding {\TEnv,\TyVar\co\kappa} \Co {\tau_1 \sim \tau_2}}{\alpha\#\TEnv};
 {\CoKinding \TEnv {\Forall {\TyVar\co\kappa} \Co} {\Forall {\TyVar\co\kappa}\tau_1 \sim \Forall {\TyVar\co\kappa}\tau_2}}
 }
}

\newcommand\KCoComp{
 \ib{\irule[\trule{co-comp}]
 {\CoKinding \TEnv {\Co_1} {\tau_1 \sim \tau_2}}
 {\CoKinding \TEnv {\Co_2} {\sigma_1 \sim \sigma_2}}
 {\TyKinding \TEnv {\tau_i\App \sigma_i} \kappa};
 {\CoKinding \TEnv {\Co_1\App \Co_2} {\tau_1 \sigma_1 \sim \tau_2 \sigma_2}}
 }
}

\newcommand\KLeftCo{
 \ib{\irule[\trule{co-left}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Left \Co} {\tau_1 \sim \tau_2}}
 }
}

\newcommand\KRightCo{
 \ib{\irule[\trule{co-right}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \sim \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Right \Co} {\sigma_1 \sim \sigma_2}}
 }
}

\newcommand\KCastCo{
 \ib{\irule[\trule{co-leftc}]
 {\CoKinding \TEnv \Co {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
 {\CoKinding \TEnv {\Cast {\Co_1} \Co_2} {\tau_1 \sim \tau_2}}
 }
}

\newcommand\KCoAx{
 \ib{\irule[\trule{co-ax}]
 {\CoKinding \TEnv \Co {\kappa_1 \then \tau_1 \sim \kappa_2 \then \tau_2}};
 {\CoKinding \TEnv {\Cast {\Co_1} \Co_2} {\tau_1 \sim \tau_2}}
 }
}

\newcommand{\KTyVar}{
 \ib{\irule[\trule{ty-var}]
 {\TyVar\co\kappa \in \TEnv};
 {\TyKinding \TEnv \TyVar \kappa}
 }
}
\newcommand{\KTyApp}{
 \ib{\irule[\trule{ty-app}]
 {\TyKinding \TEnv \sigma {\kappa' \to \kappa}}
 {\TyKinding \TEnv \tau \kappa'};
 {\TyKinding \TEnv {\sigma\App\tau} \kappa}
 }
}
\newcommand{\KFCon}{
 \ib{\irule[\trule{ty-fcon}]
 {F \co \many \kappa^n \to \kappa' \in \TEnv}
 {\many {\TyKinding \TEnv {\sigma} {\kappa}}^n};
 {\TyKinding \TEnv {F \many\sigma^n} {\kappa'}}
 }
}
\newcommand{\KTyCon}{
 \ib{\irule[\trule{ty-con}]
 {T \co \kappa \in \TEnv};
 {\TyKinding \TEnv {T} {\kappa}}
 }
}
\newcommand{\KTyAll}{
 \ib{\irule[\trule{ty-all}]
 {\TyKinding {\TEnv,\TyVar\co\kappa} {\sigma} \star}
 {\fresh \TyVar \TEnv};
 {\TyKinding \TEnv {\Forall {\TyVar\co\kappa} \sigma} \star}
 }
}

\begin{figure}[ht]
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \Co \kappa$}\\
 \KReflCo \rsp \KSymCo \rsp \KTransCo \\
 \KForallCo \rsp \KInstCo \\
 \KCoComp \\
 \KLeftCo \rsp \KRightCo \\
 \end{gather*}
 \caption{Coercion Typing: Excerpt of Static Semantics of \SFC}
 \label{fig:sfc-typing-co}
\end{figure}

To formalize our intuitions of how the coercions ought to behave, we provide static semantics in the form of declarative style typing rules in \pref{fig:sfc-typing-co}. All coercions are types such that their kinds tell us which two types can be considered equal. We read the kinding judgment $\CoKinding \Gamma \Co \kappa$ to say, under the assumptions in $\Gamma$, $\Co$ has kind $\kappa$. The kinding rules \trule{co-refl}, \trule{co-sym} and \trule{co-trans} makes coercion an equivalence relation. The rules \trule{co-$\E\forall$} and \trule{co-$\I\forall$} justifies coercions between polytypes and their instantiations. If two polytypes are equal, then their instantiations with equal types are also equal \trule{co-$\E\forall$}, and similarly if two types, with a free type variable, are equal then type abstraction on both the types yields equal polytypes \trule{co-$\I\forall$}. The rule \trule{co-comp} enables lifting coercions for higher kinded types and reason equalities between them.

As an example of why coercion composition is useful, consider a higher kinded algebraic type !Tree! and a coercion $\Co\co\sigma_1\sim\sigma_2$, then using $\tau_1$ and $\tau_2$ to be equal to !Tree!, we have that $\refl{\texttt{Tree}} \App\Co : \texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_2$. On the other hand, if we have a coercion $\texttt{Tree}\App\sigma_1 \sim \texttt{Tree}\App\sigma_1$ then we can recover the coercion components using the \trule{co-left} and \trule{co-right}, for the higher kinded type and its arguments respectively. In full generality, we can state the lifting property formally in \SFC using \pref{thm:sfc-coercion-lifting}. It captures the intuitive idea that if we have two equal types, substituting it with two equal sub-parts in them maintains the type equality.
\begin{theorem}[Coercion Lifting]\label{thm:sfc-coercion-lifting}
 If $\TyKinding {\TEnv,\TyVar\co\kappa'} \phi \kappa$, where $\TyVar$ is free in $\phi$
 and does not appear free in $\TEnv$,
 $\CoKinding \TEnv \Co {\sigma_1\sim\sigma_2}$, and $\TyKinding \TEnv {\sigma_i} \kappa'$
 then, $\CoKinding \TEnv {\Set{\TyVar\mapsto \Co}\refl\phi} {\Set{\TyVar\mapsto\sigma_1}\phi \sim \Set{\TyVar\mapsto\sigma_2}\phi}$
\end{theorem}
\begin{proof}[Proof Sketch of \pref{thm:sfc-coercion-lifting}]
 Proof is by induction on the derivation of the well kinded type $\phi$. In each of the four cases, whenever in the original derivation the rule \trule{co-refl} was used, it is replaced by the derivation of $\CoKinding \TEnv \Co {\sigma_1 \sim \sigma_2}$ % TODO: I am not convinced some how
  % We have 4 cases:
  % \begin{itemize}
  % \item[\trule{ty-var}] Either the type variable is $\TyVar$ and we are done, or it is not and we use \trule{co-refl}.
  % \item[\trule{ty-app}] By induction hypothesis and using \trule{co-comp}.
  % \item[\trule{ty-fcon}]
  % \item[\trule{ty-all}]
  % \end{itemize}
  % we have a derivation $\Typing {\TEnv, \TyVar\co\kappa'} \phi \kappa$. Then using \trule{co-refl} we obtain $\Typing {\TEnv, \TyVar\co\kappa} {\refl\phi} {\phi \sim \phi}$.
\end{proof}

\begin{figure}[ht]
\begin{gather*}
 \fbox{$\TyKinding \TEnv \tau \kappa$}\\
 \KTyVar \rsp \KTyApp \\
 \KTyCon \rsp \KFCon \rsp \KTyAll
\end{gather*}
 \caption{Kinding Judgments: Excerpt of Static Semantics of \SFC}
 \label{fig:sfc-typing-ki}
\end{figure}

The kinding judgment for types, \fbox{$\TyKinding \TEnv \tau \kappa$}, other than coercions, are listed in \pref{fig:sfc-typing-ty} are fairly standard in comparison to \SF. The judgment $\trule{ty-app}$ enables kinding applications of higher kinded types like !List Int! or !Tree b!. We note that the kinding judgment for $\trule{ty-fcon}$ and $\trule{ty-con}$ are distinct as only fully saturated type function constructors are valid types in the system. The impredicative nature of \SFC is evident from $\trule{ty-all}$.

\newcommand\TVar{
 \ib{\irule[\trule{var}]
 {x\co\tau \in \TEnv};
 {\Typing \TEnv x \tau}
 }
}

\newcommand\TAbs{
 \ib{\irule[\trule{\I\to}]
 {\Typing {\TEnv,x\co\sigma} {M} {\tau}};
 {\Typing \TEnv {\Lam x M} {\sigma \to \tau}}
 }
}
\newcommand\TApp{
 \ib{\irule[\trule{\E\to}]
 {\Typing \TEnv \Tm {\sigma \to \tau}}
 {\Typing \TEnv N \sigma};
 {\Typing \TEnv {\Tm \App N} {\tau}}
 }
}
\newcommand\TTyApp{
 \ib{\irule[\trule{\E\forall}]
 {\Typing  \TEnv \Tm {\Forall {\alpha\co\kappa} \tau}}
 {\Kinding \TEnv \sigma \kappa};
 {\Typing  \TEnv {M\App\sigma} {\tau}}
 }
}

\newcommand\TTyAbs{
 \ib{\irule[\trule{\I\forall}]
   {\Typing {\TEnv,\alpha\co\kappa} \Tm \tau}
   {\alpha\#\TEnv};
   {\Typing \TEnv {\Forall {\alpha\co\kappa} \Tm} {\tau}}
 }
}

\newcommand\TAlt{
 \ib{\irule[\trule{alt}]
 {H\co{\Forall{\many{\alpha\co\kappa}}{\Forall{\many{\beta\co\iota}}{\many\sigma \to T\many\alpha}}}\in{\TEnv}}
 {\Subst = \Set{\many{\alpha \mapsto \tau'}}}
 {\Typing {\TEnv, \many{\beta\co\Subst\iota}, \many{x\co\Subst\sigma}} {N} {\tau} };
 {\Typing \TEnv {H\App\many{\beta\co\Subst\kappa}\App\many{x\co\Subst\sigma} \to N} {T\many{\tau'} \to \tau}}
 }
}

\newcommand\TCast{
 \ib{\irule[\trule{cast}]
 {\Typing \TEnv {\Tm} {\tau}}
 {\CoKinding \TEnv \Co {\tau \sim \sigma}};
 {\Typing \TEnv {\Cast \Tm \Co} {\sigma}}
 }
}
\newcommand\TCase{
 \ib{\irule[\trule{case}]
 {\Typing \TEnv {\Tm} {\sigma}}
 {\many{\Typing \TEnv {P \to N} {\sigma \to \tau}}};
 {\Typing \TEnv {\Case \Tm {\many{P \to N}}} {\tau}}
 }
}

\begin{figure}[ht]
\begin{gather*}
  \fbox{$\Typing \TEnv M \tau$}\\
  \TVar   \rsp \TAbs \rsp \TApp\\
  \TTyAbs \rsp \TTyApp \\
  \TCast  \rsp \TCase \\
  \TAlt
\end{gather*}

 \caption{Typing Judgments: Excerpt of Static Semantics of \SFC}
 \label{fig:sfc-typing-ty}
\end{figure}

A typing judgments for terms, \fbox{$\Typing \TEnv \Tm \tau$}, is shown in \pref{fig:sfc-typing-ty}. The judgments inherited from \SF are $\trule{var}$, $\trule{\I\to}$, $\trule{\E\to}$, $\trule{\I\forall}$, and $\trule{\E\forall}$. The novel judgment \trule{cast} transforms a term $M\co\tau$ to a term $M\co\sigma$ with a witness coercion $\Co\co\tau\sim\sigma$. The two typing judgments worth discussing are that for typing case statements $\trule{ty-case}$ and typing alternatives $\trule{ty-alt}$. A case statement can only be well typed if the discriminant has a type $\sigma$ and each of the alternatives $P \to N$ have the same type $\sigma \to \tau$. For alternatives, if the pattern is a data constructor, only the existential type variables $\many\beta$ are brought into scope explicitly in the pattern. \TODO{FIXME: \trule{alt} scope escape check is it necessary?}
For example, consider the data constructor !IsEquals! and its type with explicit kind annotations from the previous section.
\begin{CenteredBox}
\begin{code}
IsEquals :: FORALL (a :: *). FORALL (b :: *). DEq b -> b -> b -> GAlgExp a
\end{code}
\end{CenteredBox}

Inside a case term the alternative would have the form

\begin{CenteredBox}
\begin{code}
(IsEquals (b :: *). (d :: DEq b) (x :: b) (y :: b)) -> N
\end{code}
\end{CenteredBox}

where $N$ is the resultant if the discriminant matches the pattern !IsEquals!.
This transformation is sound as the type of !IsEquals! is isomorphic to

\begin{CenteredBox}
\begin{code}
FORALL (a :: *). (EXISTS (b :: *). (DEq b, b, b)) -> GAlgExp a
\end{code}
\end{CenteredBox}
% How does case and alternative may encode GADTs
% example of how \exists is just \forall

\subsection{Operational Semantics}\label{sec:sfc-op-sem}
\newcommand{\Beta}{
 \ib{\irule[\trule{$\beta$}]
 {};
 {$\stepsto {(\Lam {x\co\tau} M) \App N} {\Set{x\mapsto N}M}$}
 }
}
\newcommand{\TBeta}{
 \ib{\irule[\trule{Ty-$\beta$}]
 {};
 {$\stepsto {(\TLam \TyVar M) \App \tau} {\Set{\TyVar\mapsto \tau}M}$}
 }
}
\newcommand{\CaseE}{
 \ib{\irule[\trule{case}]
 {};
 {\stepsto {\Case {(K \many\sigma\many\phi\many\Tm)} {\Set{...; K\App\many\beta\App\many x \to N; ...}}} {\Set{\many {\beta\mapsto\phi}, \many{x\mapsto\Tm}}N}}
 }
}
\newcommand{\CoTransE}{
 \ib{\irule[\trule{Co-Trans}]
 {};
 {$\stepsto {\Cast {(\Cast \Val \Co)} {\nu}} {\Cast \Val {(\trans{\Co} {\nu})}}$}
 }
}

\newcommand{\TyPush}{
 \ib{\irule[\trule{ty-push}];
    % {\Co : {\Forall {c\co\kappa} \tau} \sim \Forall {c\co\kappa} \tau'};
 {$\stepsto {(\Cast{\TLam {\TyVar\co\kappa} M}\Co)\App \tau} {({\TLam {\TyVar\co\kappa} (\Cast M {\Co\At\TyVar})})\App \tau}$}
 }
}

\newcommand{\CoPush}{
 \ib{\irule[\trule{co-push}]
 {\substack {\mathlarger{\nu\co \sigma_1' \sim \sigma_2'}\\
 \mathlarger{\Co_1 : \sigma_1 \sim \sigma_1' = \Left {(\Left \Co)}}}}
 {\substack {\mathlarger{\Co\co (\sigma_1 \sim \sigma_2 \then \sigma_3) \sim (\sigma_1' \sim \sigma_2' \then \sigma_3')}\\
 \mathlarger{{\Co_2: \sigma_2 \sim \sigma_2' = \Right{(\Left\Co)}\quad{\Co_3:\sigma_3\sim\sigma_3' = \Right\Co}}}}};
 {$\stepsto {(\Cast{\TLam {\TyVar\co(\sigma_1\sim\sigma_2)} M}\Co)\App \nu} {\Cast {(\TLam {\TyVar\co(\sigma_1\sim\sigma_2)} M)\App (\Co_1 \circ \nu \circ \Sym \Co_2)} {\Co_3}} $}
 }
}

\newcommand{\Push}{
 \ib{\irule[\trule{push}]
 {\Co : \tau_1 \to \tau_2 \sim \tau_1' \to \tau_2'}
 {\Co_1 = \Right (\Left \Co)}
 {\Co_2 = \Right \Co};
 {$\stepsto {({\Cast {\Lam x M} {\Co}}) \App N} {\Cast {(({\Lam x M})\App {(\Cast N {\Sym \Co_1})})} \Co_2}$}
 }
}

\newcommand{\HPush}{
 \ib{\irule[\trule{h-push}]
 {\substack{\mathlarger{\Co : T\App\many\sigma \sim T\App\many\tau}\\
 \mathlarger{H : \Forall {\many{\TyVar\co\kappa}} {\Forall {\many{\beta\co\kappa'}} \rho \to T\App\many\TyVar^n}}}}
 {\substack{\mathlarger{{\Subst = \Set{\many{\TyVar_i \mapsto \Co_i}, \many{\beta_i \mapsto \phi_i}}}}\\
 \mathlarger{\Tm'_i = \Cast {\Tm_i} \Subst\rho_i}}}
 {\Co_i = \Right (\Left^{i-1}\Co) }
 {\phi' =
 \begin{cases}
 \Cast {\phi_i} \Subst(v_1 \sim v_2) &\text{if }\beta_i:v_1 \sim v_2\\
 \phi_i\quad &\text{otherwise}
 \end{cases}
 };
 {$\stepsto {\Case {(\Cast {H\App \many\sigma\App\many\phi\App\many\Tm} \Co)} {\many{\Ptrns \to N}} }
 {\Case {(H\App \many\tau\App\many{\phi'}\App\many{\Tm'})} {\many{\Ptrns \to N}} }$}
 }
}

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Value Types} && T\Val &::= T \bnfor \tau \to \tau \bnfor \Forall {\TyVar\co\kappa}\\
 \text{Plain Values} && \Val &::= H \bnfor \Lam {x\co\tau} M \bnfor \TLam {\TyVar\co\kappa} M \\
 \text{CValues} && C\Val &::= \Val \bnfor \Cast \Val \Co\\

 \text{Evaluation Contexts} && \EvalCtxt &::= \EvalCtxtHole{-} \bnfor \EvalCtxt\App M \bnfor \EvalCtxt \tau \bnfor \Cast \EvalCtxt \Co \bnfor \Case \EvalCtxt {\many{P}}\\
 \end{syntax}
 \begin{gather*}
 \fbox{$\stepsto M N$}\\
 \Beta \rsp \TBeta\\
 \CaseE \rsp \CoTransE\\
 \Push \rsp \TyPush\\
 \CoPush\\
 \HPush
 \end{gather*}
 \caption{Operational Semantics of \SFC}
 \label{fig:op-sem-sfc}
\end{figure}

The values terms are fall into two categories: normal values ($\Val$) and cvalues ($C\Val$) that are values with coercion casts respectively. CValues are needed to maintain type preservation, which would otherwise break in the presence of casts. The cvalues can step in one of the four ways \trule{push}, \trule{ty-push}, \trule{co-push} or \trule{h-push}. In \trule{push}, the coercion $\Co$, applied to the lambda term, is split so that it is applied to the argument ($\Co_1$) and to the redux reduction ($\Co_2$). Applying this rule exposes a \trule{$\beta$} redux. The rule \trule{ty-push} for type application that moves the coercion inside a type abstraction instantiated at the type variable. The rule \trule{co-push} is just like \trule{push} but for moving coercions.

The most complex rule is \trule{h-push} which makes more sense with an example in hand. Consider the case scrutinee $\Cast {(Cons\App \texttt{Int}\App x\App y)} \Co$ where, !Cons : FORALL a. a -> [a] -> [a]!, !$\Co$ : [Int] \sim [T Bool]! and !T! is a type constructor. The cast transforms the scrutinee into a type ![T Bool]! by pushing the coercion into its sub-components.
$$
\stepsto {\Cast {(\texttt{Cons}\App \texttt{Int}\App \texttt{x}\App \texttt{y})} \Co} {\texttt{Cons} \App (\texttt{T}\App \texttt{Bool}) (\Cast {\texttt{x}} \Right\Co) (\Cast {\texttt{y}} {(\refl{[]}\Right\Co)})}
$$
Coercion lifting plays an important role here to make sure that the term sub-components $\Cast M_i {\Subst}\rho_i$ is of the appropriate type. Each rule is derived in a systematic way by making sure the type of the term does not change after moving the cast. This coercion operation calculus of course is not needed during runtime. The system maintains a strict phase distinction and the coercions are erased after type checking. They are however, necessary to prove the important meta-theoretic property of the calculus.

\subsubsection{Soundness}
So do the static semantics effectively weed out all the programs that may fail at runtime. One way to show soundness for a system is by proving subject reduction property.

\begin{prop}[Progress and Subject Reduction]\label{prop:sfc-ty-safety}
 If $\Typing \TEnv \Tm \tau$ then, either $\Tm$ is a cvalue or, $\stepsto \Tm \Tm'$ and
 $\Typing \TEnv {\Tm'} \tau$
\end{prop}

\subsubsection{Consistency}
We want to ensure that we can never derive anything obviously wrong such as !coBAD : Int ~ Bool! in our system.
!coBAD! has the power to cast a term of a type !Int! into !Bool!. If such unsound equalities were to be allowed, either by derivation or specified by the programmer, any term that has a type !Int! could be casted into a term !Bool!. This would mean we would be able to escape the protection provided by the typechecker.
If our system has a top level axiom such as $coBAD \co \texttt{Int} \sim \texttt{Bool}$, this coercion can be used in a cast, to convert an ill-typed term to a well typed term thus breaking subject reduction property. The theorem needs to be strengthened using an appropriate restriction on $\TEnv$. In general, checking consistency at top level is an undecidable. There exists is a conservative approximation of consistency which can be checked syntactically and it is sufficient for this purpose.

\begin{definition}[\Good $\TEnv$]
 A type environment, $\TEnv$, is \Good $\TEnv$ when it satisfies the following properties:
 \begin{itemize}
 \item If $\CoKinding \TEnv \Co {T \many\sigma \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $T\App\many\sigma'$
 \item If $\CoKinding \TEnv \Co {(\sigma' \to \sigma) \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $\tau' \to \tau''$
 \item If $\CoKinding \TEnv \Co {\Forall {\TyVar\co\kappa} \sigma \sim \tau}$ and $\tau$ is a value type, then $\tau$ is of the form $\Forall {\TyVar\co\kappa} \sigma'$
 \end{itemize}
\end{definition}

\begin{theorem}[Progress and Subject Reduction]\label{thm:progress-sfc}
 If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ then, either $\Tm \in C\Val$ or, $\stepsto \Tm \Tm'$ and
 $\Typing \TEnv {\Tm'} \tau$
\end{theorem}

The above consistency criteria is the sufficient to guarantee soundness. It is designed with modularity in mind: an extension to the system, mandates re-characterization of the consistency criteria as well. The burden of defining what consistency should look like depends on the language features the language supports, which is an obligation on the language designer. We will see in detail how the consistency naturally falls out in the following section when we try to encode language different language features in to \SFC. \pref{thm:progress-sfc} also establishes the phase distinction property\cite{harper_higher-order_1989} where where types no not interfere with the execution of the program. Thus if we erase all the type and kind annotations from a well typed term, we can be certain that the term does not crash at runtime. This gives us a way to represent programs efficiently for the purposes of execution once we typecheck them.

\section{Encoding Language Features in \SFC}\label{sec:sfc-encoding-features}%%%%%%%%%
\subsection{GADTs}\label{sec:fc-encodes-gadts}
We now formalize the intuition of how GADTs can be encoded into \SFC using a type directed elaboration. Although \SFC does not distinguish between monotypes and polytypes, the surface level syntax does. This is due to pragmatic reasons, allowing a higher ranked type scheme makes the type inference non-trivial\cite{jones_practical_2007}.

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Constraints} && C \bnfeq& \empt \bnfor C, c\co\tau\sim\tau'\\
 \text{Polytypes} && \pi \bnfeq& \eta \bnfor \Forall\TyVar.\pi\\
 \text{Constrained Types} && \eta \bnfeq& \tau \bnfor C \then \eta\\
 \text{Monotypes} && v,\tau \bnfeq& \TyVar \bnfor \tau\to\tau \bnfor T\App\many\tau
 \end{syntax}
 \caption{GADT Surface level type syntax}
 \label{fig:gadt-type-syntax}
\end{figure}

\newcommand\GADTVar{
 \ib{\irule[\trule{g-var}]
 {x\co\pi \in \TEnv};
 {\GTranslate C \TEnv x {\pi} x}
 }
}
\newcommand\GADTEq{
 \ib{\irule[\trule{g-eq}]
 {\GTranslate C \TEnv \Tm \tau \Tm'}
 {\CoKinding C \Co {\tau \sim \tau'}};
 {\GTranslate C \TEnv \Tm {\tau'} {\Cast {\Tm'} \Co}}
 }
}
\newcommand\GADTForallI{
 \ib{\irule[\trule{g-$\I\forall$}]
 {\GTranslate C \TEnv \Tm \pi \Tm'}
 {\fresh \TyVar {C, \TEnv}};
 {\GTranslate C \TEnv \Tm {\Forall {\TyVar\co\star} \pi} {\TLam {\TyVar\co\star} \Tm'}}
 }
}
\newcommand\GADTForallE{
 \ib{\irule[\trule{g-$\E\forall$}]
 {\GTranslate C \TEnv \Tm {\Forall {\TyVar\co\star} \pi} \Tm'};
 {\GTranslate C \TEnv \Tm {\Set{\TyVar\mapsto\tau}\pi} {\Tm'\App \tau}}
 }
}
\newcommand\GADTCI{
 \ib{\irule[\trule{g-$\I C$}]
 {\GTranslate {C,c:\tau\sim\tau'} \TEnv \Tm {\eta} \Tm'};
 {\GTranslate C \TEnv \Tm {\tau\sim\tau'\then\eta} {\TLam {(c\co\tau\sim\tau')} \Tm'}}
 }
}
\newcommand\GADTCE{
 \ib{\irule[\trule{g-$\E C$}]
 {\GTranslate {C} \TEnv \Tm {\tau\sim\tau'\then\eta} \Tm'}
 {\CoKinding C \Co \tau\sim\tau'};
 {\GTranslate C \TEnv \Tm {\eta} {\Tm'\App\Co}}
 }
}
\newcommand\GADTAlt{
 \ib{\irule[\trule{g-alt}]
 {\substack{
 \mathlarger{H\co \Forall {\many\TyVar} {\Forall {\many\beta} {\many{\tau'\sim\tau''} \then \many\tau \to T\many\TyVar}}}\quad
 \mathlarger{\many\TyVar \cap \many\beta = \varnothing}\quad
 \mathlarger{\fvs{\many\tau, \many{\tau'}, \many{\tau''}} = \fvs{\many\TyVar, \many\beta}}\quad
 \mathlarger{\Subst = \Set{\many{\TyVar\mapsto v}}}\quad
 \mathlarger{\fresh {\many{c}} {C, \TEnv}}\\
 \mathlarger{\GTranslate {C,\many{c\co\Subst{\tau'}\sim\Subst\tau''}\,} {\,\TEnv,\many{x\co\Subst\tau}\,} \Tm {\tau'} \Tm'} }};
 {\GTranslate C \TEnv {H\App\many x \to \Tm} {T\App\many v \to \tau'}
 {H\App(\many{\beta\co\star})\App(\many{c\co\Subst\tau'\sim\Subst\tau''})\App(\many{x\co\Subst\tau}) \to \Tm' }}
 }
}


The typing-cum-elaboration  judgment \fbox{$\GTranslate C \TEnv \Tm \pi \Tm'$} says that given a well typed surface level term $\Tm$ with type $\pi$, it is elaborated to a term $\Tm'$ in \SFC under the constraints C and typing environment $\TEnv$. The key idea of the elaboration is that the type equality constraints, $\tau\sim\tau'$, are elaborated to coercions in \SFC. The constraint C is a collection of named type equalities. The rules \trule{g-var}, $\trule{g-\I\forall}$ and \trule{g-$\E\forall$} are standard rules used for elaborating Hindley-Milner style system\cite{wadler_polymorphism_1989} to \SF while \trule{g-$\I C$} and \trule{g-$\E C$} reminiscent of elaborating typeclass constraints. In the implementation, the solver would produce the coercion $\Co$ using the list of assumptions $\TEnv$ in $\trule{g-\E\forall}$. The complex looking \trule{g-alt} elaborates case statements into \SFC. Each surface level data constructor is elaborated to the \SFC data constructor with explicit existential coercions as pattern variables. The rule $\trule{g-eq}$ is the same as \trule{cast} rule. The important detail here is that the coercion, $\Co$, is inferred from the constraint context $C$. Constructing the appropriate $\Co$ algorithmically is possible by using a simple unification algorithm based on \cite{lassez_unification_1988}.


\begin{figure}[ht]
 \centering
 \begin{gather*}
 \fbox{$\GTranslate C \TEnv {\Tm} {\pi} {\Tm'}$}\\
 \GADTVar \rsp \GADTEq\\
 \GADTForallI \rsp \GADTForallE\\
 \GADTCI \rsp \GADTCE
 \end{gather*}
 \begin{gather*}
 \fbox{$\GTranslate C \TEnv {p \to \Tm} {\pi \to \pi} {p' \to e'}$}\\
 \GADTAlt
 \end{gather*}
 \caption[Encoding GADTs]{Type-directed Translation of GADTs in \SFC}
 \label{fig:encoding-gadts}
\end{figure}

We can now make more sense of the type magic which the !eval :: GAlgExp a -> a! seemed to perform by elaborating the definition of the eval function into \SFC.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
data GAlgExp :: * -> * where
  Value  :: FORALL a. a -> GAlgExp a
  Plus   :: FORALL a. (a ~ Int) => GAlgExp a
                                -> GAlgExp a
                                -> GAlgExp a
  IsZero :: FORALL a. (a ~ Bool) => Int -> GAlgExp a
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
eval :: FORALL a. GAlgExp a -> a
eval (Value x) = x
eval (Plus (co :: a ~ Int) (x :: Int) (y :: Int))
   = (((eval (x /> sym co)) /> co)
     + ((eval (y /> sym co)) /> co)) /> sym co
eval (IsZero (co :: a ~ Bool) (x :: Int))
   = (isZero x) /> sym co
\end{code}
\end{minipage}

In the !Value! case, the argument to the constructor is a generic variable of type !a!, while in the case of !IsZero! an existential coercion, !co :: a ~ Bool!, is brought into scope. This coercion can be used for type refinement to the result type of !isZero x! to convert it back to a generic variable !a!. Recall that the type of the function !isZero! is !Int -> Bool!. The !Plus! although looks complicated, can be read systematically. Each argument !x! and !y! is first, coerced to a generic type !a! using the existential coercion !co : a ~ Int! and then coerced back into !Int! to perform the addition operation (!+!). Before returning, it is casted back into the generic type variable !a!.

The following lemmas are extensions of the meta-theoretic properties discussed in the previous section. We want to formalize the argument that ensures we have not lost the subject reduction property mentioned in \pref{thm:progress-sfc}.
\begin{lemma}[Type Preservation]
 If $\GTranslate C \empt \Tm \tau {\Tm'}$ then $\Typing C {\Tm'} \tau$
\end{lemma}
Type preservation is an important sanity check for the elaborations. It says that the the elaborated terms have the same type that was inferred at surface level.

\begin{theorem}[GADT Consistency]\label{thm:gadt-consistency}
 If $\dom\TEnv$ does not contain type variables and coercion constants, and $\CoKinding \TEnv \Co {\tau\sim\tau'}$ then, $\tau$ and $\tau'$ are syntactically identical.
\end{theorem}
\pref{thm:gadt-consistency} says that if two base types, or type function free types, are provably equal, then they must be syntactically identical. All GADT programs are sound in \SFC.

\begin{theorem}[GADT Soundess]
 If $\GTranslate \empt \empt \Tm \tau \Tm'$, then $\manystepsto {\Tm'} \Val$ iff $\manystepsto {\compile\Tm} \Val$ where $\Val$ is some value of ground type and $\compile\Tm$ is type erased term.
\end{theorem}

\subsection{Generative Abstract Types}\label{sec:fc-encodes-newtypes}
Generative abstract types become extremely simple to translate into \SFC. Each surface level type declaration gives rise to an coercion axiom. For example, consider the !newtype HTML! example from \pref{fig:html-generative-type}. The data type declaration generates an axiom

\begin{CenteredBox}
\begin{code}
coHtml :: Html ~ String
\end{code}
\end{CenteredBox}

The axiom !coHtml! can be used freely to cast any term with a type !HTML! to be used in the context that expects a !String!. The elaboration scheme enables us to not only have the cake but eat it too: it has the capability to provide type safety at zero run cost.

\begin{figure}[ht]
  \centering
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      mkHtml :: String -> Html
      mkHtml x = Html (escapeString x)

      unHtml :: Html -> String
      unHtml x = case x of HTML y -> y
    \end{code}
  \end{minipage}%
  \begin{minipage}[ht]{0.5\linewidth}
    \begin{code}
      mkHtml :: String -> Html
      mkHtml x = (escapeString x) /> (sym coHTML)

      unHtml :: Html -> String
      unHtml x = x /> coHtml
    \end{code}
  \end{minipage}
  \caption[\lstinline{HTML}]{\lstinline{newtype HTML} functions (left) and its elaboration in \SFC (right)}
  \label{fig:newtype-html-example}
\end{figure}

For example, in the programmer written function !mkHTML! as shown in \pref{fig:newtype-html-example}, the elaborated \SFC code will have a type cast instead of explicit boxing with a data constructor. Similarly, in the elaborated !unHTML! function, code would contain a type cast instead of explicit unboxing using a !case! statement as shown above. We now formalize the above elaboration procedure using a type directed translation. We require a new category of datatypes which allows only one data constructor and produces a coercion axiom during elaboration to \SFC.

\begin{figure}[ht]
  \centering
  \begin{syntax}
    \text{Newtype Type Constructors} & T_N\\
    \text{Newtype Data Constructors} & D_N
  \end{syntax}
  \begin{syntax}
    \text{Types}               && \tau,\sigma \bnfeq& \alpha \bnfor \cdots \bnfor T_N\\
    \text{Newtype Declaration} && D_{cl} \bnfeq& \texttt{\textbf{newtype}}~ T_{N}~\many\alpha = D_N~\Set{ x :: \tau }\\
  \end{syntax}
  \caption{\lstinline{newtype} support in \SFC}
  \label{fig:newtypes-sfc}
\end{figure}

The type directed translation is shown in \pref{fig:nt-elaboration}. The rule \trule{nt-ax} generates an axiom from the newtype declaration. There are two elaboration rules for the use of newtype data constructors, the rule \trule{nt-pack} is used when the data constructor is used to create a term of the type newtype, while the rule \trule{nt-unpack} is used eliminate the match statement and replace it with the term casted to the representation type using the appropriate instantiations of the coercion axiom.


\newcommand\NTAx{
 \ib{\irule[\trule{nt-ax}];
 {\NTranslate \TEnv {\texttt{newtype}~ NT~ \many{\alpha} = ND~ \Set {x : \tau}} {\texttt{axiom}~coNT : \Forall {\many{\alpha}} {NT~\many\alpha \sim \tau}}}
 }
}

\newcommand\NTElab{
 \ib{\irule[\trule{nt-pack}]
 {\NTTranslate \TEnv M \tau {M'}};
 {\NTTranslate \TEnv {ND\App M} {NT\many\tau} {\Cast {M} {\texttt{sym}~(coNT \At \many\tau)}}}
 }
}

\newcommand\NTPatElab{
 \ib{\irule[\trule{nt-unpack}]
 {\NTTranslate \TEnv M \tau {M'}};
 {\NTTranslate \TEnv {ND\App M} {NT\many\tau} {\Cast {M} {coNT \At \many\tau}}}
 }
}


\begin{figure}[ht]
\centering
\begin{gather*}
\NTAx \\ \NTElab \rsp \NTPatElab
\end{gather*}
\caption{\texttt{Newtype} type directed translation}
\label{fig:nt-elaboration}
\end{figure}

It is now easy to see how on the surface level the functions !mkHtml! and !unHTML! seem to pack and unpack
the !String! value stored in the !Html! type, the compiler generates code where all the packing and unpacking of the values are removed using explicit type casts. Further during runtime, the explicit casts are also removed. This optimization step is not performed on algebraic data types declared using !data! keyword. The runtime cost characteristics of types declared using !newtype!s and !data! are different. Due to the newtype definition
we know that !Html! \emph{is} !String! as !String! is the concrete runtime representation of !Html!. This means that we should not pay any runtime cost for coercing !Html! to !String! and vice versa. This is achieved using the
coercion axiom introduced during the elaboration step !coHtml :: Html ~ String!.


\subsection{Type Computation}
\subsubsection{Associated Types}\label{sec:fc-encodes-assoctypes}
Elaborating associated types to \SFC is very similar to translating GADTs. The constraint context now contains class predicates along with equality predicates. This extension is also needed if GADT data constructors need to constrain certain type parameters. The equality constraints can now appear not only in the context of GADT data constructors but any term type.

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Class Declarations} &&C_{ls} \bnfeq& \textbf{\texttt{class }} D\App\many\TyVar \textbf{\texttt{ where }} \many{dsigs}; \many{sigs}\\
 \text{Instance Declarations} &&I_{nts} \bnfeq& \textbf{\texttt{instance }} D\App\many\tau \textbf{\texttt{ where }} \many{adata}; \many{val}\\
 \text{Associated types} &&dsigs \bnfeq& \textbf{\texttt{type }} \tau\\
 \text{Method signatures} &&vsigs \bnfeq& x\co\tau\\
 \text{Associated type instance} &&asigs \bnfeq& \tau = \sigma\\
 \text{Method bindings} &&asigs \bnfeq& x = \Tm
 \end{syntax}
 \caption[Class Syntax]{Class and Associated Types Surface Syntax}
 \label{fig:assoc-types-syntax}
\end{figure}

The judgment $\DTranslate C {D\App\tau} d$ elaborates the predicate $D\App\tau$ to a dictionary $d$ in \SFC. The rule \trule{subst} allows replacing type class parameters with equal types. This is necessary to account for associated types that appear in the type class signature. $D\App\tau$ may contain an associated type, and depending on the instantiations of the free type variables in $\tau$ an appropriate coercion can be used to justify casting the dictionary $d$ to the corresponding actual type.

$$
\ib{\irule[\trule{subst}]
 {\DTranslate C {D\App\tau} d}
 {\CoKinding C \Co {D\App\tau \sim D\App\tau'}};
 {\DTranslate C {D\App\tau'} {\Cast d \Co}}
}
$$

Reconsider the !Con c! typeclass and a function !sumCon :: (Con c, Num (Elem c)) => c -> Elem c!, which sums up all the elements of the collection !c!. The predicate !Num (Elem c)! asserts that the elements of the collection can be summed up. A reasonable use of !sumCon! would be at type ![Int]!, as integers can indeed be summed up. This would result in instantiating the type parameter !c! with ![Int]!. The rule \trule{subst} in this case justifies the use of !Num Int! dictionary as if it was a !Num (Elem [Int])! dictionary.

Each class declaration introduces a new class predicate name in the type environment along with its method names. With associated types, a new ``type function'' name---!Elem c! in the case of !Con c!---is also added to the type environment.
Each instance introduces a new axiom into the typing environment. For the !Con c! typeclass the instance !Con [Int]! introduces the coercion !CoCon : Elem [Int] ~ Int!. In general, each instance generates an axiom of the form !co: (FORALL $\many{\TyVar\co\star}$. F $\sigma$ $\sim$ $\sigma'$)! where $\fvs\sigma = \many\TyVar$
and $\fvs{\sigma'} \subseteq \many\TyVar$. These axioms induce a rewrite function on types and are also called as rewrite axioms.

\begin{theorem}[Associated Type Consistency]
If $\TEnv$ contains type rewrite axioms that are confluent and terminating, then $\TEnv$ is consistent.
\end{theorem}
For non-overlapping instances, it is indeed the case that the rewrite axioms are confluent and terminating.

\subsubsection{Open Type Functions}\label{sec:fc-encodes-opentypefun}
An unusual feature of the system, which also makes the type system expressive is the ability to write open type functions. The intention is to generalize associated types to be independent of typeclasses. Type functions are able to express complex type level computations. Further, they are also open, meaning, just like class instances, they can be extended. An example of how type computations are expressed by type functions that define addition of naturals at type level, shown in \pref{fig:open-type-fun-add}.
\begin{figure}[ht]
 \begin{minipage}[ht]{0.4\linewidth}
 \begin{code}
 data Z
 data S n
 \end{code}
 \end{minipage}%
 \begin{minipage}[ht]{0.4\linewidth}
 \begin{code}
 type family Plus m n
 type instance Plus Z n = n
 type instance Plus (S m) n = S (Plus m n)
 \end{code}
 \end{minipage}
 \caption{Type level Arithmetic}
 \label{fig:open-type-fun-add}
\end{figure}
Open type functions thus give type computation a flavor of rewrite rules. Each type family instance directly translates to coercion axioms. Thus for !Plus m n! type function, the two associated instances would introduce the following axioms !CoPlusZn :: Plus Z n ~ n! and !CoPlusSmn :: Plus (S m) n ~ S (Plus m n)!. There are certain caveats on how these coercion axioms introduced by type functions can be used by the typechecker to avoid inconsistency. For example, consider an open type function !F! and two of its instances that give rise to axioms !coFIB! and !coFBB! respectively.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
type family F a
type instance F Int = Bool
type instance F Bool = Bool
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
axiom coFIB: F Int ~ Bool
axiom coFBB: F Bool ~ Bool
\end{code}
\end{minipage}

We can derive a coercion !Int ~ Bool! if we are not careful enough on how we use the coercions. Both axioms have !Bool! as their result type, hence we can derive !F Int ~ F Bool! by transitivity, and then we can use the !right! construct to derive !Int ~ Bool!. the term $\texttt{right}\App(\trans{\texttt{coFIB}}{\Sym{\texttt{coFBB}}})$ has the type !Int ~ Bool!. It is crucial to use the !left! and !right! rules only on types that are not applications of type functions as they can be non-injective. This also influences the surface language design: the type functions need to be fully saturated.

\subsubsection{Functional Dependencies}\label{sec:fc-encodes-fundeps}
Consider the !Con e c! from \pref{fig:tc-collection-fd}.

\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
class Con e c | c ~> e where ...
instance Con Int [Int] where ...
empty :: FORALL e c. Con e c => c
extend :: FORALL e c. Con e c => e -> c -> c
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{code}
type FDCon c : *
axiom FDConInt : FDCon [Int] ~ Int
empty  : FORALL c. Con (FDCon c) c => c
extend : FORALL c. Con (FDCon c) c => e -> c -> c
\end{code}
\end{minipage}

There is a way to encode functional dependencies via the mechanism of open type functions. Each functional dependency induces a special type function that relates determiner to the determinant. Each typeclass instance introduces an axiom that encodes the functional mapping. Further, every type that mentions the determinant of the functional dependency gets replaced by the type function: !Con e c! gets converted to !Con (FCCon c) c!.
We skip the formalization for encoding functional dependencies until further \pref{sec:conclusion}; while there is an encoding mechanism that is sound, a complete encoding that covers all exotic cases is an open problem

\subsection{General Observations}
In the current formalization, all the types (and coercions) are explicit in the system. This means that while it is easy to design and implement a machine checker to check its well-formedness, it is difficult for humans to read and understand it. It is also well known that type inference for \SF is undecidable\cite{wells_typability_1999}, but would be worth while to investigate if, as an optimization step, the explicit coercions can be elided and then by means of type inference they could be reconstructed, albeit the intension of \SFC is to be used as core language, and programmers would never have the need to read the verbose code the same way we do not expect programmers to understand assembly code. We call the system with implicit coercion calculus, \SFCi. The key difference between \SFC and \SFCi is that where \SFC has a coercion type $\Co$ of kind $\tau\sim\tau'$, \SFCi only gives the equality kind in curly braces $\tau\sim\tau'$. Hence, for terms
\begin{itemize}
\item Type casts, $\Cast \Tm \Co$, turns into $\Cast \Tm \Set{\tau \sim \tau'}$ and,
\item Coercion applications, $\Tm\App\Co$ turns into $Tm\App\Set{\tau \sim \tau'}$
\end{itemize}
\begin{theorem}[Undecidability of coercion reconstruction of \SFCi]
 If $\Tm_i$ is an expression in \SFCi and $\TEnv$ is a typing environment, then reconstructing a \SFC term $\Tm$ such that $\Typing \TEnv {\stepsto {\Tm_i} \Tm} \sigma$, where $\Typing \TEnv \Tm \sigma$ holds is undecidable.
\end{theorem}
The proof of undecidability amounts to reducing the problem of coercion reconstruction to A-ground theories in a semi-Thue system, which known to be undecidable\cite{post_recursive_1947}. If there exists an alternative formulation of \SFCi with fewer explicit type equalities and is sufficient to encode all the language features while enjoying decidable type checking is an open question.

The second observation is that the coercions are types, kinds of which give type equalities. This is novel to \SFC, and the status quo is to express coercions as terms. The goal of the core language is to be practical; due to all coercions being encoded at type leve, they can be erased at runtime to generate efficient code. Another reason is that for implementation purposes, the error term, $\bot$ which trivially just halts the program, by throwing an exception, can be typed at all types. If equality had been encoded at the level of types by making coercions at the level of terms, we would have an ``error coercion'' term, $\bot :: \tau \sim \sigma$. We would then have had to guarantee evaluating the the type equality evidences before we evaluate the term. Without this guarantee we cannot use a coercion for casts and maintain soundness. To avoid such complications and maintain a phase distinction \SFC follows the slogan: \emph{Kinds are propositions for type equality, proofs are (coercion) types}

\section{Extensions of \SFC}\label{sec:fc-extensions}%%%%%%%%%
\subsection{\SFR}\label{sec:sfr} % R for roles

\begin{CenteredBox}
  \begin{code}
    type family F a :: *
    type instance F Html = Bool
    type instance F String = Char
  \end{code}
\end{CenteredBox}

Consider the two modules previously defined in \pref{subsubsec:gen-abs-types}: !module Html!, where the library writer defines a !newtype HTML! and the other, !module Client!, a client module that uses the library module !HTML! along with an open function !F!, that maps !HTML! to !Bool! and !String! to !Char!. This declaration adds two new type equalities namely !F Html ~ Bool! and !F String ~ Char!. But, in the presence of the equality !Html ~ String!, we can make type unsound coercions by deriving !Char ~ Bool!.

At first sight, it is puzzling to comprehend why is this even possible although we had a characterization of $\Good\TEnv$ in \pref{thm:progress-sfc} to make these ``bad'' coercions impossible to derive. By careful examination, we can in fact derive the coercion !Char ~ Bool! as follows: by virtue of the type function axiom we have !Char ~ F String!, then we use coercion lifting to derive !F String ~ F Html! (as !String ~ Html!), and finally, obtain !Char ~ Bool! (by using !F Html ~ Bool!, and !F String ~ Char!). The problem stems from the fact that we are using unconstrained coercion lifting; the use non-parametric features (type functions) of the language in the context that assumes parametricity (newtypes). Type functions interacting with newtypes is just one case where seemingly innocuous features mingling with each other cause type unsoundness. Just limiting the use of type functions however is not enough, similar type unsoundness behavior is can be observed by using GADTs and newtypes as well\cite{weirich_generative_2011}. In the previous \pref{sec:sfc-encoding-features}, we studied \SFC encodings of each language feature and their respective soundness criteria was straightforward when viewed independently. However, the above ``bug'' is an evidence that we need to do more work to prove type soundness.

 \begin{figure}[ht]
 \begin{syntax}
 \text{Type Vars} &\TyVar,\beta,\Co &\qquad\text{Type constants} &T \\
 \text{Term Vars} &x,y &\qquad\text{Newtypes} &\shl{\NType} \\
 \text{Coercion Vars} &c &\qquad\text{Type Functions} &F\\
                      &  &\qquad\text{Indices} &i,n \in \mathbb{N}\\
 \end{syntax}
 \begin{syntax}
   \text{Roles} &&\rho \bnfeq& \shl{\texttt{N} \bnfor \texttt{R} \bnfor \texttt{P}}\\
   \text{Kinds} &&\kappa \bnfeq& \star \bnfor \kappa \to \kappa \bnfor \shl{\sigma \sim^\kappa_\rho \tau}\\
   \text{Types} &&\tau,\sigma \bnfeq& \TyVar \bnfor \mathcal{T} \bnfor \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor F\\
    \text{Type Constants} &&\TypeConst \bnfeq& T \bnfor (\to) \bnfor \shl{\NType}\\
 \text{Coercions} &&\nu,\Co \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\nu\Co % equiv relation
 \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction, instantiations
 \bnfor \nu\App\Co \\
                  &&              &\bnfor \Left \Co \bnfor \Right \Co \bnfor \Nth i \Co \bnfor \TypeConst\App\many\Co \bnfor F\many\Co \bnfor \shl{\SubCo \Co} \\  % compose/decompose
 \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
 \text{Patterns} &&P \bnfeq& H\App \many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co
 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Role Context} &&\mathcal{R} \bnfeq& \empt \bnfor \mathcal{R},\alpha\co\rho\\
                     &&\roles{\TypeConst} \bnfeq& {[\rho \mid \alpha\co\rho \in Params(\TypeConst)]}\\
                     &&Params(\TypeConst) \bnfeq& \many\alpha\co\rho\quad  \text{s.t. } (\TypeConst\App\many\alpha) \co \star% \\
                     %&&\roles{F} \bnfeq& \many{\alpha\co N}\qquad\text{ where }F\many\alpha\co\star
 \end{syntax}
 \caption{Excerpt of Syntax of \SFR; extension of \SFC}
 \label{fig:sfr-syntax}
 \end{figure}

\SFC requires amends to ensure that such unsound coercions cannot be derived within the system.
The main extension to \SFC is by making the equality predicate finer grained by being able to express \emph{how} the two types are equal. In the above case, we see that we have two flavors of type equality coercions: (i) nominal equality, which is established via type function instances (!F String ~ Char!), and (ii) representational equality which is established via newtype definitions (!Html ~ String!). With these different notions of equality, one can restrict coercion lifting as representational equality does not imply nominal equality.

\subsubsection{Syntax}\label{sec:sfr-syntax}
We formalize the syntax of the system in \pref{fig:sfr-syntax} and call it \SFR. We extend the syntax infrastructure from \pref{fig:sfc-syntax}. Although the original work by \citet{breitner_safe_2014, weirich_generative_2011} formalizes the system slightly differently, the presentation in this report is easier to understand as an simple extension of \SFC, and it is also representative to what the GHC compiler implements. \TODO{should the $\teq\rho$ still stay in the kind land or should I pull it down into types land?} The key extension to represent the notion of finer grained type equality, is done by using \emph{roles}, $\rho$. All type parameters to a type constructor are decorated with a role that answers the question how are the two types in question equal. We define the function $\roles{\TypeConst}$ that returns the roles of all the type parameters of a type constant $\TypeConst$.

\subsubsection{Static Semantics}\label{sec:sfr-static-sem}
The main judgment is for coercions $\CoKinding \TEnv \Co {\tau\sim^\kappa_\rho\sigma}$. This is read as ``in the type environment $\TEnv$ the coercion $\Co$, witness the equality between types $\tau$ and $\sigma$ that have the same kind $\kappa$ at role $\rho$''. The ``equality at role $\rho$'' is the novel feature of \SFR. We will omit the kind $\kappa$ when it is clear from the context. \SFR has three different roles for three different purposes:
\begin{itemize}
\item\textbf{Nominal Equality} This is the strictest kind of equality denoted by $\teqN$ which holds exactly when the two types are the ``same''. For example, an type function axiom !F Int = Bool! makes !F int!$\teqN$!Bool!
\item\textbf{Representational Equality} This equality holds when the two types have the same runtime representation. For example, a new type declarations !newtype Age = Int! makes !Age!$\teqR$!Int!
\item\textbf{Phantom Equality} This equality holds always for all types: !FORALL a b. a $\teqP$ b!.
\end{itemize}

\newcommand\CastR{
  \ib{\irule[\trule{cast-r}]
    {\Typing \TEnv \Tm \tau}
    {\CoKinding \TEnv \Co {\tau \teqR {\tau'}}};
    {\Typing \TEnv {\Cast \Tm \Co} {\tau'}}
  }
}
\newcommand\CastN{
  \ib{\irule[\trule{cast-n}]
    {\Typing \TEnv \Tm \tau}
    {\CoKinding \TEnv \Co {\tau \teqN \tau'}};
    {\Typing \TEnv {\Cast \Tm \Co} \tau'}
  }
}

\newcommand\KSubCo{
 \ib{\irule[\trule{co-sub}]
 {\CoKinding \TEnv {\Co} {\tau \teq\rho \tau'}}
 {\rho < \rho'};
 {\CoKinding \TEnv {\SubCo \Co} {\tau \teq{\rho'} \tau'}}
 }
}

\newcommand\KNthCo{
 \ib{\irule[\trule{co-nth}]
   {\CoKinding \TEnv {\Co} {T \App \many\sigma \teqR T\App\many{\tau'}}}
   {\many\rho = \text{roles}(T)};
   {\CoKinding \TEnv {\Nth i \Co} {\tau_1 \teq{\rho_i} \tau_2}}
 }
}

\newcommand\KLeftCoR{
 \ib{\irule[\trule{co-left}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \teqN \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Left \Co} {\tau_1 \teqN \tau_2}}
 }
}

\newcommand\KRightCoR{
 \ib{\irule[\trule{co-right}]
 {\CoKinding \TEnv {\Co} {\tau_1 \App \sigma_1 \teqN \tau_2 \App \sigma_2}};
 {\CoKinding \TEnv {\Right \Co} {\sigma_1 \teqN \sigma_2}}
 }
}

\newcommand\KTyAppCo{
  \ib{\irule[\trule{co-ty-app}]
    {\substack {\mathlarger {\Kinding \TEnv {\tau_1\App\sigma_1} {\kappa}}\\
               {\mathlarger {\Kinding \TEnv {\tau_2\App\sigma_2} {\kappa}}}}}
    {\substack {{\mathlarger{\CoKinding \TEnv {\Co_1} {\tau_1 \teq\rho \tau_2}}}\\
               {\mathlarger {\CoKinding \TEnv {\Co_2} {\sigma_1 \teqN \sigma_2}}}}};
    {\CoKinding \TEnv {\Co_1\App\Co_2} {\tau_1 \sigma_1 \teq\rho \tau_1 \sigma_1}}
  }
}

\newcommand\KFAppCo{
  \ib{\irule[\trule{co-F-app}]
    {F \in \TEnv}
    {\Kinding \TEnv {F\App\many{\tau}} {\kappa}}
    {\Kinding \TEnv {F\App\many{\sigma}} {\kappa}}
    {\many{\CoKinding \TEnv {\Co} {\tau \teqN \sigma}}};
    {\CoKinding \TEnv {F \many\Co} {F \many{\tau} \teqN F \many{\sigma}}}
  }
}

\newcommand\KTyConAppCo{
  \ib{\irule[\trule{co-tycon-app}]
    {\many\rho = \roles{\TypeConst}}
    {\substack {\mathlarger {\Kinding \TEnv {\TypeConst\App\many{\tau}} {\kappa}}\\
               {\mathlarger {\Kinding \TEnv {\TypeConst\App\many{\sigma}} {\kappa}}}}}
    {\many{\CoKinding \TEnv {\Co} {\tau \teq\rho \sigma}}};
    {\CoKinding \TEnv {\TypeConst \many\Co} {\TypeConst \many{\tau} \teq\rho \TypeConst\many{\sigma}}}
  }
}


\begin{figure}[ht]
 \centering
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \tau \kappa$}\\
 \KSubCo\\
 \KNthCo \rsp \KLeftCoR \rsp \KRightCoR \\
 \KTyAppCo \rsp \KFAppCo  \\
 \KTyConAppCo
\end{gather*}
  \begin{gather*}
    \fbox{$\Typing \TEnv M \tau$}\\
    \CastR \rsp \CastN
  \end{gather*}

 \caption{Excerpt of Static Semantics of \SFR: Coercion Calculus}
 \label{fig:sfr-typing}
\end{figure}
Type safe casts can now be expressed using the typing rules \trule{cast-r} and \trule{cast-n} as shown in \pref{fig:sfr-typing}. The rule \trule{cast-r} captures our notion of equality that arises due to coercion axioms introduced by newtype definitions as the rule enables coercions between expressions that have the same representation type. The rule \trule{cast-n} captures the notion of type equalities introduced by instantiating axioms arising due to type functions. The phantom equality on types seems unnecessary and outright wrong: why would any two types be equal if they are obviously not equal? There are cases where type parameter instantiations do not influence the value structure of the type, especially in cases of multi-parameter data types. Phantom equality is useful to assert equality on those types and keeps the type system usable. An additional goal of this extension of finer grained equality is that it should be as backwards compatible as possible i.e. be able to type check all the previous code that passed type checking and rightly did so.

Extending the type system with the two rules \trule{cast-r} and \trule{cast-n} would in theory be enough to express what we want, but we can do better by realizing that the equality roles form a total relation: $N < R < P$. If two types are nominally equal, they are representationally equal as well. Similarly, if the two types are representationally equal, they are also equal at phantom role. This is captured in the \trule{co-sub} shown in \pref{fig:sfr-typing}. It says that if we \emph{want} to have equality at representational role, and we are \emph{given} that the two types are nominally equal, then we can generate a coercion that proves the wanted equality from the given equality. Nominal equality is the strictest from of equality while phantom equality is the weakest.

\newcommand\RVar{
  \ib{\irule[\trule{r-var}]
    {\tau\co\rho' \in \REnv}
    {\rho' \leq \rho};
    {\Typing \REnv \tau \rho}
  }
}

\newcommand\RTyApp{
  \ib{\irule[\trule{r-ty-app}]
    {\Typing \REnv \tau \rho}
    {\Typing \REnv \sigma N};
    {\Typing \REnv {\tau\App\sigma} \rho}
  }
}

\newcommand\RTyConApp{
  \ib{\irule[\trule{r-tycon-app}]
    {\many{\Typing \REnv {\tau} {\rho}}}
    {\many{\rho} \text{ is a prefix of } \roles{\TypeConst}};
    {\Typing \REnv {\TypeConst\App\many\tau} R}
  }
}

\newcommand\RFApp{
  \ib{\irule[\trule{r-f-app}]
    {\many{\Typing \REnv \tau N}};
    {\Typing \REnv {F\App\many\tau} N}
  }
}


\newcommand\RTAssign{
  \ib{\irule[\trule{r-T}]
    {H\co \Forall{\many{\alpha\co\kappa}}{\Forall{\many{\beta\co\kappa'}}{\many\sigma \to T\many\alpha}} \in C_{trs}(T)}
    {\tau \in \many\sigma}
    {\Typing {\many{\alpha\co\rho},\many{\beta\co N}} \tau R};
    {\RoleAssign {\many\rho} T}
  }
}

\newcommand\RNTAssign{
  \ib{\irule[\trule{r-nty}]
    {co\NType : \Forall \alpha {\NType\many\alpha \teqR \sigma}}
    {\Typing {\many{\alpha\co\rho}} \sigma R};
    {\RoleAssign {\many\rho} \NType}
  }
}

\newcommand\RToAssign{
  \ib{\irule[\trule{r-$\to$}]
    {};
    {\RoleAssign {\alpha\co R, \beta\co R} {\alpha \to \beta}}
  }
}

\newcommand\REqAssign{
  \ib{\irule[\trule{r-$\sim$}]
    {};
    {\RoleAssign {\alpha\co\rho, \beta\co\rho} ({\alpha} \teq\rho {\beta})}
  }
}

\begin{figure}[ht]
  \centering
  \begin{gather*}
    \fbox{$\many\rho \forces \TypeConst$}\\
    \RNTAssign \rsp \RTAssign\\
    \RToAssign \rsp \REqAssign
  \end{gather*}
  \begin{gather*}
    \fbox{$\Typing \REnv \tau \rho$}\\
    \RVar \rsp \RTyApp\\
    \RTyConApp \rsp \RFApp
  \end{gather*}
  \begin{gather*}
    \fbox{$\rho \leq \rho'$}\\
    {\ib{\irule[]{};{N \leq \rho}}} \rsp {\ib{\irule[]{};{\rho \leq \rho}}} \rsp {\ib{\irule[]{};{\rho \leq P}}}
  \end{gather*}
  \caption{\SFC Assignment and Validity Rules for Roles}
  \label{fig:sfr-validity}
\end{figure}


Role assignments to type parameters are performed during type checking the user defined type constant. They are formalized using the role validity as shown in \pref{fig:sfr-validity}. The judgment $\RoleAssign {\many\rho} \TypeConst$ is to be read as ``$\many\rho$ are appropriate for the type constant \TypeConst'', while the judgment $\Typing \REnv \tau \rho$ is read as ``under the role context assumptions $\REnv$, the type $\tau$ has the role $\rho$''. We do not want a global type context to claim that a type parameter's role which is in conflict with the role given to it by the definition the type constant. The role assignment algorithm plays an important role (no pun intended) in inferring the best role for the type parameters for a type constant by walking over its definition. We want to ensure that the role assignments are no too restrictive nor to permissive. In the former case, we can assign each type parameter a nominal role but we would fail to type check a huge majority of the programs that we want to be able to type check. In the later case we can assign each type parameter a phantom role but that would be type unsound.

\begin{CenteredBox}
  \begin{code}
    data Any a = Any
    newtype App f a = MkApp (f a)
  \end{code}
\end{CenteredBox}
Consider the following definitions shown above for !Any a! and !App f a!. With the rules given in \pref{fig:sfr-validity}, for the generative type !App!, the roles inferred will be $\roles{\texttt{App}} = R, N$. Now, due to the rules of the coercion calculus we can derive the following representational type equality:
\begin{align}\label{eqn:app-any-newtype}
  \texttt{App } \App \texttt{Any}\App \texttt{Int} \teqR \texttt{App}\App\texttt{Any} \App{Bool}\tag{app-co}
\end{align}
This is possible because of the following chain of reasoning:
\begin{align}
  \texttt{App} \App \texttt{Any}\App \texttt{Int}%
  \teqR \texttt{Any}\App \texttt{Int}%
  \teqR \texttt{Any}\App \texttt{Bool}%
  \teqR \texttt{App}\App\texttt{Any} \App{Bool}\nonumber
\end{align}
The first step (and the last step) of the reasoning is valid due to the instantiations of the coercion axiom $\texttt{coApp} : \Forall{f, a}. \texttt{App}\App f\App a \teqR f\App a$, which rises by virtue of the definition of the newtype. We do not want to allow the rule \trule{co-nth} to \pref{eqn:app-any-newtype}. Because if we did, then we would be able to produce a coercion $\texttt{Int} \teqR \texttt{Bool}$, a blunder. We thus carefully allow the \trule{co-nth} only on vanilla algebraic type constants ($T$); not on newtypes ($T_N$) and neither on type function constants $F$.

\subsubsection{Meta-theory}\label{sec:sfr-metatheory}
The proof of progress does not change with respect to \SFC as the only additional construct is roles, which does not play any part in the operational semantics of the language. To recall, the proof of progress is necessary to show that the well typed terms are either values or they can take a step.

\begin{theorem}[Progress for \SFR]\label{lem:sfr-progress}
 If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ then, either $\Tm \in C\Val$ or, $\stepsto \Tm \Tm'$
\end{theorem}

The proof of preservation uses the formulation of coercion calculus extended with roles and thus requires some attention. The reason for type unsoundness was that we allowed too many coercions with unrestricted coercion lifting in \SFC.

\begin{lemma}[Soundness of Role Narrowing]\label{lem:role-narrowing}
If $\RoleAssign{\many\rho}{\TypeConst}$, and $\many\rho' = \Set{\rho' \mid \rho \in \many\rho, \rho' \leq \rho}$ then $\RoleAssign{\many\rho'}{\TypeConst}$
\end{lemma}

The above \pref{lem:role-narrowing} says that it is sound to make the role assignments stricter for any type constant. It formalizes the argument we hinted at in \pref{sec:sfr-static-sem}. Narrowed role assignments will reject more programs so the programmer will find it difficult to work with even if it guarantees type safety.
We now need a role assignment algorithm which ensures that the typechecker can assign the roles automatically. This characterization of role inference runs parallel to type inference: we want to develop an algorithm, possibly efficient, such that when the typechecker assigns the roles, it is most general, in the sense we do not want to make the system unusable, but also we want to have roles to be correctly assigned.

\newcommand\RoleInfer{$\mathcal{R}_{infer}$\xspace}
\newcommand\walk[2]{\texttt{walk}(#1,~#2)}
\begin{figure}[ht]
  \begin{algorithmic}[1]
    \Procedure{\RoleInfer}{[$\TypeConst$]}\Comment{Infer the roles of type parameters of all $\TypeConst$s}

    \State For all $\TypeConst$, populate $roles(\TypeConst)$ using programmer supplied annotations. \\
    \qquad Or default ADT and newtype type parameters to $P$.
    \State For every $H\co\tau \in C_{trs}(\TypeConst)$ and every $\sigma$ that appears in $\tau$: call $\walk\TypeConst\sigma$\label{here}
    \State For every newtype $T_N$, and its representation type $\sigma$: call $\walk\TypeConst\sigma$
    \State If any role parameter changed in previous steps then go to Step 2.
    \State For all $\TypeConst$, Check $\RoleAssign {\roles\TypeConst} \TypeConst$. Error on inconsistency.
    \EndProcedure
  \end{algorithmic}

  \begin{algorithmic}[1]
    \Procedure{\texttt{walk}}{$\TypeConst, \sigma$}\Comment{Role marking for $\TypeConst$}
    \State $\walk \TypeConst {\alpha} \bnfeq$ Mark $\alpha$ to $R$, if not already marked.
    \State $\walk \TypeConst {H\App \many\tau} \bnfeq$ let $\many\rho = \roles{H}$
    \If {Any $\rho_i = N$ such that $\rho_i \in \many\rho$}
        \State Mark all unmarked parameters in $\tau_i \in \many\tau$ as $N$
    \ElsIf{Any $\rho_i = R$}
        \State call $\walk{\TypeConst}{\tau_i}$
    \EndIf
    \State $\walk \TypeConst {\tau\App\tau'} \bnfeq$ $\walk{\TypeConst}{\tau}$ and then mark all unmarked parameters in $\tau'$ as $N$
    \State $\walk \TypeConst {F\App\many\tau} \bnfeq$ Mark all type parameters in $\many\tau$ to $N$
    \State $\walk \TypeConst {\Forall{\beta\co\kappa}\tau} \bnfeq \walk \TypeConst \tau$
    \EndProcedure
  \end{algorithmic}
\caption{\RoleInfer algorithm and $\walk\TypeConst\sigma$}\label{alg:role-infer}
\end{figure}

The role inference algorithm, \RoleInfer is much simpler than type inference algorithm, as there are only three choices to be made for the type parameters for each type constant in question. We will use the following role defaulting policy for type parameters that are unspecified by the programmer: ADT and newtypes get phantom roles type functions get nominal roles. To allow the programmer to override the defaulting role assignment for a type constant declaration, we introduce a new optional syntax.
This syntax is not backwards compatible but it is lightweight and can be easily incorporated by using compiler pre-processor pragmas. GHC already supports CPP style macros\footnote{\url{https://downloads.haskell.org/ghc/latest/docs/users\_guide/phases.html\#standard-cpp-macros}} which can decide by checking the compiler version if these type role annotations are to be kept in the source code to be passed to the typechecker or not.

\begin{CenteredBox}
\begin{code}
type role App representational nominal
newtype App f a = MkApp (f a)
\end{code}
\end{CenteredBox}
In the above code, for the !App! user defined type, the type parameter !f! is assigned representational while !a! is assigned nominal, or that $\roles{App} = [R, N]$
% \TODO{The role inference with some defaulting works well in practice,}
% \TODO{Where it does not work give the role using an annotation.}
% \TODO{We have a role sub-tying relation, it is okay as there is total ordering.}
% \TODO{The role check/inference is be performed during or after kind checking.}
% \TODO{how does it avoid unrestricted coercion lifting?}

\RoleInfer is described in a declarative style in \pref{alg:role-infer}
The $\walk{\TypeConst}{\sigma}$ is the workhorse that marks the type parameters of the $\TypeConst$ with appropriate roles. The algorithm \RoleInfer runs $\walk{\TypeConst}{\sigma}$ for every new $\TypeConst$ defined by the programmer. In the first case of $\walk\TypeConst\sigma$, if $\sigma$ is a type variable, $\alpha$, which is not a type parameter, we can just ignore it.

We have the following properties for \RoleInfer algorithm:
\begin{enumerate}
\item terminating;
\item sound role choice for all type parameters and;
\item optimal role choice for all type parameters and
\end{enumerate}

The algorithm is terminating as at every call to $\walk{\TypeConst}\sigma$, we either mark the type parameter role stricter that the previous one we never touch it again. Due to the lower bound on the role ordering, and there being a finite number of type parameters, we are guaranteed to reach a fixed point. The role assignment soundness property follows directly from \pref{lem:role-narrowing}, as each walk essentially narrows the role. The algorithm a greedy algorithm which proves that the role assignment is optimal, as if there was a valid role assignment that was wider than the one assigned by the algorithm, then the algorithm would have chosen that before narrowing it.

\begin{lemma}[Preservation for \SFR]\label{lem:sfr-preservation}
If $\Good \TEnv$ and $\Typing \TEnv \Tm \tau$ and $\stepsto \Tm \Tm'$, then $\Typing \TEnv {\Tm'} \tau$
\end{lemma}
The role directed coercion calculus can be proved to be confluent only if the right hand sides of the coercion axioms have linear patterns. Confluence of type rewriting guarantees consistency for the complete system. In case of overlapping non-linear patterns, the proof of confluence is an open problem\cite{mizuhito_rta_1995}.
Finally, The type preservation lemma, \pref{lem:sfr-preservation},
along with \pref{lem:sfr-progress} is the formal guarantee that
ensures type soundness.

To summarize, \SFR strove to find a good power to pain ratio by making minimal changes to the system for the benefit to the programmer and re-establishing type soundness by disallowing unsafe coercion lifting by virtue of roles.

\subsection{\SFP}\label{sec:sfp} % P for promotion
Consider a standard GADT implementation of a vector !Vec! that also stores its length at the type level.

\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
        data Z
        data S n
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
    data Vec : * -> * -> * where
       Nil : Vec Z elem
       Cons : elem -> Vec n elem -> Vec (S n) elem
\end{code}
\end{minipage}

The types !Z! and !S n! encode the size of the vector. The data
constructor !Nil! is a zero length vector which is asserted by the
index type !Z!, while !Cons! appends an element of type !elem! to a
vector of size !n! to return a vector of size !S n!. With a primitive
kind system, there is no enforcement that the type argument to !Vec!
has to be either !Z! or !S n!. Any type of kind, $\STAR$ is a valid
argument. A desirable alternative would be to restrict the kind
of the index type that describes the size of the vector.

\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
         data Nat = Z | S Nat
\end{code}
\end{minipage}%
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
     data Vec : Nat -> * -> * where
        Nil : Vec Z elem
        Cons : elem -> Vec n elem -> Vec (S n) elem
\end{code}
\end{minipage}

In this setting, the typechecker has enough information to reject
semantically absurd types like !Vec Char a!. As the kind of !Char! is
$\STAR$ and not !Nat!. Similar problems arise while writing addition function on type level
naturals. The situation is worse as writing type functions does not
include any kind information; the argument and return types are
defaulted to kind $\STAR$, making it seem ill-kinded. The intention of
the programmer is not that any type of kind $\STAR$ is a valid
argument to the function !Plus!. A better alternative would be to
allow specifying the concrete kind specification to the function---in
this case specifying that the arguments and return kind of !Plus! is a
!Nat!.


\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
     type family Plus m n
     type instance Plus Z m = m
     type instance Plus (S n) m = S (Plus n m)
\end{code}
\end{minipage}
\begin{minipage}[ht]{0.4\linewidth}
\begin{code}
     type family Plus (m:Nat) (n:Nat) :: Nat
     type instance Plus Z m = m
     type instance Plus (S n) m = S (Plus n m)
\end{code}
\end{minipage}

Allowing user defined kinds, albeit more expressive, are awkward to use without kind polymorphism. Consider a higher kinded datatype !TApp! which is useful for generic programming that encodes type application:

\begin{CenteredBox}
\begin{code}
data TApp f a = MkTApp (f a)
\end{code}
\end{CenteredBox}

A naive algorithm may compute the kind of the type argument !f! to be $\STAR \to \STAR$, making the kind of !TApp! to be $(\STAR \to \STAR) \to \STAR \to \STAR$. This however may be too restrictive as it allows encoding only those types that have a kind $\STAR$. A more appropriate kind for !TApp! would be $\Forall k {(k\to\STAR) \to k \to \STAR}$, allowing a wider range of types to benefit from generic programming techniques. For example, we can then use feature to write generic programs over higher kinded types like !List!s and !Trees!\cite{magalhaes_generic_2010}.

We have identified two requirements: allowing user defined kinds, and
more expressive kind polymorphism. An unimaginative solution for the former would involve adding a new syntax to the language that accepts kind definitions similar to the type definition. For example !kind Nat = Z | S Z!. An improvement would be by reusing the datatype declaration syntax. For any algebraic datatype that the programmer defines, each of its data constructors will be promoted along with the type constant. To encode type level naturals, the programmer declares the datatype, as they normally would, and the compiler will automatically generates new type and kind level bindings. For example, in case of datatype !Nat!, the data constructor !S : Nat -> Nat! gets promoted to the type level while !Nat! get promoted to the kind level. Kind polymorphism is straight forward by allowing kind quantification only in prenex form. For example for a datatype !T : FORALL$\many{a}$.$\many{k}$ -> *!, all the kind variables $\many{a}$ are quantified before the type arguments of kind $\many{k}$. This simplifies the semantics of the type constants.

\subsubsection{Syntax}\label{sec:sfp-syntax}
\begin{figure}[ht]
 \centering
 \begin{syntax}
 \shl{\text{Kind Vars}} &\shl{\chi, \mathcal{Y}} &  & \\
 \text{Type Vars} &\TyVar,\beta,\Co &\qquad\text{Type constants} &T,\shl{H} \\
 \text{Term Vars} &x,y &\qquad\text{Indices} &i,n \in \mathbb{N} \\
 \text{Coercion Vars} &c & &
 \end{syntax}
 \begin{syntax}
 \text{Sort} &&\square & \\
 \text{Kinds} &&\kappa,\eta \bnfeq& \STAR \bnfor
                                  % \shl{\texttt{CONSTRAINT}} \bnforc
                                  \kappa \to \kappa \bnfor \sigma\sim\tau \bnfor \shl{\chi} \bnfor \shl{T\App\many\kappa}\\
 \text{Types} &&\tau,\sigma \bnfeq& \TyVar \bnfor T
                                  \bnfor \tau \to \tau \bnfor
                                  \tau\App\tau \bnfor \Forall {\TyVar\co\kappa} \tau  \bnfor F_n
                                  \bnfor \shl{\Forall \chi \tau} \bnfor \shl{\tau\App\kappa} \bnfor \shl{H}\\
 \text{Type Constants} && T \bnfeq& T \bnfor T_N\\
 \text{Coercions} &&\nu,\Co \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\nu\Co % equiv relation
 \bnfor \Forall {\TyVar\co\kappa} \Co \bnfor \Co\At\tau % abstraction instanst
 \bnfor \nu\App\Co \bnfor \Left \Co \bnfor \Right \Co  % compose/decompose
 \bnfor \shl{\Forall \chi \Co} \bnfor \shl{\Co\App\chi} %
 \bnfor \shl{\Co\At\chi}\\
 \text{Patterns} &&P \bnfeq& H \App\shl{\many\chi}\App\many{\TyVar\co\kappa}\App{\many{x\co\tau}} \\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co %
 \bnfor \shl{\TLam \chi \Tm} \bnfor \shl{\Tm\App\kappa} \\

 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Substitutions} &&\Subst \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
 \end{syntax}
 \caption{The Syntax of \SFP as an extension for \SFC}
 \label{fig:sfp-syntax}
\end{figure}

\SFP\cite{yorgey_giving_2012}, achieves both the requirements mentioned above with a relatively small extension to \SFC. The syntax for the core language is shown in \pref{fig:sfp-syntax}. The key changes in the language of types---highlighted to compare with \SFC---are allowing kind variables ($\chi$), kind abstraction ($\Forall \chi \kappa$) and application ($\tau\App \kappa$). These changes are reflected in the coercion language which needs additional constructs: kind poly-type congruence ($\Forall \chi \Co$), kind application ($\Co\App\chi$) and kind instantiations ($\Co\At\kappa$).

Other noteworthy addition to the kind and type language are promoted constructors $T\App\many\kappa$ and $H\App\many\tau$, respectively. The data constructors are now explicit in their kind polymorphism by expecting all the kind variables $\many\chi$ to be instantiated before any type variables are instantiated, evident in their type signatures, which now expects the kinds before the type as formal parameters.
% \footnote{As a  convenience notation, $\Forall {\many\TyVar} \tau$ is just a shorthand for $\Forall {\TyVar_1} {\Forall {\TyVar_2} {... \Forall {\TyVar_i} \tau}}$, while $\many\tau \to \tau'$ is a short hand for $\tau_1 \to \tau_2 \to ... \to \tau_i \to \tau'$}

$$
H : \Forall {\shl{\many\chi}} {\Forall {\many{\TyVar\co\kappa}}
  {\Forall {\many{\beta\co\eta}} {\many\tau \to T
      \App\many{\chi}\App\many{\TyVar}}}}
$$

Promotion of type and data constructors may result in name-space issues. It, however, can be disambiguate from the context. Another mechanism is to add explicit quotation marks as prefix to aid disambiguation.
For example ``!T!'' is a type constant but ``!'T!'' is a kind constructor.
Finally, to stop the buck on the syntax classification hierarchy \emph{all} kinds are classified by a unique sort, $\square$. To keep the kind inference tractable, there are no kind level lambdas, only promotion of higher kinded types which would stand in for kind functions.

\subsubsection{Static Semantics}
It is not clear which constructors can be promoted without threatening
either the consistency, or usability of the language. Backwards
compatibility is an important aspect while adding a new feature, as
the code that compiled without promotion should also work after
promotion. We use the following criteria:

\begin{itemize}
\item For a type constant, $T\App\many\kappa$ is a valid kind only if $T$ is fully saturated and has a kind $\many\STAR \to \STAR$
\item For a data constructors, $H\App\many\tau$ is a valid type if all the type arguments $\many\tau$ to the constructor can be promoted and the type of the data constructor can be promoted.
\end{itemize}

The restriction on the promotion of type constant is because promoting higher kinded types, such as $(\STAR \to \STAR) \to \STAR$ would not be possible without having a richer kind classification system. Further, promotion of type constant that themselves accept promoted types such as !Vec : * -> 'Nat -> *! are not promoted as that would involve double promotion of the type !Nat! or would require type and kind levels to be fully dependent, making the system complex. Kind polymorphic type constant are also not promoted as that would require polymorphic sorts, which is absent from the system.

\newcommand\KCoTAppK{
 \ib{\irule[\trule{co-$\tau\kappa$-app}]
 {\Kinding \TEnv {\kappa} {\square}}
 {\CoKinding \TEnv {\Co} {\tau \sim \tau'}};
 {\CoKinding \TEnv {\Co\App\kappa} {\tau\App\kappa \sim \tau'\App\kappa}}
 }
}

\newcommand\KCoKAbs{
 \ib{\irule[\trule{co-$\kappa$-abs}]
 {\Kinding \TEnv {\kappa} {\square}}
 {\CoKinding \TEnv {\Co} {\tau \sim \tau'}};
 {\CoKinding \TEnv {\Forall \kappa \Co} {\Forall \kappa \tau \sim {\Forall \kappa \tau'}}}
 }
}

\newcommand\KCoVarAx{
 \ib{\irule[\trule{co-$\Co$-ax}]
   {\substack {\mathlarger{c\co \Forall{\many\chi}{\Forall{\many{\alpha\co\eta}}{\tau'\sim\tau''}} \in \TEnv}\\
              {\mathlarger\Subst_\kappa = \Set{\many{\chi \mapsto \kappa}},
                \Subst_\tau=\Set{\many{\alpha\mapsto\tau}},
                \Subst_\sigma=\Set{\many{\alpha\mapsto\sigma}}}}}
   {\substack{\mathlarger{\many{\CoKinding \TEnv \Co {\tau\sim\sigma}}}\\
             {\mathlarger{\many{\CoKinding \TEnv {\tau, \sigma} {\Subst_\kappa\eta}}}}}};
 {\CoKinding \TEnv {c\App\many\kappa\App\many\Co} {\Subst_\kappa\Subst_\tau\tau' \sim \Subst_\kappa\Subst_\sigma\tau''}}
 }
}

\newcommand\KCoKappaInst{
 \ib{\irule[\trule{co-$\kappa$-inst}]
 {\Subst = \Set{\chi \mapsto \kappa}}{\Kinding \TEnv \kappa \square}
 {\CoKinding \TEnv {\Co} {\Forall\chi\tau \sim \Forall\chi{\tau'}}};
 {\CoKinding \TEnv {\Co[\kappa]} {\Subst\tau \sim \Subst\tau'}}
 }
}


\newcommand\KLift{
  \ib{\irule[\trule{H-lift}]
     {\vdash \TEnv}
     {H\co\tau \in \TEnv}
     {\empty \vdash \tau \hookrightarrow \kappa};
     {\Kinding \TEnv H \kappa}
  }
}


\newcommand\KKTyAbs{
  \ib{\irule[\trule{ty-$\I{\forall\chi}$}]
     {\Kinding {\TEnv, \chi\co\square} {\tau} {\star}};
     {\Kinding \TEnv {\Forall{\chi}{\tau}} \star}
  }
}

\newcommand\KKTyApp{
  \ib{\irule[\trule{ty-$\E{\forall\chi}$}]
     {\Kinding \TEnv \tau {\Forall \chi \kappa}}
     {\Typing \TEnv \eta \square}
     {\Subst = \Set{\chi \mapsto \eta}};
     {\Kinding \TEnv {\tau\App\eta} {\Subst\kappa}}
  }
}

\newcommand\TyKAbs{
  \ib{\irule[\trule{$\I\kappa$}]
     {\Typing {\TEnv,\chi\co\square} \Tm \tau};
     {\Typing \TEnv {\TLam \chi \Tm} {\Forall \chi \tau}}
  }
}

\newcommand\TyKApp{
  \ib{\irule[\trule{$\E\kappa$}]
     {\Kinding \TEnv \Tm {\Forall \chi \tau}}
     {\Kinding \TEnv \kappa \square}
     {\Subst = \Set{\chi \mapsto \kappa}};
     {\Kinding \TEnv {\Tm\App\kappa} {\Subst\tau}}
  }
}

\newcommand\TyCase{
  \ib{\irule[\trule{case}]
    {\Typing \TEnv \Tm {T\App\many\kappa\App\many\sigma}}      {\many{H\co\Forall{\many\chi}{\Forall{\many{\alpha\co\kappa}}{\Forall{\many{\mathcal{Y}}}{\Forall{\many{\beta\co\eta}}{\many\tau \to T\App\many\chi\App\many\alpha}}}} \in \TEnv}}
     {\substack{\mathlarger{\Subst_\kappa = \Set{\many{\chi \mapsto \kappa}}}\\
               {\mathlarger{\Subst_\sigma  = \Set{\many{\alpha \mapsto \sigma}}}}}}
     {\Typing {\TEnv, \many{\chi\co\square},\many{\beta\co\Subst_\kappa\eta}, \many{x\co\Subst_\kappa\Subst_\sigma\tau}} u \tau};
     {\Kinding \TEnv {\Case \Tm {\many{H\App\many{\mathcal{Y}}\App\many{\beta\co\eta}\App\many{x\co\tau} \to u}}} {\tau}}
  }
}


\begin{figure}[ht]
  \centering
\begin{gather*}
  \fbox{$\Kinding \TEnv \tau \kappa$}\\
  \KLift \rsp \KKTyAbs \rsp \KKTyApp\\
\end{gather*}
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \tau \kappa$}\\
 \KCoVarAx \rsp \KCoTAppK \\
 \KCoKAbs \rsp \KCoKappaInst
\end{gather*}
\begin{gather*}
  \fbox{$\Typing \TEnv \Tm \tau$}\\
  \TyKAbs \rsp \TyKApp\\
  \TyCase
\end{gather*}
 \caption{Excerpt of Static Semantics of \SFP: Kinding and Typing Judgments}
 \label{fig:sfp-static-sem}
\end{figure}


The programmer is expected to provide minimal (if any) kind annotations to the programs. This necessitates changing the type inference algorithm. While instantiating a kind polymorphic term, fresh kind unification variables are generated, and during type unification, the kinds of the types are also unified. This is necessary due to the design decision: there are no kind equalities. The unification of kinds does not produce any evidences, unlike for type unification. This helps in solving kind unification on the fly in contrast to solving type unification where they are collected as equality constraints and then solved separately, generating coercion as evidences.


The datatype declaration is type checked in a manner very similar to mutually dependent terms:
\begin{itemize}
\item The type declarations are sorted into strongly connected components;
\item The kind inference assigns a new unification kind variable to each of the type constant and then walks over the definitions solving for kind constraints that may arise;
\item Finally, all the constructors are kind generalized at the end.
\end{itemize}

\subsubsection{Operational Semantics}\label{sec:sfp-opsem}
\TODO{Does adding kind abstraction and kind application business to terms meddle with the operational semantics? we need to make sure we can erase them and preserve the phase distinction property.}

\subsubsection{Meta-theory}\label{sec:sfp-metatheory}
\TODO{What is the consistency criteria? how is progress and preservation guaranteed?}

In \SFP, there are no kind coercions nor there are kind equality
constraints. This keeps the design and implementation simple but at
the expense of leaving out certain programs that can be well
typed. The introduction of the kind variables in a controlled environment allows to
side step a lot of issues that would make the system complex and difficult to work with.
A logical continuation of the work would be to answer the
question of how can we incorporate kind equalities in the type system.


\subsection{\SFK}\label{sec:sfk} % K for kind eq
% We have type equalities, why not kind equalities?
% But we would then have two kinds of equalities: type and kind.
% So why not just squish types and kinds together, making it truly impredicative
GADTs are expressed using explicit type equality predicates in the type system. For example, consider the following GADT that encodes type representations:

\begin{minipage}[ht]{0.5\linewidth}
\begin{lstlisting}
data TyRep : * -> * where
  TyInt : TyRep Int
  TyBool : TyRep Bool
\end{lstlisting}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{lstlisting}
data TyRep : * -> * where
  TyInt : a ~ Int => TyRep a
  TyBool : a ~ Bool => TyRep a
\end{lstlisting}
\end{minipage}

The right hand side is just an elaborated version of the left hand side with explicit type equalities equalities.
Now, we can use local type equalities to pattern match on the different values of \lstinline{TyRep} to produce appropriate values. The function \lstinline{zero} computes a default value for each type representation.

\begin{minipage}{0.5\linewidth}
 \begin{codef}
 zero : forall a. TyRep a -> a
 zero TyInt = 0
 zero TyBool = False
 \end{codef}
\end{minipage}%
\begin{minipage}{0.5\linewidth}
 \begin{codef}
 zero : forall a. TyRep a -> a
 zero (TyInt (co : a ~ Int)) = 0 /> sym co
 zero (TyBool (co : a ~ Bool)) = False /> sym co
 \end{codef}
\end{minipage}

The operator \lstinline|sym| reverses the direction of the type equality \lstinline|co : a ~ Int| to \lstinline|Int ~ a| and then the cast operator, \lstinline{/>}, uses the flipped coercion to justify the return type of the function.

It may be tempting to extend \lstinline{TyRep} datatype to represent more complex types such as lists. However, \SFC is not expressive enough to achieve this without auxiliary definitions. The type system is not expressive enough allow higher kinded types. Consider a new definition of \lstinline{TyRep} extended with two new data constructors \lstinline{TyList}, which represents a list type, and \lstinline{TyApp} which represents type application.

\begin{minipage}[ht]{0.4\linewidth}
\begin{lstlisting}
data TyRep :: forall k. k -> * where
  TyInt :: TyRep Int
  TyBool :: TyRep Bool
  TyList :: TyRep []
  TyApp :: TyRep a -> TyRep b -> TyRep (a b)
 \end{lstlisting}
\end{minipage}%
\begin{minipage}[ht]{0.5\linewidth}
\begin{lstlisting}
data TyRep :: forall k. k -> * where
  TyInt :: ((k ~ *),(a : k) ~ Int) => TyRep k Int
  TyBool :: ((k ~ *),(a : k) ~ Bool) => TyRep k Bool
  TyList :: ((k ~ * -> *), (a : k) ~ []) => TyRep k []
  TyApp :: (k1 ~ * -> *) => (k2 ~ *)
        => TyRep k1 a -> TyRep k2 b
        -> TyRep k2 (a b)
\end{lstlisting}
\end{minipage}

The new \lstinline{TyRep} accepts two parameters, the first is the kind and the second is the type that has the kind of the first parameter. Pattern matching on the data constructor now exposes kind equalities along with type equalities. The new \lstinline{zero} function that now also returns the zero case for list would look as follows:
% \begin{minipage}[ht]{0.5\linewidth}
\begin{codef}
 zero : forall (a:*). TyRep a -> a
 zero TyInt = 0
 zero TyBool = False
 zero (TyApp TyList _) = []
\end{codef}

With explicit kind equalities allowed in the system, while the case for \lstinline{TyInt} and \lstinline{TyBool} in \lstinline{zero} is the same as in \SFC, the case for \lstinline{TyApp} can now be allowed and type checked. The first argument to \lstinline{TyApp} is inferred to have kind \lstinline{* -> *} and the second argument is inferred to have kind \lstinline{*}, making the result to be of kind \lstinline{*}. As a side note, because we have constrained the kind of the type parameter to \lstinline{zero} to be a \lstinline{*}, we cannot write \lstinline{zero (TyApp ty b) = zero ty} as \lstinline{ty} would be inferred to have kind \lstinline{k -> *}.

Adding kind equalities to the system is non-trivial owing to its interaction with type equalities. A few challenges are:
\begin{itemize}
 \item \SFK squashes the types and kinds to be the same by adding the axiom \lstinline{* : *}, or ``type is of kind type''. This necessitates a new formalism that proves that the meta-theoretic properties of \SFK are carried over from \SFC. In dependently type languages \lstinline{* : *} axiom amounts to adding inconsistency, however this is not a problem in Haskell as all kinds are already inhabited;
 \item in \SFC, equalities could only exist between types of the same kind, but now, due to explicit kind equalities, it is possible to construct equalities between heterogeneous types. The matters get more complicated with polymorphic types;
 \item coercions should not interfere with the operational semantics of the language;
 \item Kind indexed GADTs need to be able to abstract over coercions and use them in the types, a feature missing in \SFC.
\end{itemize}

\subsubsection{Syntax}\label{sec:sfk-syntax}
In \SFP, we had disallowed promoting types that were indexed or dependent on a promoted type, we can now get rid of this restriction because we make no distinction between kinds and types.

\begin{figure}[ht]
 \centering
 \begin{syntax}
 \text{Type Vars} &\TyVar,\beta,\Co &  & \\
 \text{Term Vars} &\TmVar,y &\qquad\text{Indices} &i,n \in \mathbb{N} \\
 \text{Coercion Vars} &c & &
 \end{syntax}
 \begin{syntax}
 \text{Type Constants} &&T \bnfeq& (\to) \bnfor \star \bnfor H\\
 \text{Type level names} &&w \bnfeq& \TyVar \bnfor F_n \bnfor T\\
 \text{Propositions} &&\Prop \bnfeq& \tau\sim\sigma\\
 \text{Types and Kinds} &&\tau,\sigma,\kappa \bnfeq& w \bnfor \tau\App\tau %
 \bnfor \Forall {\TyVar\co\kappa} \tau \bnfor %
 \Forall {c\co\Prop}\tau \bnfor \Cast\tau\Co \bnfor \tau\App\Co\\
 \text{Coercions} &&\MCo,\Co \bnfeq& c \bnfor \refl\tau \bnfor \Sym\Co \bnfor \trans\MCo\Co \\ % equiv relation
 && \bnfor& \shl{\ForallC {\MCo} {(\TyVar_1, \TyVar_2, c)} \Co} \bnfor \shl{ \MCo\App(\Co, \Co')} %
 \bnfor \shl{\ForallC {(\MCo_1, \MCo_2)} {(c_1, c_2)} \Co} %
 \bnfor \Co\At\MCo \bnfor \shl{\Co\At(\MCo, \MCo')}\\ % abstraction instanst
 && \bnfor& \MCo\App\Co \bnfor \Left \Co \bnfor \Right \Co %
 \bnfor \Nth i \Co \bnfor \shl{\Kind \Co} \bnfor T\App\many\phi \\  % compose/decompose
 \text{Types/Coercions} && \phi \bnfeq& \tau \bnfor \Co\\
 \text{Patterns} &&P \bnfeq& H\App \shl{\many{\Telescope}}\App{\many{x\co\tau}} \\
 \text{Telescopes} &&\Telescope \bnfeq& \empt \bnfor \Telescope, \TyVar\co\kappa \bnfor \Telescope, c\co\Prop\\
 \text{Terms} &&M,N \bnfeq& x \bnfor \Lam {x\co\tau} M \bnfor M\App N %
 \bnfor \TLam{\tau\co\kappa} M \bnfor M\App \tau \\
 && \bnfor& \Lam {c\co\Prop} {\Tm} \bnfor \Tm\App\Co \bnfor \shl{\Contra \Co \tau}%
 \bnfor H \bnfor \Case M \many{P \to M} \bnfor \Cast \Tm \Co\\

 \end{syntax}
 \begin{syntax}
 \text{Typing Context} &&\TEnv,\Delta \bnfeq& \empt \bnfor \TEnv,x\co\tau \bnfor \TEnv,\TyVar\co\kappa \bnfor \TEnv,H\co T \bnfor \TEnv, \Co \co \tau\sim\sigma\\
 \text{Substitutions} &&\Subst \bnfeq& \empt \bnfor \Set{\many{\TyVar \mapsto \tau}}
 \end{syntax}
\caption{The Syntax of \SFK}\label{fig:sfk-syntax}
\end{figure}




\newcommand\TContra{
 \ib{\irule[\trule{t-contra}]
 {\CoKinding \TEnv {\Co} {T\App\many\phi \sim T'\App\many{\phi'}}}
 {T \neq T'}
 {\Kinding \TEnv {\tau} {\star}};
 {\Typing \TEnv {\Contra \Co\tau} {\tau}}
 }
}

\newcommand\KCAppCo{
 \ib{\irule[\trule{co-capp}]
 {\CoKinding \TEnv {\Co} {\tau\sim\tau'}}
 {\Typing \TEnv {\tau\App\MCo} {\kappa}}
 {\Typing \TEnv {\tau'\App\MCo'} {\kappa'}};
 {\CoKinding \TEnv {\Co\App(\MCo, \MCo')} {\tau\App\MCo \sim \tau'\App\MCo'}}
 }
}

\newcommand\KCAllT{
 \ib{\irule[\trule{co-$\I{\forall\tau}$}]
 {\substack{ \mathlarger{\CoKinding {\TEnv,\TyVar\co\kappa,\TyVar'\co\kappa',c\co\TyVar\sim\TyVar'} {\Co} {\tau\sim\tau'}}\\
 \mathlarger{\Kinding \TEnv {\Forall {\TyVar\co\kappa} {\tau}} {\star}}}}
 {\substack{ \mathlarger{\CoKinding \TEnv \MCo {\kappa\sim\kappa'}}\\
 \mathlarger{\Kinding \TEnv {\Forall {\TyVar'\co\kappa'} {\tau'}} {\star}}}};
 {\CoKinding \TEnv {\ForallC\MCo{(\TyVar,\TyVar',c)}{\Co}} {\Forall {\TyVar\co\kappa}{\tau} \sim \Forall {\TyVar'\co\kappa'}{\tau'}}}
 }
}

\newcommand\KCAllC{
 \ib{\irule[\trule{co-$\I{\forall\Co}$}]
 {\substack{\mathlarger{\CoKinding {\TEnv,c\co\Prop,c'\co\Prop'} {\Co} {\tau\sim\tau'}} \\
 \mathlarger{\Kinding \TEnv {\Forall {c\co\Prop} {\tau}} \star}%
 \quad\mathlarger{\Kinding \TEnv {\Forall {c'\co\Prop'} {\tau'}} \star}
 }}
 {\mathlarger{\fresh {\Set{c, c'}}{\Erased\Co}}}
 {\substack{\mathlarger{\CoKinding \TEnv {\MCo_1} {\sigma_1 \sim \sigma_1'}}\\
 \mathlarger{\CoKinding \TEnv {\MCo_2} {\sigma_2 \sim \sigma_2'} }}}
 {\substack{\mathlarger{p = \sigma_1\sim\sigma_2}\\
 \mathlarger{p' = \sigma_1'\sim\sigma_2'}}};
 {\CoKinding \TEnv {\ForallC{(\MCo_1,\MCo_2)}{(c, c')} {\Co}} {\Forall {c\co\Prop}{\tau} \sim \Forall {c'\co\Prop'}{\tau'}}}
 }
 }

\newcommand\KCInstCo{
 \ib{\irule[\trule{co-$\E\forall\Co$}]
 {\CoKinding \TEnv {\Co} {\Forall {c\co\Prop} {\tau} \sim \Forall {c'\co\Prop'} {\tau'}}}
 {\CoKinding \TEnv \MCo \Prop}
 {\CoKinding \TEnv {\MCo'} \Prop'};
 {\CoKinding \TEnv {\Co\At(\MCo, \MCo')} {\Set{c\mapsto\MCo}\tau \sim \Set{c'\mapsto\MCo'}\tau'}}
 }
}

\newcommand\KExtCo{
 \ib{\irule[\trule{co-ext}]
 {\CoKinding \TEnv {\Co} {\tau \sim \tau'}}
 {\CoKinding \TEnv \tau \kappa}
 {\CoKinding \TEnv {\tau'} \kappa'};
 {\CoKinding \TEnv {\Kind \Co} {\kappa \sim \kappa'}}
 }
}

\subsubsection{Static Semantics}\label{sfk-static-sem}
\begin{figure}[ht]
 \centering
 \begin{gather*}
 \fbox{$\Typing \TEnv M \tau$}\\
 \TContra
 \end{gather*}
 \begin{gather*}
 \fbox{$\CoKinding \TEnv \Co {\tau \sim \tau}$}\\
 \KCAllT\\
 \KCAllC\\
 \KCAppCo \rsp \KCInstCo\\
 \KExtCo
 \end{gather*}
 \caption{Static Semantics of \SFK (Excerpt)}
 \label{fig:sfk-typing}
\end{figure}

\subsubsection{Operational Semantics}\label{sfk-op-sem}
\TODO{How does adding kind equality change the operational semantics
  of the language? }

\TODO{Push rule gets uglier?}

\subsubsection{Meta-theory}\label{sfk-meta-theory}
We have the following properties due by the way of static semantics.
All coercion proofs have no computational value, they are irrelevant
to the proof of equality. The ensures that even for heterogenous type
equalities, the proof of coercions between kinds, plays no value
during runtime. But is this limiting the expressivity in any way? We
do not want to allow only ``trivial'' equality proofs.


\TODO{\SFK is a representative formalization of the current GHC compiler. It supports kind heterogeneous type equality or in other words, the type language is un-kinded due to impredicativity.
}

\TODO{how to characterize type soundness and preservation?}

\section{Related Work}\label{sec:related-work}
%%%%%%%%%
\subsection{Ad-hoc polymorphism}\label{sec:rw-adhoc-poly}
\subsubsection{Typeclass alternatives}
\citet{kaes_parametric_1988} had similar ideas related to implementation aspects of adhoc
polymorphism before \citet{wadler_polymorphism_1989}. The work on
functional dependencies for multi-parameter typeclasses is directly
influenced by parametric type classes\cite{chen_parametric_1992}.
Implicit objects\cite{oliveira_typeclasses_2010} provide a mechanism for the
programmer to choose the intended behavior instance if there are
multiple available options. This pushes the burden of choosing the
right instance on programmer whenever the typechecker cannot uniquely
resolve an option or when the programmer wants to force the
typechecker to resolve it to a specific instance. Constraint handling
rules(CHR)\cite{fruhwirth_theory_1998,stuckey_theory_2005} is a recent
way to account for operator overloading. The limitation of using CHRs
is that the programmer needs to ensure confluence external to the
system to be able to get coherence guarantees.

\subsubsection{Traits and Interfaces}
Object oriented languages provide an explicit mechanism of abstraction over behaviour of classes in the form of traits (cf. Rust, Scala) or interfaces (cf. Java, C\#). In dynamically typed languages (cf. python), traits are emulated using implicit programmer conventions. Typeclasses subsume traits and interfaces. Typeclasses can be higher kinded, which often is lacking in object oriented languages, with an exception of Scala where traits can be declared on higher kinded types by means of generics. There is no equivalent of multi-parameter typeclasses and funtional dependencies in interfaces and the expressivity of type level computation is non-existent. Traits in Scala 3 can have parameters, but there is no way to express a functional relation between the parameters. Superclasses (not to be confused with superclasses for classes in object oriented languages)  mechanisms exists in both typeclasses and traits and interfaces that establish a containment relationship. The abstract behaviour described by superclasses is contained in the sub-classes.

\subsubsection{Closed typeclasses}
GHC/Haskell does not support closed typeclasses out of the box. In Scala, sealed traits accomplish this behaviour. Closed typeclasses, as opposed to the open typeclasses, cannot be extended with instances. This can be useful in cases where we want the typechecker to be able to deduce that certain typeclass constraints can never be satisfied. In current Haskell unsatisfiable typeclass constraints, amout to functions that cannot be called. However, with closed typeclasses, the typechecker would be able to identify such cases and warn the programmer about unusablity of the functions. Only parital behaviour of closed typeclasses can be emulated in current Haskell, by limiting the export names from modules. This just allows closedness property of closed typeclasses. The typechecker cannot use the information that the unexported typeclass is effectively closed to reason about whether the constraint can be solvable or can never be solved.

\subsubsection{Instance Chains}
Instance chains\cite{morris_instance_2010} enable finer grained control of which instance can be used to satisfy a typeclass constraint. The typeclass !Show! specifies how a value can be converted to a string to be printed as shown below:

\begin{CenteredBox}
\begin{code}
type String = [Char]
class Show a where show :: a -> String
class Show a => Show [a] where show = ...
class Show String where show = ...
\end{code}
\end{CenteredBox}
Now, if we know how to show a type, we can write a generic instance of show on a list of those types. However, we may want to override this behaviour for some specific instances of types such as !String!, which is declared as a type synonym for a list of !Char!. The typechecker now has two ways of resolving the typeclass constraint !Show String!: it can either use the generic instance via !Show [Char]!, or it can use the specific instance !Show String!. The typechecker cannot know apriori which is the more appropriate option. Such instances are called overlapping instances. Standard Haskell disallows such overlapping instances. Instance chains solves this problem by allowing programmers to specify in what order the the instances can be resolved. It also subsumes closed typeclasses by providing the capability to express that any instance that does not match with any of the instances listed above, it should be considered as a failure.

\subsection{Type Functions}\label{sec:rw-type-fun}
\subsubsection{Injective type famililes}
Type functions are non-injective by default. It can be a useful programming idiom to declare a type function to be injective so that the typechecker can use this information to rule out programs which violate this property and further use the injective property to refine the type information. For example, consider an injective type function !F! declared to be injective below:

\begin{CenteredBox}
\begin{code}
type family F a = r | r ~> a
type instance F Int = Char
type instance F Bool = Int
\end{code}
\end{CenteredBox}
This syntax is direct inspiration from functional dependencies for typeclasses. The declaration !r ~> a! says that the result of the type function !r!, can uniquely determine the argument of the type family !a!. Now, during type checking the instances of type functions, the typechecker would reject any axiom which would make !F! non-injective. For example, in the above code, the declaration !type instance F Char = Char! would be rejected by the typechecker. Further while trying to solve a wanted type equality constraint such as, !F a ~ Char!, the typechecker can leverage the fact that !F! is injective, and hence, there can be only one solution for the equation: !a ~ Int!, thus aiding the solver with extra information to solve the original wanted constraint. Injective type families\cite{stolarek_injective_2015} can be extended for closed type families as well.

\subsubsection{Closed type families}
The programmer may want to write type functions to intentionally restrict extensions. For example, the previously defined !Add! type function is defined only on !Z! and !S n! types. Closed type families\cite{eisenberg_typefamilies_2014} help in precisely expressing such use cases as shown below:

\begin{CenteredBox}
\begin{code}
type family Add m n where
  Add (S m) n = S (Add m n)
  Add _ n = n
\end{code}
\end{CenteredBox}

An additional advantage is that the programmer can also define a default case handler (!Add _ n = n!) for such type functions. This declaration is impossible with just open type functions as there is no guarantee of the order in which the axioms are matched. The syntax for writing closed type functions is very similar to writing case statements at term level but for types. Closed type families cannot prove confluence of type rewriting with non-linear type patterns (!Add x x!) as it is reducible to open problem in term rewriting community\cite{mizuhito_rta_1995}. It has to appeal to infinitary unification\cite{jaffar_efficient_1984} of types to claim consistency, which is unsatisfactory, as Haskell does not have infinite types.

\subsubsection{Constrained type families}
Constrained type families\cite{morris_typefamilies_2017} allows type computation to proceed only when the type family application can be provably reduced to a ground type. This simplifies the meta-theory of the language significantly. Constrained type families can prove confluence of type rewriting hence gives stronger type soundness guarantees. It avoids the pitfall of having to prove consistency by resorting to the use of infinitary unification and a conjecture to prove consistency. Closed type classes, which otherwise would need special infrastructure, fall out naturally in this system. The formalized system however, does not have a corresponding  working implementation, and there is no clear way of retrofitting representational type equality without resorting to the roles infrastructure as in \SFR.

\subsubsection{Partially Applied Type families}
In \SFC, We have a strict restriction on type functions to be fully applied to avoid unsoundness. The restriction can be resolved by using a stratifying the type function hierarchy\cite{kiss_higher-order_2019}: the functions that can be matched on, and the other that cannot be matched on. This allows the typechecker to reject unsafe programs which potentially violate the safety property by using unrestricted coercion lifting.


\subsection{Modules}\label{sec:rw-modules}
% Modules and typeclasses
% are they same
% no, different.
% Their co-existence incoherent
Modules help decompose large programs into smaller programs by allowing the programmers to establish separation of concerns. All general purpose programming languages provide some capabilities to make programs modular. \citet{macqueen_modules_1984} pioneered the formalizing semantics of modules in Standard ML.
Various attempts\cite{dreyer_modular_2007, wehr_ml_2008, white_modular_2014} have been made to enable a happy co-existence of parametric and implicit adhoc polymorphism together in a language by encoding typeclasses as modules and vice versa. However, every attempt has failed to avoid incoherence issues without crippling the language expressivity. ML and its variants\cite{milner_definition_1997,leroy_ocaml_2023} disallow allow operator overloading to enable modular compilation, while Haskell like languages compromise on the correct module behavior to allow operator overloading via typeclass mechanisms.


\subsection{Type Inference}\label{sec:rw-type-inf}
This report does not delve into the type checking problem. We have assumed that there either exists an algorithm which can efficiently compute the types of a programmer provided term, which does not contain any types via a complete type construction\cite{milner_theory_1978}, or the programmer supplies sufficient typing annotations so that the type construction problem becomes tractable via partial type construction\cite{pierce_local_2000, dunfield_bidirectional_2021}. \HMX and its variants is the state of the art type modular inference framework that is based on by using constraint handling rules. The stratified type inference algorithm\cite{pottier_stratified_2006} works in two phases: the first phase collects the typing constraints and then the second phase solves them. This is different than the greedy approaches used by original typing algorithms\cite{lee_proofs_1998} $\mathcal{W}$ and $\mathcal{M}$. The stratified type inference systems are better suited for languages that support GADTs and language features that introduce local constraints\cite{vytiniotis_outsideinx_2011}.



\section{Future Work}\label{sec:future-work}
Both functional dependencies and associated types provide the same set
of features: improve type inference by resolving ambiguous types and
enable type level computations. It would be tempting to pose the
questions such as: is one style more expressive than the other or is
it just a matter of stylistic preference? In other works, are
functional dependencies equivalent to associate types? Jones' original
work does not delve into formalization aspects of functional
dependencies feature. \citet{jones_language_2008} claim (without
proof) that they  are equivalent in expressiveness but favor
functional dependencies as it introduces minimal overhead in terms of
type system implementation. \citet{karachalias_elaboration_2017} use
an elaboration technique go the other way around. They however fail to
compare how their system translates improvement in type inference
which is offered by a functional dependencies and pose it as an open question.

Open problems and future direction: Are we falling in the same of
cramming the language pit by adding equalities. give more examples.
There is no satisfactory account of functional
dependencies in any of these system although they intrinsically exhibit
type equality. Functional dependencies have the following nice
properties\cite{jones_simplifying_1995}:
\begin{itemize}
\item They aid type inference by improving and simplifying types
\item Each type equality is embedded in a typeclass constraint
\end{itemize}

The first point is good from a programmers perspective. The types that
are inferred by the typechecker may be ambiguous, but with the presence of
functional dependencies, the inferred type can be matched exactly with
the semantics.

\begin{code}
  t :: C Int b => Int -> Int
  class C a b | a ~> b
\end{code}

there can only be one unique value of b, and hence this type is not
ambiguous. The compiler has only one option to choose from.
The improving and simplifying types goes beyond just the above
``simple'' example.

For the second point, the current typechecker implementation is
plagued with cases that handle class constraints and those that handle
type equality constraints. With functional dependencies, we would have
type equalities only due to class constraints. The typechecker can
then be simplified by having only once case that handles typeclasses
and it embeds type equalities uniformly. \TODO{Is this safer that
  having bare type equalities? maybe we can relax $\Good\TEnv$
  condition?}

\section{Conclusion}\label{sec:conclusion}
\begin{figure}[ht]
 \centering
 \begin{tabular}[ht]{c | c}
 Parametric Features                    & Non-parametric Features \\
 \hline\hline
   \multirow{2}*{Modules\cite{macqueen_modules_1984}}    & {Typeclasses\cite{wadler_polymorphism_1989}}\\
                                        & \emph{Functional Dependencies}\cite{jones_tcfd_2000}\\
   \hline
   ADTs\cite{burstall_hope_1980}         & \emph{GADTs}\cite{cheney_first-class_2003}\\
   \hline
   \multirow{3}*{\emph{Generative abstract types}\cite{breitner_safe_2014}}
                                        & \emph{Open Type Functions}\cite{schrijvers_type_2008}\\
                                        & Closed Type Functions\cite{eisenberg_typefamilies_2014},\\
                                        & \emph{Associated types}\cite{chakravarty_associated_2005}
 \end{tabular}
 \caption{Features of Haskell}
 \label{fig:haskell-lang-features}
\end{figure}
We surveyed some important language features--the ones which functional
programmers prefer---and seen in some depth, a theoretical
foundation of a modern day declarative functional programming
language. The language features are tabulated in the
\pref{fig:haskell-lang-features}. The features that we have
covered in some detail in this monograph are emphasized.
There are two ways of supporting a new programming language
feature, either via encoding them with the help of libraries or, via
supporting them as a core language feature. In the latter scenario, we
realized that composing different language features is non-trivial and
their subtle interactions can threaten semantic consistency. We
first identified that the three seemingly different language features:
generative abstract types, generalized algebraic datatypes and type
functions can be encoded using a simple single construct: type
equality. We then showed how non-parametric features
interacting with parametric features of the language that contain
explicit type equalities can cause type unsoundness bugs, unless they are
treated carefully. Taming ad-hoc language features is non-trivial and requires
sophisticated proof techniques to formalize and argue about their
correctness. As new programming languages are designed, built and
used, it is not only important to identify and carry over the good design principles
established by theory using the formalization techniques, but also
strengthen the foundational theory of programming languages
by understanding the real requirements of practice.

%\newpage
%%%% Bibliography
\bibliography{comp}%%%%%%%%%

\end{document}

%%% Local Variables:
%%% mode: latex
%%% eval: (visual-line-mode 1)
%%% eval: (auto-fill-mode 1)
%%% eval: (tex-source-correlate-mode 1)
%%% eval: (flyspell-mode 1)
%%% TeX-command-extra-options: "--synctex=1"
%%% TeX-master: t
%%% End: